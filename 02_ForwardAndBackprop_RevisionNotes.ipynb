{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikram310/Courses/blob/main/02_ForwardAndBackprop_RevisionNotes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhDxkOubyYNe"
      },
      "source": [
        "## Reference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7KidcCe0Lvx"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61crXPwqxqM9"
      },
      "source": [
        "\n",
        "\n",
        "- **How to create a model that helps in multiclass classification**\n",
        "    - Notations\n",
        "\n",
        "- **Softmax classifier**\n",
        "\n",
        "- **Categorical Cross entropy**\n",
        "\n",
        "- **How to train a NN?**\n",
        "    - Forward Prop\n",
        "    - Backward Prop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from category_encoders import TargetEncoder"
      ],
      "metadata": {
        "id": "XRY3oc8bM-Zh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "e0767de6-da4f-4930-babd-dc413b561ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-14f4446f821e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTargetEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTFPIy3KzJxY"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h55EujXBM6D"
      },
      "source": [
        "Lets start working towards our goals that we mentioned earlier\n",
        "\n",
        "## How to create a model that helps in our multi-class classification?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG0r1deM1ok0"
      },
      "source": [
        "Coming back to the problem at hand.\n",
        "\n",
        "> **Our data consists of 3 classes.**\n",
        "\n",
        "#### Question: Can we use simple logistic regression to classify 3 classes?\n",
        "\n",
        "No. We can only represent the simple Logitic Regression as a single-neuron model. i.e. predicting 2 classes.\n",
        "\n",
        "#### Question: How can we modify the existing network to account for multiclass classification ?\n",
        "\n",
        "Perhaps we can use multiple simple LogReg units (LRUs).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi0c_FtnUaY4"
      },
      "source": [
        "So, instead of using single model,\n",
        "- we train multiple models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsH7nOPrU1sJ"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1cKo51V8wHKr4iCp3EX-NompxGhU7txGT' width=\"700\"></center>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kEgoAHX32G8"
      },
      "source": [
        "\n",
        "\n",
        "### Can we do this using a single model ?\n",
        "\n",
        "Recall multi-class classification.\n",
        "- We calculated the probability that a given data point belongs to class A, B or C repectively.\n",
        "- Then, we returned the class with highest probability as the answer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UtYJd20b0dO"
      },
      "source": [
        "**This gives us the intuition that perhaps, our output layer should have 3 outputs. One for each class.**\n",
        "\n",
        "So we can have a NN that looks like:-\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XArp3UMbdhgv"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=131k2Wo-rcM6w8ZB7twLi2_h4-UKs5ILI' width=\"700\"></center>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPqkNWmJeoE6"
      },
      "source": [
        "Do notice that\n",
        "- we have same number of outputs (3 outputs)\n",
        "- and same number of connections\n",
        "\n",
        "\n",
        "The difference here is that\n",
        "- computation is happening together\n",
        "\n",
        "So, instead of having weight vector,\n",
        "- we'll have weight matrix multiplying with data matrix.\n",
        "\n",
        "We'll see it in code later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x36XV69dUeiW"
      },
      "source": [
        "A model formed by utilising multiple neurons is called a **Neural Network (NN)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkowDYSWnJJe"
      },
      "source": [
        "Let's learn some notation to make our life easy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMWsJqxbnRWf"
      },
      "source": [
        "### Notations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqQ4M0Y3nSq2"
      },
      "source": [
        "####  **Inputs**\n",
        " - We have two features for each datapoint: $x_{i1}$ and $x_{i2}$\n",
        "\n",
        "\n",
        "#### But we have m datapoints?\n",
        "\n",
        "#### Question: How did we represent m datapoints?\n",
        "\n",
        "Ans: As a matrix where each row represent a datapoint\n",
        "$\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} \\\\\n",
        "x_{21} & x_{i2} \\\\\n",
        "...  & ...  \\\\\n",
        "x_{m1} & x_{m2}\n",
        "\\end{bmatrix}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzKCBb2SnW1r"
      },
      "source": [
        "#### **Neuron**\n",
        "\n",
        "- Neuron is represented using $f_i$ where\n",
        "    - i refers to the neuron number\n",
        "\n",
        "For example: $f_1$ means 1st neuron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GsSHuFxCXwG"
      },
      "source": [
        "\n",
        "#### **Weights**\n",
        "\n",
        "- Weights are defined by notation: $w_{ij}$ where\n",
        "    - i is the source neuron\n",
        "    - j is the destination neuron\n",
        "\n",
        "Let's define the weight associated with input $x_{i1}$, going to neuron 2, $f_2$ (say) as: $w_{12}$\n",
        " - Similarly, we define other weights values as $w_{11}, w_{12}, w_{13}, w_{21}, w_{22}, w_{23}$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnAnu9ocpp-j"
      },
      "source": [
        "\n",
        "####  **Bias**\n",
        " - Each neuron will have a bias term associated with it ($b_i$): $b = \\begin{bmatrix}\n",
        "b_1 \\space\n",
        "b_2  \\space\n",
        "b_3\n",
        "\\end{bmatrix}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6RiVfMwsuIH"
      },
      "source": [
        "\n",
        "#### **z value**\n",
        "$ $\n",
        "- z value represent the linear operation i.e additive multiplication of inputs with their respective weights\n",
        "    - $z_1 =w_{11}. x_{i1}  + w_{21}. x_{i2}$\n",
        "    - $z_2 =w_{12} .x_{i1}  + w_{22}.x_{i2}$\n",
        "    - $z_3 = w_{13} .x_{i1} + w_{23}.x_{i2}$\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4rnjMlOsvov"
      },
      "source": [
        "#### **Output**\n",
        " - Each neuron will apply it's activation function on the z values to outputs: $a^1_1, a^1_2, a^1_3$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgX3tkxTuPYY"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1LDBuwe2xUQ_CF9ymiHvxzb2ojuYq16oz' width=\"800\"></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHDSMRYB4n4z"
      },
      "source": [
        "\n",
        "#### Q. What is the problem with this formulation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zElbHCbSqwxE"
      },
      "source": [
        "#### What if the model predicts >0.5 for more than one of the classes?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoUGvQe-qzek"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=10HtCW-GFURe89gFpN2OwcC7Pj1xddHOb' width=\"800\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3Hm0FPPqy7D"
      },
      "source": [
        "- Model will predict the presence of multiple classes in the output - [1, 1, 0], [1, 1, 1], [1, 0, 1]\n",
        "\n",
        "#### But how can multiple outputs be 1 ?\n",
        "\n",
        "Recall that we are taking sigmoid of output\n",
        "- range of $σ ∈ [0,1]$\n",
        "- and we have sigmoid for each class.\n",
        "\n",
        "Hence, each probability can be > 0.5 and therefore multiple class labels can be 1.\n",
        "\n",
        "But, **We want these probabilities values to sum to 1, as the we had in Logistic Regression (p and 1-p)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNj_ccUMH_O0"
      },
      "source": [
        "<hr style=\"border:1px solid gray\"> </hr>\n",
        "\n",
        "## Softmax classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nejBoCbZrEV2"
      },
      "source": [
        "\n",
        "#### What constraint do we want on your output probabilities ?\n",
        "\n",
        "Consider three outputs - $z_{1}$, $z_{2}$, $z_{3}$\n",
        "\n",
        "- We want a function that should map $z_{1}$, $z_{2}$, $z_{3}$ to output such that\n",
        "    - sum of output probabilities = 1\n",
        "\n",
        "#### How can we map the outputs of three neurons in the last layer such that they sum upto 1?\n",
        "- One such function is a **softmax function**\n",
        "\n",
        "where\n",
        "$$p_i = \\frac {e^{z_i}} {\\sum^k_{i =0} e^{z_i}}$$\n",
        "\n",
        "Here, $p_i$ refers to the prob. of datapoint belonging to class i\n",
        "- The denominator here is the normalisation term, to make $p_{1}+ p_{2}+ p_{3}= 1$\n",
        "\n",
        "<br>\n",
        "\n",
        "So, softmax can be thought of as sigmoid-like function for multiclass setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh9DWlFm1-6o"
      },
      "source": [
        "\n",
        "#### <font color ='red'>(Optional) </font> Question: But why not directly use $\\frac{z_i}{\\sum^k_{i=0} {z_i}}$? Why to raise the $z$ to power of exponential?\n",
        "\n",
        "**Intuitive reason** - Ensures that values are **non-negative**, and lie only between 0 and 1.\n",
        "- as the value of $z_i$ ranges from -∞ to ∞\n",
        "\n",
        "<br>\n",
        "\n",
        "Besides this, softmax function has some other desirable properties\n",
        "1. Nice diferentiable $\\frac{de^x}{dx} = e^x$\n",
        "2. The output probabilities can be interpreted as log likelihoods (log odds)\n",
        "\n",
        "Lets redraw the flow for an input data-point belonging to class C\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=12QEBrbkVaZiK-eXpGC9xI5xTNxjshk8U)\n",
        "\n",
        "\n",
        "If we were to just use normalized z values instead of exponential\n",
        "- the ratio of prob would be 1:3:6.\n",
        "\n",
        "- However, softmax pushes the probability of largest number closer to 1. Hence, the term soft-max\n",
        "\n",
        "\n",
        "\n",
        "So, we decide to keep **softmax** as the activation function in our NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiRHK40JouWh"
      },
      "source": [
        "Softmax calculator: https://keisan.casio.com/exec/system/15168444286206"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5qfsj0dEwin"
      },
      "source": [
        "---\n",
        "\n",
        "We know we can train a simple Logistic Reg model using SGD and all, but\n",
        "\n",
        "## How to train a NN?\n",
        "\n",
        "\n",
        "\n",
        "Lets first define some variables"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "!gdown 1dLOPwh01o3k8p_hK633ixhD1ehz6nNWk\n",
        "df = pd.read_csv(\"/content/spiral.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "o9UGfgup2bki",
        "outputId": "5cd7ea43-b0d4-4677-81a6-691f84a21f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dLOPwh01o3k8p_hK633ixhD1ehz6nNWk\n",
            "To: /content/spiral.csv\n",
            "\r  0% 0.00/12.9k [00:00<?, ?B/s]\r100% 12.9k/12.9k [00:00<00:00, 20.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         x1        x2  y\n",
              "0  0.000000  0.000000  0\n",
              "1 -0.000650  0.010080  0\n",
              "2  0.009809  0.017661  0\n",
              "3  0.007487  0.029364  0\n",
              "4 -0.000027  0.040404  0"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-ff248665-095b-44e3-bbf6-ecd627dddd46\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.000650</td>\n",
              "      <td>0.010080</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.009809</td>\n",
              "      <td>0.017661</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.007487</td>\n",
              "      <td>0.029364</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.000027</td>\n",
              "      <td>0.040404</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff248665-095b-44e3-bbf6-ecd627dddd46')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-bcffdc22-600d-4355-8d6e-597141bd0f1a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bcffdc22-600d-4355-8d6e-597141bd0f1a')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-bcffdc22-600d-4355-8d6e-597141bd0f1a button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff248665-095b-44e3-bbf6-ecd627dddd46 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff248665-095b-44e3-bbf6-ecd627dddd46');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMyAGfg2h4aP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Separating feature and label columns\n",
        "X = df.iloc[:, :-1].to_numpy()\n",
        "y = df.iloc[:, -1].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry6jA2XAMx2X",
        "outputId": "6e010531-e5c1-48ec-81dc-1b63cec6387d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 3 300\n"
          ]
        }
      ],
      "source": [
        "d = X.shape[1] # 2 - dimensionality, number of features\n",
        "n = len(np.unique(y)) #3 - number of classes\n",
        "m = X.shape[0] # number of examples\n",
        "print(d, n, m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6494vpurEV2"
      },
      "source": [
        "\n",
        "#### Question: How many coef parameters $W$ will this system (of three LR/neurons) will have?\n",
        "\n",
        "\n",
        "- We have 2 features $x_1$ and $x_2$ and which going into 3 LRs --> $d=2$\n",
        "- Notice that #LRUs/#neurons in last layer = number of classes --> $n = 3$\n",
        "- Thus, we will have\n",
        " - 2 $w$s for LR1, and\n",
        " - 2 $w$s for LR2, and\n",
        " - 2 $w$s for LR3,\n",
        "\n",
        "Therefore a total of: $6$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4zChByiEdEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3nJ8iIS1OR8"
      },
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOoAAADRCAYAAADCKS4gAAAABHNCSVQICAgIfAhkiAAAIABJREFUeF7svXeTXceRL5i3HbxtuEaj0Q3vCXovkARJifKGkkYzszsz78VuxMbG/rHfYN5n2D83Yt57s2M0GnlSJEVSEr0DKQIkvPfeAw3Xdn+/MueUO/ee230bhBQ6ZOOeU5WVlZVVWSYrK6syjEdKPASqRHBhqEUFyDDKSRtFRQE+8HBlGHm7udsE1RK6xDp0VaMDcRpjSbwAi2mzGQwDl0+1jglxO7SFUdVorQLLQsR02QRFCcNwh65qdIQ8C9G41WDeizhD7v/p8yyWkgQL6g6qlBXUqpIXZFuiruomNEsQtZ88tzHNN0FxlF/tgASWxgclyWA2WRv6C89Crid55slcwLMoQYixsd9NjUXXeGzkR/Un6MFqJ6iOro7YRvadt5FslPCL4VnUtmsVukZ8jeg6arIA1MugkbVdkF+V4DoEtTyhGKZ1lqPkZF6xGpH6l2QUkIIZcmGcIsid5QN2pOT56ThdM09EVxRgIaPfseKZyugO4VnODcOzJF0GqlZdslhOOxtpXZI9eVYOliRtutpqtrOodkcfUHrqW5VvZFhWC8WNsyoOyzG3TIWoDCb/J82NjLZCZE5FpVGoULclFKKyQADA6zB+ikG/OJ6RLj61aSuEaDDPTD6leVZMl62qqhBuXVZnhOaSoas6qK3Pqjlrxo/g39KCGq1RWdgETX6w82VfC9Jp2vPIquARjkQ+ATOiJLapuRExkINFR1YFj9IHAVF8WGp+O0D2tSBdmLoqeIQjkc9feOZzwOMZP/jknUqq/QcsbNhnaUGNaCa9UeUX06VB60iQoUqlSYU59BREp6grC1oWzs3DS2M/6kCkQasnSMemQlNhf3480/wvKKupnHRsKjQVNjKepdpePWGlBbU2UrdQBQWsjWQMIEgLn5rD/xjkXQtlwKc7hm1/+jwrZmVxTLK2SoNX4VkScX2BdSiTUogtcYm4gqg42IbEMQms+VKxHHgKRcmw+jKIoeMQnXG94dWUXkW4CopYJ3gBlirB9WUQQ8chPs/C+PC7Cml/4lHlBVXxJGSMO0r571ZhEfKnUojDH/FyOwxHqwpkCgpkePiTtNmcmSLA7UZ5BNry8TcxAjNrzz4kp83SZcjT6ZN0NYJn6QIU8UxBhzyzgRaV93un8iykC60pXU2oPbcu3cIFbSGrT7+dNYJnSdaOMLCuqW/UfLOAKKYmOZaNZJtSzOLF/nqJS6L2wIo/atJFAJvc1uFoaIvIzwKimJq0MQWfO51n5BefqD7dAmiQ5L8RZ/5MeZYsfEFg+REVCMLOS/dmEVtNX2ZrxdRYQIDbN9qKtb85KHA4mSqM+MfBnIG6tOW9rE9bns68eaNjnqvFRXrStJn0tkGWpEuBqTQ+XTYkK1kVulzamO5O5BnpimlDmZ1KVxzEP3md2FQjbGcN4llGTwna8rqM6zMvTWPe6hpR/Swd4jw6qxOd7GVdKYvKVR1fBB4KQZY8xhOHaGwqvCgyznBkIXXg//PhWR2FTnG1juQRz1L4vLA6kKt0AXy9yWvS4wOMQlDrzMkBH+MyRYTVk189sFFGYxhwp9LFIo+MtpGlqofFY5+DT81Y5lfX1DdkEgmr79Epqg6gVRHmOdafd1XEiBwZRp0qkTYRRAoKgqsQ98XwrH46qxQhiCrEXRBREFwlw/pT+HWTp1dvI0NXhb76o0YlqGmBs9qzoHTqkymqlbpanJ8+1h4XY9Z0ulo9J5/sVdOWLlNxbWn4sFxA6iByS5XG/6fKM82XdJm+eJ6NjK5EOwsQFbXS4vyIc3RPHYKaIC8L8pti3ngd4kwpeN7Qf1wBQhxReVnhw/kezjRFFk+eIMMcplcZumdDHRqcV582TVeOytDmEZ/TltOl89JgOrVX4gyhS6SlLeCN+bxTeaY579IcbnGEPDN1ZYr+58kzv3U36qsOQQ0akdMe3SZHqfK/fVKVksZ7cgFSUczGyUo10qzB4tUgyNG4CUyol95mVp0ulbVHm6bLospoc2i3tDHO0sVonzY/xC1Ljqo6bXcqzxTnEzyz5Yp5xnmQrs87lWeky6XNr0uvNQTtNK/NsXirQ1D97P1x0JGMTMq8GsxbbwbqxJtXF4tKgP9jzDrEhwWowsGG4OdbCUZgN1a9uwEKKfP1cWjMtvx5nKUhpMilzdJlU9vfnC4Xe5yvgrd0ucQaUJ8PpL3xPHNpV9m6ZP7J80yXx61Ltz59/lpOJNqZ11JdjjXmvTFaX1ZcVKJkoEN1rfjGFFA1Ko8228r0KjciO2uFcUxGUYRzBLQmcSQD7yiepUtai26kKgGSxl2r+LUQ14qvmWs5gDHOZsQjajnqXSgz1rFA6gkFIRgLM7jsRSdTn0GYwVjuh/m6faaDS73m8S4+j5yQ9NGSVEj4ncczTWqKZ3Eh/iR4lpGdalOpsLictyNkVIKaFcM0XK9iIkE04kFYH1B9Rk6tssWPr9/VM9mUpJBdGrHHXoAG2RXzNaMtx6Npc7qVYMFocRfZnOrMcoqyN5eujPS4XCrE41mO8S88s8zxq9Tnmc/7iGfODMptN/o9rg+3Pl141pH37ZM06q+6pr4kJCPd+0CEXdcZm7Ys2lJfrcxuXApeIXMjDHaHBi+/MK/ATKUqrGWpgzvjcmGYjXBojEmMp39/IjxzShWXwW2ChfwJ2qmCc3lmKuzPiGdBiUf9WZegVm2wJjLB/kIi83pN1XCeLKtXBnlCGKSrYjfm0hWhCSgsSxeTZbSFnYOO0ZiVQAIggjHRpljVuRAQ6eIPo8rSxXR/5jzzeVqdw6XqsgbPElXRkKCRCaqbdarsqbAsTToyHVpQRgDHfmsLYL+Q4BqlSUWnwv7Cs7z2UvxJhf2Z8mxUa1TFk9QokQrLGJiOTIcWS1nKtXUx9G2KYcNhJ5JkikNDqrCpsL/wrHrFVeVZ9aRlY++UdjZyQVUNcgRPkEh/JjDZoCyqENInIoEqtS9ai/IUGpUmimCA0c6y4eCPPxGYSZsMr5OYQk5Y5OpX02XfbBbJ/BEYhSf2kuskMwePkSv6VHAtnpmS1Or7ItqS2r1EF/oF8yyiuyCgvqkvC5XoxeLgICQGKCCnzmAXb0EefvBtoovFsFmVostN4KStkx2lwO9wnlG+Mj1TUKCYlU5IHFmKHaWA6uZZKax1AdU3oiaElC0yDkYIC2efEEDF8R8HKHv1w21wAJ33xkV5qPCQNkOIRRrSZcnyiHfJdOh1gv1QUyqLO5VHRBczdniWSqMyCbiQZeyH2+AAugTPdAo/e0OXRVqVNtIYkpkRmUVZEAOdhQ/hbXAQfwaFGx/XJWPvVJ75lDfiq44RFVMVdHeuRwFWQVG9MTyMt99heHFBQsjw26b0aSuGyul1YYreG00X8dWiLYy332F4edqqQAJp5jYFYNXqMqS9ETy7eqMiZy+LHD0ncuaCyNXrIn2Q0vEtItMmicyZIdLRLtI+Dd8T4nIU8aYoPMZgQ+rgbgmeFecz8pg6BDWRiSpfQSELghNY6grK0CbxJwMN/lRcKqwucjLgEFP47QMWxBYEj4yiRKok/mTgmPGMS99b/RDO8yKb94kcOA7hvCnS1irSDAHlM4h4wtwagNCOF1kyX2RVt8iyTpFJ+G5ye5Rq5CdYUHdQEn8ysG7U9SRogKAWZHf7ywJCSmRKED6qskvAG/DG/hTkWxDc2LxDbCUybRDPBjG3vdQrsueEyIc7RK5jBG2GgM7GiDkXo+ckjJrNWIzdvCVyEXAnLwL+EmoJAkshfvwekbU9IrOmOsJagvywxHV9jzX+ksSYPqwkdAhmeraisoTh6b11B8pNoN6pGUwpyEPMlrB48hZBejTrD8Lwsal1miCl/TR0ETrCHfKn8Ls4bYjzjuUZCI2XQcU8u9k/jGluRXYeFflkG4R1t8iCbpG71+BvsUgXprgcVfkMAc3NPggpBPkYpsXbD4hs+kzk95t013ov4GdMyZk7pjyLvcclazWkIQk0isC6BdWaznvHsFOtCa0+EpvkNNmBcuPNu4fDckOpBllqnz0pscaUwW9Rhlk+bX53oOMC6i1tDo0ZREYXkEeBpDLubpJ02WK5FToanpEuPqSp0TwL6KrGs0HUwdELFflwp8i2XSLXrorMw3T24hWRzzCyTsV0dh5G1HEGZzOQTRw/jGluRYUv68BadbbIL18V+WAL4DHy3os1LEdfli2oKQSkxMaBqtbO3KQN4JlblaN5Ly+opgDh+KaCnV6HTqDtd8iuMC0J92HAzKxx5YzNYGyQ+c2afwnaXLrCfGvTxRSGtqCH1eU3VeC0BUVbCboULWPFM7cFuzzLeMxS+XxWJbX22g5dI+UZ+8nTGEnf3CyyA0I6fabItzeKLIbgHce09oW3RfZinbq8i+vPmGcU2umTRZ5YK3IeU+Gte/F3SGThHEyXp2u++23IL5OpmfLtbJQ8s/k1+rf89oxbAEWFrm0/OBdSxkdJ8mRZOaJmwgAVCPymQUV4WPsWzHsx6VSs29O6dGmYCKefNKBd56f30G0rD/MwmZofhT/K5A7hWcZjS3NOl1sfbgecLI5f5IhnnMJex3rz7c9F9mH6OgfC9cR9Ind1Y52JdelCTHep3eXTD21vEc+oPJrQhinyUr2OPXUGyqizOe0RmxnlV1PcHWU8AKCBjfC47awkzwKWNOyzvKBGWZJyltDliC0qw/R7wK+CKwhy+DwbpE9lQYCq64ZUIoR5hOg1Ykhb1kF4Zc1p0yWyZbTFj7B4qf2PAtoU0J3Os7i2qxQUURW5hXUm16SfY8o7AdPb9ctF1i0UmTxuWFqaodGF8I3DHwWRe6j28a8N0Tyjq5sFEPQFmDIPAO+RUxTuO5Bn1Zky4tj6BLWeNjlikoKEjlxURWl6Pw2TSJQISuMLAcPvkL4a8V8Ez9IFi0MbxrMY9RAk7wqUQZsx3WW/unoZhLRbZCaVQKajpYA2oQU2QWjdJ7UUYfwUrE07Z2n4E9h3vd5Xg/cRWfXCRwjQW7kV2gB8iSxSQfUJaoBBTW45H4waY16AsCjhN5Nmk2T9EdMZJEqDEUgTov81dEW0afSEDmlxM47yiALSCFJgPl7SZWjzSppTE9IVfus8TGgqw0ThUmD5lMVyztDVAJ5xH/QE9kp3Qru7dJHIfRhN52Gq606GqAyajTAaNkzEyFr02HZGeZ6JqTIF+xIUUtex/2rlJuRR+O2VP8WMMeJZUZnqDS+tTGLZwhmnmkBmgW7tmgo3PyTKZZzCZYRK9Z46IC05iAuPtOWbGzkmTYbOJYvPMrW0mQCTn83WZZrF6NEVFsAmiGjL1FuAMLpet+C23F8wz2zHOJY8uwIh2o/p6TlYHi2CoQL3SlspaQ7POPV9Yp2e+k6dSKbW5tn4cWgmwHPzBqbA2F91qiLqeJ3Wka9qWB9BnSgcUV3aNkpgTRfBavMshdxSOfLf0oJqG3/W5BUXXKLydzcqRbYOc2JSQKZMWkPuACgC8K2C+E/W7LS8R3RZuLw/sBWVytajLQVg6OKPR5ulK8uOop43FRUc0ZZncDt5ZjvJrClGdI2SZ8B35Rq0vRhRexZDUOdixISApXjWTmUS2KCtjXyeKe4EtHGqzGZHRdWA4rl+UlXl1SXBUkAmfa12Vg/Pcqoa91ZaUG0pc+YVE5HtEep2WgxYIyZP7iCKmJ0HJLfPnDy8vUugzLbJatCRinZpU+NoRBdTOU2lBi/uKJ7R2uA4zIcwt6zQXGgqFpaTIFHjWmUYkpIsqsOkPox057BHSvtdmv9x31PteWYcyUdOzxzQgShqZwPUOuGvFfj4p4ipwVudc/G/jW5nxTmNPKa0oHq8SNaUK0wGIIArxU8C8UHaPHmYYQGmEEwhchE6AB5+k2e1H4smQVtRtl5HUARkS2klPYArKKlPaSN5RlxXMRy+9yk2Pg9jqMM3reLnYAN01gypzIT93gz8TcPfFGxwci7KYc55uD49jynvVQjrQmzHjMNU1S9WkbookLkEz/qAm+tSWjGpqTSfL5pniginEgxZjfwpLaiNzPQvuO5wDnD4oxBS/o7Dho8WCexIqHbtwTy2FVLCjdBOvM/HnskcWC/MwDdGXMJxQL6CNST3Rucg2I6mjSg17YA57eV2T1sVBVQj8rqTcJQW1FyBY8iPunrdrfnBzhde1aARpXPZYYFysCR4tTlrlCBFF/M0gC58lNahzcyry4LrGUGAMMKfos0Bwutt5xlJmo6p7jOPwnavW+QwhPTAMfxhKnwOpkQnMZ/djrB+zG+7IIVLF+BoyxJYMUCt2w5ToXHj5Oa18XLz5nhpg6URrYrC7RdPAAKeVGtn3PK5jMGev1M5mLNfUMi+YJ4pGoJh3Svk6D9KC2qmzyRPLF1RwwvJzYnXsmVbXppwV2dqU0YVp/L2M86+7EtNumwBUMXIKNYSx/RZ2tzqiGlzmcPmY6Z4dxjPPCpTPONUliPq2hUiayCANC+i/d6JkyIHIbTzIJC78XseR1zegTXDm/h7CMKq7PraZWjaEhnu7YFhwyQ18vnr0KByorp0eEZm888kIRlnoaDiGrUDWzoTIKj6Gft2VpNncZNpaEhpQc1Y4bVUt5ZdpiHcjnqmhHoJFjdtd+xMr1zcDC00wlTWOv+INq/yXRYbeh2yLV2k3gbrkrhfbhfi8t9pICTH9qqGtuz7DuOZx2dLWxHPGE+V7XgIYQesDe5ajQOkFFyMrFTrnsLUmKPtR9tFejHcvf5HGbdyvUxYNklaJ0ySFqT3FW0j59l5oOc5Vjat2RBUVf9uO2Neqnoa387q4pnbRBr0XlpQXcHI885qOSAHjLJMy+slwUJXGHzA3FDdP3uioLL6cdMU4XJhzHsB2Xn1+iJrq19lDU1GbgOb05bl4tJmBNZNn78HjGHjQpAKDaL8ZldUTpe2BvNsGEMYVblXMIKehZHtUWyQcjq8H39U73IEpgsGrlsfWCPX5qyRG7fmShPWqoxSJoGOtNoSuDzLCl6FZ8eQ9U3sz87FkpiDuhLIO5JnpK2xT3lBTbWgsD07tIXNMI9yY/R7UI8KNBeGxDhrK8fjRXGO5Vlm6EEDKMLmG6on4FzaFJIA0+3gGVqvP4ppMuIyhbSBODototr2BiTiGmwAr0AQqcLlyHkCknIaIynVra1oOvSdcgjhvZBInlV7/G5Mge+WW8Od0r9vglSAhsWvadzvkpHgGdsHjRv2Y2nMLZkuCCqVzwnuOlUdIXXi8teMtpHyLIm18YHlBbVq3lVaX9V0JSJHjXrUCBSRqc6kBPVVQBpDV5UMiqOojaFAcq+DKtpbmMpeh5qWLhdoqUA3DBewJuU7LespsGegSDqAYyvHIbidkJJuTIW5NTMbiqeTpzF0Am5JN6zmO3DQlBunmmfMqt4n5Az3Ts8g271H0B9gO5eCag0o6sHd+DqsJ/fRwY5KUPW0I2SrmplyUqL+LfNEvX+WCDjCSARl050C5HnOPg1ReEHNpagOybClJAkKb8ni3laeqaEIrXwAQjkAoaSmtp+jJYSSe6UX0PovYrTk2pLT2iswoD0NgdwPgdyB0XMKmsdirEu7sIdK1S3VrFOwJ3IFgv3WXiiasD3DKe98KJ5Ww7KBWzXQ+vJETBvA1aCMrChUTZHSSPMsVZcu/8nWG+gDth7GLBskLb5X2wxTQaVZXpLxrKNUxao2BBxhJIJStClw85TP2U01svdRCKqjB/Uo9vWjjOJjeWRlIwuvWlqmCgAQVMhvlVOgO86SxzrlYVSO1ygAy/pSSarSZUukgRSO6kQpynLabCYmOKNZI8l4Y1M5dDEooo1MVejxq/44YuK3Hy28F5JyAcJ4FlNWKoDOcAqLP/o4OYrvQxDOHghaJxZ98zFU0ZKAOK5CuLswfE1AE+E09yYE/DIkj4L6GBRKH8Da/iYEvwVxszCqPrJGW9i3tQjtdifij9kegsxPB5qWcZZBPs9qsW0Qm6aXeyvy6TYtoKsXYo2qpr13UjtT5JRsA7qq6v13FILqjJmm3THzcNXmV4RWxHhlqlVT3ticp1RvSeYEY3kmE3FGeYhpPBlsWabbvHxCStHm5hVIeV08U2tKMz29DMlQQom/0xDEk5yqYhg6gVGS60jue8K6SB1XeQIuFWikwOkrnRG9uwfCCWG8q0fkWw+LfBPC/vl+ODjCfPMMRtt7EP6DxzAtxuj7xhbtH2Ulwmh1v+UgzrBhK8dYIEzHILsUM+D9GGA/3oF+AL/KyyANKLL6LMezy9crsuUQtm6B57ln0Z+gL6G9RSgVdfGMyaNnFHVpyEk2xyifkQWMzgthMk+rcQzIzj6rFadanM3MwsSwcYhLYEFsQXBctFqAbnwt2BB7DZ5xhBuCQHIO2Mt1JKasPOfFaStHSRoinMKakq77mgA3HsPZZEjLZPxOgZKHFkVTMAy1Q0Cn0W4X4c2Ym57HXsdr7+MX+OiHk1PjC8BLqwLa/XEPhFsyXZA0rme3QnAPQePLPdavQWhvgJbNGFlpkfR//4M2lICal+SSnHe3Q6Y/Evnq49AxrdInaEKBCgXO5UwvBvDth0V+9wmyAkk//LLuHyagb+GjuRzwOvusVgfV4iwFFiaGjUPC+mz8dx0jaoK8RFA+pgYjWPYZhGcMZ+HiuLjIFiaGjUPc1AWxBcHJfJPltZAuIvteNYGThZmH0DaOAnHLLO6uoXVSMK9CAKng4ehFDSzDOUfneS+OLlw/0v52LtaS0yCEMymQ+Kb5DoWUgjkRv1g/quGICzzmQ7zEx3VpOwSY585mcBqMQ6IcuhZjnjkFaanp3QWp24v9UloaPIMRdzFG5HcgQTwd/iQWjtOQzlg2kDSiWbcY+qej8CCIpC3I9r6lEFaQlS8HU8zXPGOftA/2FZ9hKXwTxX0MWVCJxPWvz/EAh/ks2vnWaYvzDbGn2mQqtVOZY/Jah6BWy79sgxzJVN7Hbde41ajx48rTNqJ0Bn3pXAjIsWAQoxe3Qah15eag0rjijyPlBQjOVfwyjgLFPUxqZrlOpHJoHKqNo91MrCtpME+TP55woWDytAu9VnMaqs6EFXCK81BOfR9dj1ESAki6uOdBd38U0rkYRSnguyApf4Tl0ZYDWMNCgJ97ROR+pDmJkfUkptVcoy7t1iO0QqIfyvxCDMYb7xd5GYP2p0BBDfB9S3SfwP6iqC4ppAeAfjOyPoNBn5aM9yLdFHgmbAqVPgXFSwWXrqNU4pphY4u9DkF1a9wQlfVebnvwN9vD8qXWlRZzqqhuz6jiDYIUbKobyOGcw78hUfbbQ+q3cJ23n9CG6WRuTpYfwKG2QqhtxZQ007pyTYnpptW6XsL7RU5l8XuOU0/8VSBINCCgv0yeXKEdLYVyJv6mQqAmQyjh8WsYghLuU8a8CSlEOShgFMpvbtQKJwoAF5L4U0o2blxSEN/djGENQko6vr0Bgo2hrRlSSEe7V6GoogKqCwtS9fg84xT1bggYFc2vYQr88Va9nF7bg6KhOONa4DsJozCLykfbVVTk8BntoPv4aaCHsD9xt15Wayj779i1M0MNfor2Loo47FPYyK86BNXtAVmAXIQqnh7bFzsWiU8moCoaoVkEAky5I9EINqF1vPtvzgrdO/t06XwBr5LYdFnGOluX5y5tVv1rkua0kXaDywRmFBmtq0YDOG6LUOt6mQJ4Tg8PbPx0oXcYrfAwwvh0Y6TqmafXgvdAe0qfJTMhnFxP8jAn541EOgKe6QwyCvWn+VfVIFzQD2PamikBkUeFo/g5DGUv/gELRAgqnR195ynlLVt1DNxf3X8IdEHCVi/Sa187PDr8ZK60jXh4lfbR+zZk+13MlrdhpFwLAZ4/tyIz0d9AUawO6lCJfBzZ8oQdd4xWLoeQYvDuRl+gntvUznRmBTwjf9A2wm7ClwFDbwN/6hJUd9YRanddmnQR3eLiHZKqGoaqSEBEQEHRs0pxap4o1WcQZlAyuixdGlbjyrqckLaMRoc2r7EYOqh5pcV4L0ZC3sFwlsodCOFpalzxewS/bIXtaJXUuM6BIK7oMaMkhhY1SkIoqZTh3Q1U9FAIMNoMY7jJynQ7eMYpOIXwFcxXN0FR9Bg6jmexJl25VNFV4Qzh6DGtEeZIv3C+mfaSo+Sn+XWaMqerK7GkpXf7fcfhJR/rVs6m//CBXl6zuMOUVKDmaoCuW556CH1AjzUVtDj934gdKv8vvp2lqR1daF2CGmaViYvLMQLZBh8kUGD8JyF7kYgZYezvr6i2a09gaDkJM7QZxQLM/PzQOG0WktGWp+CbIpsNlCMNjQVoqcPpKf/UmhIKmTMXtZE6F1gUuCmY99ErQg9a3WoMHxwdOd+jkofhXMRxa4TTTY6Y9J8ZuTtwJv518Iy8dMHL8EzBU3u8FRLEIY0jPjugZQu1tRGPwfDhGnnvQQgnmLUEZeOZVJU45JnTZSJuXFtF3cw2BUVfBNnmDtJZTDSuoX+jQtsOyNRd9XCJjP6MPn85ImeYw6pTEWGgbmL1trMR80xzRWUaVJGNacjvqAQ1ZpGhKRnhBJpXNTOEHuU82jqFkXvp3FVgOx8YrKjl2mf70RawTuEJKs4CddIiliQzRhsCfKJCvQ6FMNS6qnUkhI0CB9OaitK84u8yNKRqHYnGzO0LwnDCRqUMs1VrSrQuKnO4nqQ3L+UFAa1tMv6odbWCGXhEKK7JmGdOu0gky+FdTqS54mBC2Ssc/Td9jqFuB8oN6fkS5pzjIDV3YX+Uoz0fdlbkx/ZDer1MzS/LqDJI5+3ymEI3A6yYDrSd7cNgMQ6Zo+/j5VG2iiah76IlE/sti7GQfkTELcGBDhKm8aTpTsM6PPNKrMML25lNNorfUQlqtXxjBvrQrByaj3KfjMoDtl227eXobamG70f85wdE3t+CsB60dXToFNSiJ8uvKGOzflSNjYodmtTxXj8KXKZ5xWjJ7RAKJfcTqY1lPDWuFFy2Kgoya5HmN7MhkDP4h4bM7RFqX6mkppSDAAAgAElEQVTkobaUrt25/gNo9UrPS1QPbBEfisNj7CqEoyaNI975GKrZXbrjeeQurTTiyI+RdNi4aKiw7Kcg0DuPiDyIBSRHU/aq9omzSJJDfnBdyj9UeZS+Zl0msTYgMKI/CohobUCupVA0XFCrFM0jiO1+5zGsUzbpXlHdhQlqqAil9Rs1hp+gc6frDe7H07VHPAfKc1PC4I6cHBVoaE7BZDgbJF2sc67FUx/XIIz8VXauHDHNqEmb18sUWMDRxIbnqbgWW4ohnQYDnL5ON2tKCiSnrtzHjKSRtJngohHdcKQszzwG1ugCYpyWQIdnVMeew5T9d++hIjDdnY9ybsR69IF1aqpuIZlSlYYWUAexwLwAXs2H8qsd8G4ZdCUgxGFGxBfgQlhS+WKmz1Fd+gVXX3H5EkARXPVUnvpBoYt5pjMHntQMLRWWJqvu0NELalB2W1fFNOsEnPJS+Uknyv/n8+jIMQC9A63gISgb6AaS65jde7X5qKo/Nx+8h75+NV8NYwm7E3Nm7gdwqsbhmYqeyxBMzrNpUncO79yi6IbQcX+QNw7xRMi8lVDu4LsdI+Q0xGHaOjxxvFS4jqz1MF88EW0hM0bIMz/7oq0DDRXJR8gzdlx0l/Dbt0T+n5eh1b0PWzVPaY8OnKLjsXXJF/VOK3vegbimAx0Xp72YPZiKyfPTdGXpNTnevwrWJhhjnuV06UwD1sd0uSEhz2ycW5+G/rijThR8FEEjF1RViKBjsSOHzx2fPNNtcfZJcFq7UYHAqw4eXIMlIOTqEEZabLGpI02qHXAZqHDm2leqKpJMJ11cc1FDwRGU3giYmOtEjoAdGAVWLdSZU3lDYVULIrzTSGA+egjev6CUO9S8QusaVozbyFQvQspy5UlV2lyeuSNtCZ5ZRupyuzmaGMsQ9avpcmnL6OJIevCwNh/8jw9E/nYD7Pweg7VRd276E9Yl1bH0/7kLgvowRly6uFfqWhZI51+TLk1MRtudy7O8bZWuS1U2cCDslG2ljfK3PkG1DYGZ5vWTC0xGpAsYUGgqlS46KCdDqH/KFOWlB0ojGnN/jqnuKXT2PWg3h7EcotkpjXGYqUpucPCHuyKc5UKk1DKRcdcrt+Ts4ma5Mm22TBzslAVNc2R8BUKa0UfiDZKMPNDcAkHlOpOKn6LHTeYQ49LFpJkStBrPPBpq88ySpEkI6ddlVzABMe5nhQzbianK29zQhNA9/yCsjb6E2USndu0HmhQlIX+o7ea505NYGtASifa+AUxNugLa/mR45laNfQfxbg0keWYrrAG/9Qlqom1kvPeIKQB0YLg7QX0MtwtPYiZKvQzvH+mCsO49jDDoNzbcqy3YxkGRlDoozA6M95vsO6EF+UkoKtl2bg7flO2TLsrWtvMysXmCLMK8ej4UO1NlIgS2TcbLOGmrtOJsc7O04a8JGlv7XwN4qlFYFhSwIg6OQxpGCxGRWVxnfL4b2y+bYVlwBidioBB6DiNp9wJ7JEVlmaSE63gqknhXIi2RaKY4iifZLd1pPLPlcxmSZE4Bz0bBnzBpfYIaplbfSZYXBlsUFMoO6mUwCrJj5zJxDr6V0TWYcR1LSdqQc9eDW5BYJnoPc+VuCo9VfrZHD4QboKzk9l4Tds9bIYhDSHt28LJcGLoiBwdPy8QKBFRacLN1KwR2HL7HyyQI7gQVzv9aYOOOP2Taiv+aYfROcW4dbsEuTDNiINRq7I5pKai/ENR8j4BnqSSpMOQQBXM9Sm02R9K3MJJyz3d1j1Yc0fA+HD1Dqqmip/3xSfSK92JZwG2ozPu1zTDKtXpZwzxqfhfgLwhW6FJxqbAC0CIUNUkdA4A6BBXrQ6xH3DrVZY6bqApHcMgT+81fCmQXtPuL0Jl/hl0BmozehzZwiYpY/A2jbR3GSDkLS8r53J5UgppjZK7XsQSlx0oqbrnGtZRMb5oqj7atk2UtnXJs8Kwch5AeGbwgp4avyMXhG3JZbqh17BSI5uSmcRDUNiOwbUqYJ2HEZdiEJnzLBMBMkKmDE/E7CaNxmxZmNRJDeNEpaEPxmA+kt2E8A/oohyhA88gxNdDabpow7tgn8tLbWqH2ONaYGx6S4c6OrD7DurJtTYVT9c4D5zxF89jdqgd1D91XN6iwRPo5FGgYVB1/sTwL6fQ44S1Di3hmUzTytw5BtUJK8vjgW1EakmtUBAjWQu3AZyn1CxWtj64yysT9EEhMezmVpTOCAdTvoZMYdaGI5f6qXaNmueGF+7A8XcEtUSqempp0LEmaDJGa3DRfups6pLf1hmwb3Cdzm2bKxOEJcn3ohlyFwPbKNbmG90uVG3Jl+DqcGlyXk4Pn5RziKNAX8HdpuA8jcLN0VybLQqTvaJ0hnZWZMq+pXeY0TZd27ARO4fqX/DDlY39iG2/EA0ViVgqTojzPbBbFv4H00tD/Y6jT/+N1vUf6vY04HIo1wuyZht6cZ7kCKhcuVSpaXh3DVLkPI+vSHuXxwc0l2V9EBIZQjqiSBDx3Bs80nVkNOXVJGvP6dNu/itGFGKN/6xBUS4FDkHoN+8Y83jSBjPRQKcZ99Dkzh+VrD+trEGhPwBMT1yGAzdgNOYGtmntXa2MfywfLRjbtQ4A9B6FuwxSZBjKWWUM4htFHcz9IzDispSZjVFzfvEJNYyuY1g5VpsGsdBi/+Bewg3gfxH9D+G9gaEBuSr/chELqptyS3qFbcnXoGgT7OoT2mlwc6JX3hs/IGXyTpiUQ2oXNc6SzeZZ0QJDnNM2QaRBqjrv50zieaZyOoLsyr96trSs+jqKne/djve/FdcbXoDRahXUpbYuzx9Bm6pLBOUoTxwMFNPrgURhaXZnD4fHsyiXGQWToCtuKrUs7rDtcYkECEkfWznKKSvAs4grSoJA+XQSqxjMHSQNfRyCoce5xQXRZovBgJGFFt0H9yy1Lrjc3Y3bGE2EToHTlbgAdZC3AlDhUJHFqRMOh3YfRFiHMszAS8wSY3bzZvXu3bN6yGS4mB+WhBx+U5cuWyaQmZ5HLA9f2AQ2sQv5LeoebIcAIGKwYwcVvX/OA9FF4h2/JNSiqLkNgz2MafWmwV4VdxBr4yNBZvA/IjKaJsrh5rnRB0zwPgtsu05UCiwor90mam4GAWjzTOBwoN4Hhr7JL3g/m/OFDfdFTN4wTvoR90jVLtbaW2yrmcY4bZGERbfMwrXkYCgB78NwriUtRQL2tb6feM4hMbtw0jjCl8gh7esLU4FmO3eWZk4/lWSK/fOvJpyvJs1T6BoY1RFDL0pPqm5hW2aKDF6cwknL7cjrWpFQwzsUaloZBrpWazesgRlN6uOPTBhmkfy373ELvf/z4cdm7Z68c3L9fvvyVr8iKFStwjHMqtoHiIusqtP/iEABeqTZSj9uO8M7Rul846vbJteabGG175ZJclVNDF7EWvghBviH7B07JQTkFL/GtsqR5nixonq2mydMq1DvTmIDLCBdxTnv4VsSzEE59sz31YeFOIX31feyVYpG/EEzcACHl3jDtkIN8k/hD2qg8or2vc2igLPWaznLQiW7KL2ZIV5IJirsFMTY4qNQi6AzMx1cbfxHCkYfHrbYIV0FnFwb7nuSLkOXhNj1HUO6d8nsqRkcui5ZAIak82DkDIEc7bgXSQIaKx1nQFisjIsBZBs6f3yF3rbtLLl+6LFs++wx29Bdl48aNctdd62XevHnYWYD9KvKpVp3JeARyZKSOmH+c4g43YTjHlJmCewlCe27okhzD6HoACqx9+LuIsAODp2R28zTpBGxXZZZMaZoC5RUxcDLODiGnJJlvwMYkjGIMpuO0FnnlHYykh0TWYwTdiH3SFfh1fZjUrhYFkdUl3THgL5mvgysZnwhUQQnmq/AEvKKFcU5e7mu1uIIkyeAiPEXhSSRjFNj83/CUwh1xSZPvB2P6mNVAQfGCYKbnjIbO7d77XNu8c2ZGO/lHMdtaiKkvvVXahwLNtelb2AqkmS1tgudCVtYvtgKNQ8qToPjpWijd3d2KnrfeelsOHTykNb0YGabAcL4FJoEFRVJZheViiFZ2gFi+GCC+sYPgts4UrIXnNrdj6jtfVrYslBWtHTK+qUVODV6Srf1HZOfAMSiqLknvcC/Emsb9nESxEXK9zE/Nv6jHTvBME2AiyEAaJBw4jMPe0Oy+CUY+i03or2JNuhyMqeb3RCFK1VVYl7qcOl/nXydpzjMG8gl4lrMtR+BMZ1W0z3gE6Ax83C6QUcTlGPVbqkgqRkfk0Xgz1lVx1ohD+4nCHTx+toWZhtTV/V1+RGURTUPSuZD8vEKyMEVrzgxLkS1s1tid4rOh9t6qKBv5y5z+gioee1uApRENIqxOnFhpnP/Bdn1gxVo08Wq/VqwtdS1rutowX162fKnMg/H4ihUr5Sc//rH87Gc/lf2YCn/nO9+R++6/HxZwUC5ZApkNPnyVhV8+DauA9KOi8Y/tnMwvDSk42k6tTJJlTQvl8ebrcnzojOwePCpbBw7Lq/27MAVulZVQQq2AUPdgPTu3mevZadBUT8SUmVrVvIHEdJl8LSFUnO3BFOPXb2DKu1Xk//oGRtJH0INhumGP1CWGsLw+3bokbn7jz6lLm6ONzb6TPLMMsqwyPLPsTPCM+OxTvZ0Z3A5tGVogyKomostSrCE8CvkR1iXBR8KzvBgNfSs/oiomsPSKQ4oI1Zc5o4sKVFE5M/iWM8WNUdCKP8RzFsohTmd7MeWlNrgdoyTdcPC0WMWsjTjqfn4QAwYUmfRKRy8nfKgr6ZmT06b7WN3UWzFyzoWjLo6u7BC2bt0mWzZvAdKKzO/owGDTiras16M51abeWD5LvW0NYWGcygwbDNPS+KINgjetaTIUTHNlVWu33NfSLYugHabvhqND5+TDgX0Q3q2YKh/GNPm4nBg+q9a9fVBUDQGqBcovGlpYWnQ+5ouq8s92wLj+XXiSOCXyo40YTR/VRgme0kjzyic/5pk7W9BFz1Ok6jLNM52Xbvx4tygsgio8Y8qxamcGs63cuGEGgpmu8ho8M0Vv9E/pEVWJp2K4W3EI9QqnoDSNzms1om3d8Y4hupBVihzIDR3k0XrJolee6bDbsB2a4SU92M6D7fxxtEsEa62wQ1tGIV6agGwyDPJXr1ktE2CUP3NGu3y06SP5+c9+hovJzsoTGzZIT08PNM1QtODRZCemU3mxveK4GkAfxMSQBoyw4/E3DuvSKRhLZzVNk8XY372MKfDF4atqXXtGrkjfYD9MMfpkB6bInw4cVJycjpG5E0I9G3+zYMgxE+vbKcNc44I5tBjathPH1KDdpdvPJ+/X1kY8jmfOkFpiw80NFV7AM50m1UwtfzwWjAnPGtHO3CaoimoDUnWZaK8j41kKuc+vkXyVFtQ08ipEVYkKcXE848n/B2H8wPtxuTeqfHoZ6njS5gj223lJEJ3jUYk5i/umyIOnz8yprBCt9z0RdywsX74ch2gmQ6s8Xd588w2sXd+SixcuyGOPPqoEeSaOt9nRtSoyJzIbcUskIEs4LZ7EP2wXzcLWzQA1yDCqwEaP9DZfU1s/FyC8Z4Zo+nhVxR2GMuoILKy43p4K44qOygyZPzxD2k9dl7nvfyRtUJrJemh1N0BxxDsJ8STaXQkKRwZSR1WrDOrhmaao3hwKUtVAE/OsRoKRsWtEqUoLatS7xKVSVcAnGVWNPCSjkf49S7TdOAXVejihS9uTFzFwHNb+p1dAE7wKZocceXmem7M7dSzS5pvMXNPFafDChQvVNs3s2bPlN7/5jWzevBkOAs/JGYyu999/H6bJ87LRtRrJURzz5ZPVLZtjQEzwSQjaFNMueTLEtwJFFLnXD+OLG5j2csS9hD1aKqBODF2Qg5gmn4Dt8lFok2fcwP4z9rAWDR2RefevlCkPLJa2BVOwZ3sLtsmYzitCDDGKNrxntGlS+a8iqQrPckj9lkBhQIpjLI4omzI8CwlQ3zovha92tkkMYaDRJ+XBEW1+VD08C/MayXf9V1pUKUBGgNXkmZqJKqgKpZ/shcUbZnNclj7/hL6l4VOE8RLbTiiXnqb1mzGsoW8xwvFCIjXTq5WRo2Hsw37jMey1/ubFF+XVV1/DUdRWefLJJ+Wpp56SJUuWqC0cjq6p/U4/m9T2d1DAWnQR3OFZuOxn9K3hfgjuVTk9cE6OXzwgBw9+LNsrJ+XmjBlwGrZQFk9aoPZr57bMlNnDM9X2T6ta2/IwgV3h6rW4R10t2hyekb9Rg65Sl27U7eDZSGU2p61EXaq6wt9IMyvJrxCstKBWpU1VpkVdXIKqOAwD9mPd+RZ0PdshrPMxcp7ClJdHH+9eCaN9WL/RQD9zwk2EfJBlYa4ZbTHEECx4euGR/v0PP5CXX35ZduAmos7OTqUVfuSRR2DxNCs3kLB5mfxsaf1fhyC8Vm3UI+CZakZn0GO9/ZHIv/9WTj+3UnY/u0h2zR6QvTBrPDRwQQYhi6thGbW4FWaNsEeeV2nH+lbbI0+sTFDKLTsMxRwxpanCM1vemnVJwNI8M5SU5lkh5Yq8mrS5dDFBITqDydBVHRRACk8hMkXbSJ/SghoVv4AbfrDzZV8L0ukCDMu1mxXZD6XR1oMYLQHLu406McVdgFkhz3Sr01URjkQ+AUeiJAbJEBQyvfCfxG2bTZs2yQcffCB79+6VDVAycYRdv349psNzM0WaiyfGGVZDAFGQwA9OlMUG8XLhdz/FzUubwJgJMvyj5+QarkrrnTCslFJnYRnFv9PQGF/GlPk8Dhxcg1kjR9YunMad2zZD5mALaG4z1rewd54JAZ6Evd+k/6IUm8EzknJ44AT2fYeBg9ZW6EUL26Ym3C1fxAITQL2Y8h+Hpc5U7I1z71xNa6MEtqW42VbhWVgl6juHt2/JbJKBBmG1uGSeowssLageXVVLV42g2qVTBjY438yLyWiySney9ECofFNnM7cCPCOgyya5AYOB06dPy84dO+XNt9+S06dgAght1urVq9XounLlSpk+fXpyKlxU4obyjJvMH8LK4wMIah+0a889jkU9TizgoC6PnHFdewvr2uuwRb4q1+UKDhKch/BSMXUFp4IGMHW+WenHAZh+uVYZUGdyF8gMdaCAe7hzoKDivi8P1Bc/w9BJD8ort95XeNe2LJZ1LVgmwMYq9RTUkgfK+ubx2D2w2WYHzet32ClTF0FjFzoXKBLWVJ46rEzOYepUmlSYQ09BdIi5Ed8jE1Sbc92E1p2goIwFeAqCC5BEwZwK34DnwoOHDsrHH3+sFE2X4PmeI+o999wj9913n3Qt6FLKJru3GyFxAkZJjsbEaSjNsbbAyoMuG3nV4YMw2XriIe0zOKlN4WkgmjViuweC24uTPtfwdxl/57EVdAJ/FyDIXBe3wSSS+7RzYYusDhJAoTUToy/P4Lbgv1BDO4AR+rX+TfJu/w5Z0tIhT7aul8WVzro6MJdnJ6E4/AjnkXcdxAiJjlhZaKGDpnkxb8tYBL0EDVsYx10A7qtT8eieW69WB+Xj6qitOkDL518dsrTBQ3J2k6m+SDkfC4Xv5AIt0oMijS11udJrKOSjXoI0SSJD2hyGhMnR6Nvg4GzOnDmyePEimY3f6xCMffv2KgN/aobH4QQJFU2Eo2WTfTxUhrawkWtYCxnShe+QZwSh2vvQUVgcwaDhPMy2uA3zpQdwVA2nFVR5U4WGoQX+o8cKerGYVpmCQwEzZUFljvQ0d8hSHKhfhHVse/MUddzvGNa2x6BRPjV8Qc5iW+gi9nQHsW2kjgEqRnMvWCvWWKZJOGx/ZOi0HBrE8TfUAY/38cC9flL0mKjgh54oKaSfwl6DuTyICQKdrdP45dARfTMIPbfyAAZtOQ7DFpwzLfrI4kjM88etUP+n+irL5TxLl+8pGv2wLL16CbBloG443/mkcOdUjPSttKAmBSMjisS5BFobz5is2ETPpgsYhd5ea1x9TZyCAk90mzZpUszMsg5py7uGmKd5ZY4fP0G6YC+8YvkKtZVz4sQJeeHFF+T0mdNKQLnFMwkGFE2wdcxyMMkz2pJ0ueWtwTM6vOZFxS/+Tl8ZcD82mmka2DHXlK4+nrFF0zwRnqPUGrULwroKVlKrmrtlXss0jL798ln/Mfl9/27s457F9PaKmkoPQGB5GKEZ6fVe7mR8V2QnTCJpvzyzeSLO485We8Spzsk3CdT1yTrcAWF84xPt/PHxu7WF2VEI5GlMhU9hGnwWAnocfcEJ/FJwd0Go9x6CwCKMVmoojLr+NXUbSN7ObJ26fM/bZU5bjXaWyaHFRxx+/Y2VkKqcQKglIae+4M0lUYFkAVFMAYY8OCs30eCDvaL99RKXRO2BFX/UpMstFunhwXIemzuFNesbb7yhtMPXcUrlgQcelG984xty7733eCNrmEFEfhYQxfhJuUDnWb9X3xL5f18X+V+eUkb2w7jLpuIc3G4Ez9Qhek77K31Y115UBhZ7hk7Izv7jMGs8h9F4kqxt7cKaFB1XU5fybnFt6CYE+hN5qW8L7LFb5b9O2Ch3V5aqU0V2hCuiDSWTm5jB/8vv9SmplYv0eeIPt+rzxcuxn06BXAit/zMwFeWtIHT7RLfMdNu0DxOMoyf0lPhxGGM9hpXAHGzZKbvw4CGX+VCk7rR2FtJa7bv8iGoK6yLTI0fc4HRIHO6mJeOyPs68xFMY4AgDERTOEInX4uJ7Hu/TkH+Zt2RL8uniCEKl0iRc70B74c4Fnbgrqk+2b9sGu+HPcXigRebRZhjGFE3RJU9xH1uKZ6TrJPal3sA2zM/fhICitX4FyiPcnFbhSRiWt4E84yhIzxetGG0nYQuH02SeAFrV2okRtwOj7yRokXtl28AR2dS3F94uepUmuQ0CegMr4U39R+UCTgjNaZmMaXaukPKrzvAcdNPX1Wa43vkAurElEFJanm3HNy3MvgTBU5cPQOhWLca23FJYrUGxPH3KMKzRKuoy9G6Yj7bj9waW7vQy0wfUPJhB/9DGbFvxSPHJ/Kn3BvKM+Iramcq4wU9dghrmrcvtioiGKApnbIFshKjNNzH5wkbOxzm6yV07XR8y/9JvroMuH4OfB4WV010KK/dWORVm4z569Ihs/Xwrpr/NMhsHY6lkqmWCWMQbL5x3wfBGtffRCnk56LefFlkENai9Ua2AWzp45DxjmTh9pUZ4MgSWWy9zYWPcAa0wt3SmwXxxEOu1k0Pn5TBOAw1gJcvWehGCOw3T6dNwIMf1K7XH42iL7D2arkFoi85cgXnyH7VQUSlELT8sO+WhNbg3tUcL5jysVXvglZQOAQjDC4+5ZcNjjdQE83J0btddQVqe7oPMqiuA1LlkZPVFtrOq1TPCyFEJ6gjz1IxE4lxwqmEqB5VjqA/ehXa7hCIsVCTNngULIGiC6UJ03759snPXTtgQT4KtMCyC8FtLWKuVVt0W9yEE9ONtek/qW09C/blCDTdFHUsaX1EJ0tCp2qBdE92qUmB5KIAa4bnN06GiwvoR69dz3PrBXi1Xd5MhmOfoLg4eLsZB2KdiupwSViqQ9hzTFxrfgyU3XTHxAoO7oSO7q0dvxU3DN6/4ob13cLZAEc+JC7W/7RBMOl3n+WQ6SKSHHbqcta57lMACvjwnykNqLtYLX8T72uGjEtT6mEBitIKo3nQ5fPBWgCgMDr8tW3w265G4CNZlJUfXmTDdoxXTBAjmR7BsouuXqbirhhpjtX3D5pGox0L8HAJ4qdUnWKjRiz0Xcs88iAuboGWBd4Z6hbQ+nsXaeJ0+LwCFdgJEcDaMHBZg73VG82SI5XVoia8qxdJxbPlMw5bOadgiUwFFd6vtOO1D5ZV9WETumf5xN2b2EK6nUTxanS3EyLmiU5+WUmzDHwXUriRSPCNlVLrTnJQHMw5gH5ZeQegRpANXOjYjcSqdW4/+uwsfcC+JKMafBCvOsK6YugWVxPAhoxLtEKFWexaQrT6ZInGETKNUcSmseT46PWEUm/xFUJY6pEt/u1o9J5/stT7aOB1WniRg5D8ZHiN+97vX5fy582pavKCrC2tXtCLz1OQZWzBdWuzYgzXpH8AGSCmPrD2CP+XnSJc4Q+i93H6e0YUMFUo90BovaMEJIBhQvNC3Byd9+rG2bcdI2yun4UNqAd7bYf1kH97VtQ/a3E+xJdzTBX9pazFZwC8vkWvjZUNOfbqlCutT49MQTbSQmlJR9+zS3PQqfpd2VjBFtrsGKa6NJc9S+Y0+LKEnK0KqmxuZljHOtkAlIvaxCvqAveaTYuo/dqeOoRmQAwLcDnp6INSPiyenTcW55GQf7saBk9Z59Wlz6TK0eXjZrjDlw4j61a99TX74w7/GPus5eenlV7CNgH0EhxBmkWWT4hkdXPPSph+/pjUrG7BP+uB6rR1RxXHLqguYk4I4foRlrsqzPEFMl8av6a/Os+lw1rYYHiyWtXRh6tsqfzv+AZyfnQgh7ZXjw5fVWtali1rbQ9x+gUCtg932RIyEuWxaSjThXolTPANPNAw6TFiurYWmmOam+7GNw6n10BfGM825Rv+bz0tqYg4bCxKYIPIxj/X3o0K04d4xU7pVpN6drNT46+ZjEPh5Bhi89BZddbpUth5St5Gasd7BS3jS1oQNd46ozz6zEQfRT8NOeJ+89vrryomaUjoFaSKeUUgPoHX99m19TcA3HtbOjHly3iSuybMEXdV55iYwhR4Bz3iUDmd0YMcEX1H4m4CtmafH3SNfwi0FVDp1N0ER5uDlPbd08zqTiiJYHLW15gzP30KG5e2hqJ2xrByVl0LfxqO5vGeZN29AKe11BBazwjNmPGPLaPxTx4hanLnfpF12JNJkDCK7zGNeoypCuBum391/Q/yMc/DiK6dNp3NjfUgFrNNzKho8EW0WHL9coy3E1s3jX3pcZs2epVy90Lif+69F29SKLnX9ITYF38c+BYeBDeu0/e4c7D04Vk85E8aGZ/HcwZY2LnXIGULwWg/sniqOYSGSLEMAACAASURBVDNH1jUvwd9imQX7YfvQ6P4s1qe8rha7TEqbS02ufeKcGFNUl4yzKTQMR9V5yI5bNNxr5daNrsbbzbOsSA19aYigBvxsKIH1Igsbkksbq1arACKo+rJJJKc2eD3ckd53373KSOKll15SGuGbHDFTj3JbgXngx5/rW7JohU7XFbQ6MnulqWS3JcwpnxYHBsQNnrTQzJDG/r1Yp7You+EWdekWbyfIbwsYVi5eaddLfRm3XXjIIhzV4rIF4pvgu01DRwI8vMEdLF4az8vRqY+7bU8V2hpBQ2MENaCkqHdUZckK5ECp16BfJ5wJ99D7SCIe+KN7FG2RqgjbCDOS1AtCoxbk0JbRRVindvDKqe7DDz0s69atk+3bt8vrr7+GEzmn4D4GpoDuQ6sjrGflI+yVboYCiX5lvr5Bm+J4/ndNvreRZ5wB8GoP/pc/5JTmlvoX01r+R0fkdB1zEjbCAwibAgdunpbXIoBegd4jT2PniQd/OjBhUE+NurTJi36dFqRAWtGa6b2SThlpfalHVAfKtCen1jQNJtzLpwZttdtZEdUjC69jjWozsMV01NMoaB4KOH5Ejd1UNZmi4n08jPUYny3MfDW4tgYJq8ilLVCbG9qyulAvfvrsy6NN49GYHdq8BWOel7VSofXS008/I0dgDPGb37wsPT2LMkMJaoqHIaSVy5j/vblJ5PfYhuGdHTRoWL40JEtlnZF7m3hGf8O92F5hxnRpyoPmqvRefeJuHxg78MzrzkEYffQdwFR3Doz+Z6vrO7y2oAoxLFfhOgYGXer6TBom8ClTlwqOKPiCf7L3LMKvSwKQVJ62UX2NciOrE+u329DOTI6N/KlLUDWTDGMMQywx6r4SxQ9yKdFRuRVtuU74AI/CoTLKKyCXDbu1oylxay2jLYFP0WboytAn8lF5W9oSeEK6FLjKOKeLp2qWLFksX//q13HiZo+88ttXZBr2Vx9++GFsxI+HkEKb8joUR7/C39rFuLgJpoGroLIcDc9IgyIm5JmO0E2zNs8IcXjopPyu71M5A5PA2S1TlVPxSTiB0wpH4uoB0A1cnnUR9+4chFuYS7gRj7fcfaftIdxyN1ud2Mmp0PCki1NeCg6N6Cdh2ktEHl0Gd84HgyWoh6wuCR/wjKM1rhuS8cDPe4tgj6KBDG6vrzHsMBVOiKwudSrmX5tnKqF5PNrciAa81yWoWQVkhXSp1LFRlFdrCYpVfJAqSKN6XgViI8yvM7ppNEGt2uyQzubA3+wpoM3Lqga8po0EEhBND788WUNjfbp0efHF38An06syGcfj7u/slqYPt4j8AkJKI9anoeFduUSGsSb1SCmgy6fb4VkC3qdL06bSV+EZ0dDrwz1tS+UcBPX6UB8M9W/BqOGK3MItd7zxbpAX14JaUrysZY7MgWnhQuynzsNVHbxYi5ZJzRjO9HUdkBRDG9eMHOUmQlDVEtxOQdySG9qy4kSNiVnr2FQUj+1y2osdM3WDhyeYKlnjeUas2eNl6MWM+qMuQc0JcvINOGb5UR/NiZbmlp/vLkiWZ5CuSqYWsnpOOtMyMJY8BRskaIGxwyysV7/85a/geNxJ+GLaLi//+09k2qKVsmI3TsQsxy7/Mzj4DSGl/Vxt2lLNMpjiO/zKSKqTZ1yfUoPLUZRH2q439WHs7JObzbeUczV6keiHwPZBcURYHn3r5aVYgydktxxTQkxFGk0H1RE6HJ+j3S8fjnYkh1dkKjewEZMTZUzSrwsa8mwQyWkzTGGlgX4bprzxOrJOniGr5KZelXamqWv8vyMT1FHRkagQxZBE3VXJJ8nAKvC3JYqFYFlQkS0YNujN8JswhrhxHML63gfywmf75IfdK6T7G9+CSQ52/CdjL8E+9TJgBAWqxTOuT4/DS/+mgd1KIEkSj8Cp5R4OaQ9AODmu0mvEjUHcZgefw9dxNO4WBLQP8Hw40lKAV7bOV7e5L6sshPvSFhyj06JF7ax3yMjhWSS7tcro8IyGXReo7YV2eTaOx3F3a9TydBvqpFYRbfzIBRWFULOXFHdTYTbHbMqjAzQvEr2fZZL6NbWJVwvJ3jLJR0uXywG7Rq3FFQehpSvqPtxMHdpcwwwWn1k24Z8HFmDbpWel/PTzvfKHiydkGCY032+fLPNxeHIc4jN3pGPGs5xPtXjWj5GSHh4+w4VWvMRZi54ucPMQz9XgYDhoprZXXQCtLoGGUEAwp8L0oQXrWPojvoS/gzjH+tnAAZnYMh432PGge4uyjCRfjDo2q0uOropn+IvYgECvnbl16QBTq3yWWmWMqLy5PjxyaOszGmVtpk5dkgobXItnHr1l21mtdpiIr09QXU4a5hJnzGAnhK98bImCmtCfUfUE8Hm8ejOfznIrJ8Khi6CKkqw3CSgNCXfIqElXRkcCEngr1GpAcdQMFwYP7z8vLYtWyD/jxMn/9+4b0t81T77/gx9IT08P1lL2ONgXzzPufd7fukoWtcAiAdrpYTLYMFlpf3HYswk+UCig3D+FsxY1wvbTYRqEsxdqpl/c/FDmYi91KhRQL93YLFfHXZcfjt8II3moejE0c4o6zNHVLl9dnlsWuPXi1KcK9kaGHPAM9mjpDIM31XdCkR4eDU7UEmtwjNqZRt3If+sTVIepORGJ0ZCSZHmYSpPx16kRt3Ic0XeDo3cXdyqfvM825BqgmrR5xDg9kR9uvwJoqB7RrZ/EPukrb4q8A4MGnOFa+8jz8rdD8GL/i1/Iv/3bv+KWxBvy/PPPy5o1a5T/JdViStHltK4sY77k4S5dDrTptAyoG+EE4dg3DBYwJCmNaW40kDV8zoOzcJ0vR9/9Q0flpzc348b1SUoJdWTgEo7FTZM1zV04qdqqDoNzwKH29xb6MJ54catM0WwDStUlqdA8G8BIfRhLf25XL+7UR90ULoU0Y5KiO29afrjLs0K6TJYakf1X8yAaqX2gUX/VIahY4WAO4nZounAxVy3TAxZlPMpnv05aD4398DsBFyTMNT3rCKEMvxAc0paxHXER0zM0Pj775U3ClYH9UeyTfiyyCWdKH14NXyH3yfil8E2EddyPYMHE9kUfwkPwL3Ljm9+Ue++5V8bBpIa8DenKGlBpunKe+dT6gsHyujyz+dAtmpFRxRL33aWN4bewRr0gl3ED3Qn5sH+nfI6rNlbgVM2SltnySPNyOFCbhzVqF6b4ber4Geud/tkuwTh/8ni/pDmt9bez4zBN1Ib4PDmjz6qq8t0WnoVcVmxr+FOHoPpCSkqKSLThYXxReHGpQgxVIBVo2MyrwKuoGL58ji5uk4rDxZ5DsDjCFsyOA9q1/5M4dLkIWl5od6dCMtauWSt/8zd/I//5n/8pe/buwfbNC/AHNCD33nc/ti5wjtWb2uU8Lk9XHZAOz8qk0jBwko4R9CyskQ7Ct9L+gZNwyH1WuSB9tnWZrICHw25ofOkZYjq8H07AeMqHHhlm4pzBkeMY/XDUbf4M3Z7ifIvaWVxXrD5aIO06jONtMMbvgKH/4nnoXMyoH+N26yxRf6noIFu3esrjTyGuL6wOQU0gVoVIMJCgBcEJLHUFZWjrxm8S8IdPo7l8A3sDeyGcH8A7w95j6Nqhenxug0gXunhjFkghpDDeffc9WAIOyy9/+UslrIO4rIq+l9avvUtdDTkqDxHVuDlCnnErhgqk3soNuF25IkcHcH3G0Ck5ientkUG6Zbksj7R1y9fbHsC2zDxl50vNr/vQa0MPhOgsDOY/3Q2Bgr3vLAiu65+3kLyCCE55j2GF8RnwcaKyHHo7engY+6eAoDHMeHSCqhp7gfYVMWFx0tNTB8pNoN7jDQXbq3tz8IxBsfTlKE0cfxCoww02A2RT60+XGKcwhq6s3CwUVY37D+HSHEx3D2K4WIoR9GsQUp6ODkZIdhB0lkZn3vylsG76eBPM6/plXEubLF2xHF4P4I7TeOlqBM80e0BnSIuKKOYZt2Y4vaXHhrPw9ntw4JQcGj4tB/tOK4XQzGb4kIKnh1swdPhK272ypHmBct2SPZaF+B2HI21L58ETBDw7fIJl+/s4PP4YDgvNgtOyFu7ZJCkxmFS0384opOdh5PUuVhd0MfoAju+uQN9I59ypKb3XGt2qNfWZ0rSoNHXyLC98Y9/qFlSKjmZbXsFJ0ylER00gOfo6UG68ec9idbYaabbIdTlOtsbsLkWbykuXKvw3Y7elzaFR0UYNxrETIi/8HqeicSKaXqQ5ki7A8GE4kOJZMwTxnrvvVs68W6Fp+fnPfo5Tb/3y93//d7hGY42yD9Yk+WU0gRlZ3oymITxDfvifd8tcxiT3ENyGbu0/KH/Excq7cEfrGtj0PoQp7tqWHtkyuB9xh+W5trVyV/My5RTNewKezYcxPpfsV2Hq/Nqb+rzooysqyp2K7pfysqZ4ZuuSUDyETk+Gb32As6cQ0keBtwNnUtUT1pEONJF+vIUdk3aW5zjqt/KCangYKlpUsNPr5A6N4xE1TEvq/WZo2KWRZoULPrMeIBPLErS5dIX51qaLKUBbSAhdth/CNPdnr8BLNDQaT96LFnMPDkbO1l1GDbrINxpFfP/578MVSZv86oVfqSnw89/7nhpxKay1aWssz3gNxhmsP3cNHpbNtw5Am3sGvpAm4o6ZLvk27HkXt8yVmfCbtH3oANamZ3AZ82R5oGWlEdKwa/Z5xrUjhfUpsIme7t/4ECdqMBWmkC3BVHjieJS2Bs9YE+ewHuX1nO/8Ud8+//hd+uC41UyX4lnW8HKaw+q1I41qZxk8W0KQRjWPsOxZ823IS3mfSREdmnI/2NnAT4xuimKnwKp8WTE8TmhAM3LGWQMWjMnCPYAcj4u7SElTxMUoT1KEwGwCxjUp/VS+9p6+3egJdOsP4a8TLQ4jpEqfQhIE09xw8uQpyiMEj8Nt3bpNLuDWNnqNoKO0VhrG3gaeXcUe6GFobz8a2CFv9n0uu+ABfwpchq5r68b6c6Xc07wUWtz5yhaYRvlv9n2mnHDfg0ui1rYsVT6BU8X1eIay02KIh7znYvTjdip9jB/CauHcFS28dDVFgVYXWTsIKSg0DzwK+A924ggvBHUyJh1PwK3UUmz7EqcnK7V45hJrYCP6zRw6XZc6kZ8myLSocY0gvPyIGiEniSSMjyXX/uYER6RH3GD6CErjTGWhsksiUZRoWoroYrzOK4QwiRM/OW06V/xLezU656EH6f0YUTndfQjeAjsx3fXOkybQJXg2AddnLFm6RJ7/7vewBTsgO3CjHD3y27Us77uJR4nR84yGC9chdjQb3DNwFEqis3IVCiPa6q7HTW3LmjqxFzpDZsA3kr6xrQnUD8lRKJKOQYnUha2Y5fCgz6NtRU/GM8N6qpjo6nMRWNWG1rcN015qgQ9i9XAUK4dp0A7PhkJoOpRPPFzOKTHlhefslc8lwClvg0h313KRNVAg8ehc1CSSTSTFMxA0Ju2siCMjC69PUAvK6WVdA6ZG9MhKgVT+1NblvEGZZZysQSffkMIAnpc2HUZr2QSNyO7DIsugwXjmYS2kCc8M4ZQ7WUBk0Ya0q1atVidu+jGy7tq5E+dZX8Rt6pNk1WpMLWHNHs4KkrhKBNK4/jr88Z7F9RXU3u7FSMpjbTRMWNQyTxnTdzXNg1NtaKCVBGgeULnUh9vcdsNjPm+B6wEsvRGmO9oEIaZaOCuhO8+F2E6hAT1/d2PrmUJIpxdw5qiUQjwEzuwpqDT0os6O7J8P+HuWgvWdwzJpnJ7jhLnpWgzrMoQKv0vAe9o9U6AQzRh81yeoAQHZ3J3htj2rX/vhvunEeYz+JmsIpcL1hw8YJnDA3CiNwa0exFp8BAzwJNDqfA2gm1RFMICeGXjj93s48L11nz70/fUn4c0ewooWlSafOeUNIMkzlQFKgNGDFyfzHpif/vSnsumjTWjQLfJ3//B3shAXVqmRNcsn4JnPDIXRfUgBzf36sAa9MHgFe6AnZPvAYdkxdAznTXEEj1PY5kVw8TlXba+E/CIuXuV4Ebev7sAVFl24FKqniS7N9D4p45M8Y0REWx5AH0druvUa9SK0uKcwWp6FKSAvgeLFxtwnJfRUTInpGR/Wl9IFE8EZmPbqTiRJqskyz0fTVotnfquIysOyKMy6PvW/+E4DKuhGPaUFVRHllwMkIzQLtNSSNEN8QRnyYhIyh/Uq1JYQwLQ5dad+uaJe63lVjoo2TWAWn9EbcFIT4IiPzSyvgowui5YW5b1oPa+/iz0B7JWu6Bb55pMyvKQ7oy0XVWZsdNAOXRpVdZ5xW2Y97mKlXe0AtMA//o8fy3hYLf3oR38tPT3dmA6bG9NMGcryjMaAV2CNuwOG8h/d2iVbB09iDTpeNo5fK/e2LJMOmA3ygiefE6bwhn198I10AAfL92La+2Dbcjgv05uWtj69urR8czDaBu3Wp+XZhDb44oXj7I52YFmct30OYPwibl3Hti4d2u6odpY1Orfko34vLai5cJg8Ve24ROXvblSKbB3mxKSAbDaIc4VUV7YKJAH4z1acoTCiy1QofnSDUsnUk8rWo80C8Cqxc9Dqvvo2TAMhpJx38dA39kkz2hQZli79bnmW5RvRllPgRnHrZsXy5fAV/ENM+QblZz/7maL3u9/9rqxYsUKtXZPEKyh2bDnPuA96dugCXKYck02De5SRwrzmqfKd8ffLStzO1ok7U7n+bPWMBYlF0+bSRVwHBo7j4qh5sD7qwDE2DIcZZBWmKigNqHlhYKvwzCbhmvbOaWcOT6K6NAW0hDf4t7Sgei0ja3lpapJ7l2nQqqHJbPK2bdLmAa4KPYXYSwrk2XZsCtiGKUdkWDRxuvvaJmzaQYNBVePibiwsOQK5I2eISOeo/k0WJofPeAY49n8TJk6UFStXyl/91V9JX1+fvPfee2od/g3YBq+FIb/SBicem81V3C5+fPgMRr9jsrf/hLphfDYE9Km2Ndhi6ZD58Lk7nZc5wQ63mtBbntG7wxV4wN8Lk8GVMBOk9rclEu4EQUGQbepqjIzqksAj41m1MtSiKq8ap5Ii2vKAWu2sVn4jia9DUB30USHC2qgFUI7UurHUk6AMLOddXDB9BMURrx+jn8snHoAWA3MzXjWhHm+8Ly5YrfxsqzVwXH9NgUuXdXfdhWnvj+QnP/mJbN68GVs4uAscC7d1a9fKOFwc5SqYhjA951Gz0/BQvxunWY5Ai4uriNUeJ+835Qg6H36NeOlTG8JUVrXoMiXi/uo5mAryrpmvwkxwChRNJUue4EnJTGuBBTxLZFQqKM+mVoYGXUmwUpmXBCotqFUHBKUJY47VS1AVhyWYQPYpRFcKk8ZShbZC9Db/i9jc40Up72Ibhht7334KjsiW6f0Ah8zkK0isOmJXocvio7BOwsj64IMPwnB/UH7+859DWD9VRvz0Zrhy1Up1pQb3I7nNcm74EuxucQnxwAk5hbtfxsFV/CIofFZDi7sAhgrToCaiT8GaT4I2ulnph6lgJ7Zruoy3wdp4HIgg2yQVDeAZc6zZOkq1MQdTTYTMFECqUMmS1WRVLYDSgppeoxr0zhzGL5PzxXKwDFULbYFysCR4JAF+Ph6vDG0xHhPiRrjvNGjYip31N2C/SyXS33wF0941Mowde1sVEU43AECN4VkFNy6Ol8cee0yZG/4C51nff/99udrbK9///vOyZNVS6Zs6LEfljGwbOiS7cUt4Ba5PHoaDsrtal2CbxWhxqxEbxiV4xouLO+Bt8NnWu2UWTsa0qssXa9SnmSMWsThK3zCelaPLJT9kgaZNEWRkL4AIEzgyoNI2+MHSSOvVauHN6OILn5pC52PU6cPSpWBCSlJp8jCXnKwTSCUJ0ZrvnC5bKBPBq6z/5SUoS6FI+iZsdzc+mvBgH2bkUZP3SW5wmKSALgbntOmugYql3bt3y69/9Sv5+S9+Ll1Le+TuHz4ifQ9MklPTbyot7hPjVsrdsCKaDyURDRfy6WmesUelDS5Bl06nAaNOKFGOGKWXs1cDNnmWxgWNESVy00EatHqCdGwqNA9zyRlJOyskuGREaUFN47MF8YoBUHxHo57LSBebi8OOVencGJqxTr3kjPRTuOEubSa8IEsVTL+W+w6I/PcX9anpJ2i/i7/p2Iqo0mt6lJSizaXLlKwGz6i2unDzkmw+8rn89PUX5J3X3oJRQJOsgfHsxmc3ymNLccysCVpcCChHPCukGW2l6LJc5q/TG1fjWcZ8lwseR/zqMV8N5VmQXZx7QQGSlI22nRUgHUVw6alvespqBSsUMPS3ZtYQ0ub3xC47fRy5RY/PcgWFIHcLQudRhMvFa94LyK7w4Dftd3/5OjKAtvfR+7AVsxZ2bbBXs9NBTEC8QwjInOiyXDIyEKLewyZTkDknyojK8DiMG8b68DwMDajF3d10TPbMOSZtT3fIhikb5eg7e+T8e0dk35Utsv6rPTJxXZfaa3UxKZxjxbOMzqJyIutaPCMOReQoeBYwrmw7y8gHg9zd+mKe5Sn82krVnAs7uvfyglpnPsVkuzFVoLLRKw0Th8YhdZGsjOyP4NwUtmGO4zTyUw9BSNfA8FS5tMtQhWZ8Ua61AwrJcpPSXI/G7xegaaUWdxe2Wc7izlE6wp4ybrKsxtUZc6dNkoNT98rHb34ohz/bL7/u/RVs7IaVH6aJUEJ5B9CBPCItEVJI3CgiavIswh1TGoHUFVAGX0KHneRZXRk3DLi8oILosM/MA6KYughUCrMiXqZQJ2BTYJqI4piMSE53jxzTN6vtgrA+gFH0YRjZd8BWzbggqEpjUWlHwDPuV17DQe1L2AY5MXROCehFCOgJXA48B5781rYulOVNC6DYmSOTO8bL2qdgVTRlrvz21VfU9o1ymva952X1mtXqKo1m9/rGgE6PM8UfRaXzwn3bMcP5avVahLVOniWagsEcx9g6TNZliWZCxMVsKomgqNw1wssLKhCFRddTtZhAHeKEJzmTU1YopGSLE6kw4p/UFNGlLc/Zpy0KJ100aDgJS/AP4Ofoc9jvLofdLu13Z80wLt01nT6NxMRH51qWLqZI8YyTLu16E3a4GEEPwrxvD46Z7cLvEQjp421L5Fst9ykPCrMq09QeqH0ojI899qjMnDlDXpj0grzwwq9x1eNN+QHcka7HwfTpWFvT725NnqEoTmky/EmeJSosbBuKO15goi4QVIuuIp6pcOZhRadGG3PpickHJhOYUVmCtrwuFXb+M2ZPXYLqU2H7UBDo1YETjgSMIhNsMbJezSCLlnBeJkyVI1c48E91lgQrDYfzORUaAz3aV+gW7/cfwfEODjjSkc+3npbhubOQR7VcstIogsrRReLTPONlEaexB7qz7yDOg+6RPYM4kI0rDB9uXSz/gONmXU3zZaLS4Jqb1Tx+01/QeBhArJMZ0+FQDH///M//U42sP7h5Q23rTJ82vWppcroMg2vxzNQR61KlAANUkoAuA2Z+yCWToAE883hJzEbQsppBVpYuRWMp2pxaB6JqLSDM35cBv+SN+BqV1rdq2ZPU6RT1psvhg7cCRGFw+K1Io/0uDRroQmUHRtLFGEnpMXDZosQ2TLIwKjBVIhWWzNTC8xdXEcLM7yh8EO2CO5PPcJKFV0T0tMySJThi1gMzv3mVdoGRH73iKgVW1YaD/Ppwr+GZM6flxRdelFdfe01mzWqX5557Tp7e+LTMmDmzqjCF5IbfMQdyiNqwfuqR8iznd0xN7RxqpcnrxoUsU5cWvl4+1KYohxjFiFrc46QqQmepe9UaTS6iP2+gtlcmBuQSzGEso8IGrb8dNtLPEV2r0/fuVjjeWdYFDS/WpHRGVmBDm65Gy4OcLhKv8guJMKXiUbGrWG8eHj6FkyxH5DiufqDQLmiahVvR2qUbBu+zKzOViV7ug4i013iQHx15d3R0yNe//nVgrMgHH7wvr0Fgh+H35OlnnsFoOw2nclxPvXmpQnIjnrnZK1bmZQ7T5qDppqvh8/SEV2EFiNxg933s2llegvrbmcuoxr2PSlCrk1HA9aLaqI7MibV4i/AXITLwHEnPQKv7MdzXvQ/TwB748XgITnc4kvJOwMKnVn7F8RREngO9DAE9aexwz8IP7jV4zp9amYiTKHOw/oQ3BVj+TFaHtXMtcyE5BRHNOG29sHuhGkkpSx988CEUTa9BqdQijz/+mMxsb9enbwrS+8EFZcqCC+IzJKONr05kLezVU4889ovItw5BTfSOWZAfV60gMRZ//ypmH1I4GodqOoN0vk6O6iQMRtLNu7QbFd6q+zSmu8tpZK+PbOX5x5TquaNLYU5bii66LbmJ6wqvwpvfyeFzcojOqnGW8/jABelsnS73wMRvqRLQdu8AdsiDWKNai2eYyS9aJM999avYommW3//+9/JLWDM1Y3/1gQcegC+m2RBWrZDKeOYVN1H2kKjkdyJdIsjyMcWz5Mwlw5FCVlTr4bytFs/yumTRctrS+FXxG8KzJCOjwBGsUUkdn7wAPgtqMCTFa4MxFeWG6Xf337A8MYaMGhow8LZvXiSMi5uUX4//8i0I6RJoY4wv2jh5lkEqyoaFdPEUywD+4yXAykh+8Cjcah6Rs4NXZTnOcT7QulSWwqvfTPgiog2RXrGGDcspWyrzEjxjsuPHj8nvXn9d/vXf/k1mz54rf/XDH6jbz2fj/la9dRMjz2uwRl2Shjh53TxLoyFiPre3neXFcWs1K5J5iQtdglMhkrq+6xJUtwf0CAvodouYsduDwUcWgYqIy41CEAZxVTo0t6S5NjkYfyxuaEDlTZwnfQ0+Kjkn/NGXZfjedXrN5uVhaMtUmSE/DV0MTtBGf0SXh6/iqodjsmVgv/yhfy+2VCZi9OyWu1p6sAe6UCbhwDUvDI4tnDSxGWs8vhi6VL7leUa3LufOnZPf/+EP8t//6Z+gYJol3/rWt2QjFEz0cqhRFfBM1wKKmVGkq8qjiwSV5FmCXyF3FTagWtgkMgAAIABJREFUs+qHP7l2lipQA8LqEtTC/KKKM5XntWTD8iSshveaSwYXJFCfhUhiElnrXJe+jS2Yl9/VfiU3wuro/vXmTKnTFArRFsHkCXqhwT05dFb2wgfRjv5jchDvSzCl7cT6k9Pb+fAxxPXohOFxeg2azCsMbAzPeH71wsXzUCy9Lq+88opaoz7xxBPyla98RTo7OxM8I4/dYNLFxwaWGT9q86xA6mN6bEjIHhUeBjaGZ42hrbgo9caMSlBDFmWZJyOcwCA+bAb0a30NAyB1O3QtGSh3q5QxRIzva/AxuQ1r0p/8TgvpBhjYPwjlEf1NqidMgyCnIwhpyzPndQ8wUMAK9AQFFCPoCaw/abgwBSNme/MU3Gy2QOZAgzsde6L0R4TbRcPcXHSJEbo8z3JE6TeOrKdPn1br1XfeeUd5jXjkkYfla1/9mtIU0/G3+wRcCZCmYvOwmGc+vP1KYUlRXwiXjHACg/iYrlRuZcKSGRfXbRmUNWDqUCbVwFQz2umiq0yBuHQ8DP+uO2HJxyv0VuCPd2vyIXvoYZ0Wf0TBcHtzVzL7XgjpThgy/CeEtB9r1IcxitJ+NxNSpgqISdDmBtHEr3f4hlyAgcJxCOZ+3Gh2GcJ6E5f5Tmoaj9FzliyGP9wOjKYzsMlSgQY3gTJJbhxYjmdxujiEdr8UyI0bNyrHae9CWN955101zXwGWzcLFixQXg7LPbVL5EOkv2pj0dQUwiUjGsezcry4PVB1CarXj+CjcKQDr9J9TqJQAZ5rsI3fdUxfV8BGZB01s7XfuDUsZy5V1O0RrI52uI+ks+apGHlpN6/UMTZjdQXiQbhQwZR3FxzG/h9QHMGT/TBNA/Go6iwkMsdDOO3Ptl9uwDP8OYjloaHTcEJ9Rk4PXpSLgzdgnNAuD8KCqKeVe6AzsMGCu07N+ppZZM8Y8czNws3L7yF0Yed3zIcBxEa15/rqb38rr7z8itpn3fDEBunpwekbGPPzyXw44d2WoR6eFdNhEKaWL6Y+vGoprCOdQ43ojAwWorC9WqgIWUyQDnHaWZ6DxzMnuCGvdQmqriiSiqeg1FFZC8g0WAyaPNUVTHvPw7drHwTWthDV5gFy8kJFXofCdidmsvSrNW8uBkncCHYPFLe81k8lIF1ck/Iy4T9AefTJHpH//Wva19FUjnB5wysa6hQ1gBvCf/SFy2vvz8AwYR/8EH0On7Y7IaizcLP2/a2L5GttizCCdsA3bnDVoGmIt4NnKRZbnqk4RYT6RzGS12dQWHlr3L/+y7/IT3/2U7l8+bLae12xcoXnP5ip+GfrqxbPot7PZKsz1/9q2hARxoU8YyWEMAZRPe1MoVD/VE9VjWeWEI3GtDO3UCqPAmJDuBF81yWoCn9ITFB2y48QLKdNJ/CLlJ8evIQdFHo+mYTRklNbwvGPqXj3CKe9OOElc+CgfTtmtW/C5xidNG/AgZfx8A2rniO4I+F1aHf3Qlh/8CWRZx83xgw6vpidOW3cYrmAu0D3Dx2Hed9BCOghZeJ397iF8vfNG2Cg0KEsiOhdnmvPbDQnrWhw3mZLyIwG80wX2v9XldEtKPJ06ZoxY4Y8+uijMg3LgB//+4/lFYyux44dk+9+77tq+4Yjq0puaC3DM02BhgyK6BHn0WYAI/XUHcizrBAubaSfTzGDvLKP9KN+QbU5qYqP5bYm0WZKSNsDllFPWXXD5p2XvCyIhkO08uPFP/qGLsQjr6mTcUUfBPQ9zGZPYB3LI6R9sF94HUvRS70i34Eyt+0SXHvymrCT8Gj/wEqRLyGQWinnTGkRsyic1+SmnMTacx+0t7xS8BK0uTOgHHoUFyXxCocFcFTNm7TH0wZ3GP6TTAW5DdMYOMZ1N0qeOawH7tgMIhswFDF5C7K0uXRxa4gXUdGYf9zfj5NXXv2tfLxpk/wTtnBOnjwhzz7zrLRjK6faMTlNT95CdT76X6/dusxRiTRtWQnwaW8AiNo74kbTzhrJM6/dpOhKW3B4yUb6MXJBpZAW5hrVTA6JRLxG9BBOlt3ECLkA57JXd+FcCNaY1PaewvqTbnR5QdAUdW0Bk+qcZuL7PrjV5T0knN0yF/rFvowDMKdOIxwnRto+gq+jfRhJ26eKrF2qrkAsFFJDJoWTx8tOYnp7APeB7sXplfGYW0/GVQ9LlLPpuVAOzYbA4qp7+MFNmfg5ZKqyppZgLMZIeeayWuNIYLJBATHup5uKCiZe7bh69WrlenQ6jsx9/PHH8hLWrZcuXpJnnn0Ws5eFOJ2Djs4+1auWJXdJ1e9RkFMCJy4CY2oEJsMV5prEZLQ4Ofr0WeTqN8/J/Uzmn6IrnAX4OY3qa+SCOsJs2ensgbJoz2FcaIuRcC/WluOx3lyI9eZljIqXMKL2QQ/Eq96pJILzu+yZCKXkYsBxPYrbHmQQI/AxCDVPqJ09DlODHYcw3EJQr2D+vAIG9p24TSjYdiAyugah68teONk8D5vbI9heOTlwDqMnvOJietuGTOlkehHWnjzBMhUC2ma97iVrbYTMuAOScSTjlRkr4eybQjtjxnR588235I0338Qyo0+eeuopWbZsmRp9//J8cRxogKCmejW05lQwysntFY6CjOfWyuHjmKli9PwmlpG8IIjbM+MwclL5SPNbezmtZREdLsyDptei55V7vDNz38c3MA2GeeAhDK3dmB93QEjp68h5qBzi3ieNE85AQA9g/Xl68AKuELwgg0DI0yt3w7xvMUbQdmhv26wXv6wsfqHsV1hU9V1VoMMUJLKYZ6oIqSSpsALQIhQKNx5OcXt6emTKlMnSPrNdfvHLX0Jg38R+az8cMQ7hRrlVWiOcKpeio4CYJOFxcYpSW/riFIz5YnmW0zb2b3UIKpb7WCy4DVAzN64521BD5vObgsclI2/umo9ZKf9oNLQGs9RTGB2Jfw6mwzR0mIQR1BVUKhzUgsXUEX846s7Hjkv7hFtyay+EdAGE9Dm49uR+KS/bxMN01N6q85/YVuFt2lv6Dsm7sMF9CmZ99+CAtr2HZQqsh6ISZQH2xS+570jLnUA1hmfM1SztVXmyJyKUMe7alXTy0YA5uA5369PWVXv7LNmwYYMyL/wf/+N/yltvv4WR9Za0QrO3Gr6YWoxrF7duNW0au+t9VptI+jyztP858cyWaSx/6xBUK6R55etO1K0ykmoaCoK1UDvwps1w//MERkFqdZfhlNmWHbgf85A+IsrrXNoheDCFlfFG62sZ0NdfkevQ+jaj7tswsvLoaCtKMBnC2jx3hvyx59sy48FxMqEDQy6HWiWgQ9j57JXdvOq+fz8cVJ+UNiiB7sL1Dt8a/4AsrnTiSr9Jyjhea2/LsDuUkKDZoci28UY8UOyqk2dMUoYsBeNCGuGxoQ5dClJFa1ryugTfJ0xQQvlf/7f/Iv8OjfDnn30mTaiQSZMmy5IlixS2OBfigsYejsGHMG2i14lx3qXOYQnK8EynKd/ODLwm5bbyrJ4aMuTV9VOHoFq8DsPVa7W+0a9SqxTD7EqVqw9rTHXnJTyg0CPKCeyqcLY6E3LG0bSlOZN2OYnp8ub92q0RO/V24OC6lpfg8h7NyvgWudreJf1zcH3fpCbpwxr0HKa3vHT308F9UBb1KvcmT7athnJoHi5JmqMcVk+AwoiNsMhAPuOmlS1Hxlxx85ohPvxmOXqe+bVaQIQKjjY6DC2alz5duv4UblOX9p1Ctnz5CvnGN74pv3nxRdmxa7v8+oUX5K//+q9kzuw56vhcOLsagIeJl19+Cds8x2XN6jVqfUslle4LXG7p0lTjWf8gOmUoF69Ds8/l0mRcWMzrfrhc0vnmqTVm51sX1WeZ6ZBUoEtKg3gWly7IfpSfIxDUOMe48gGDwCjcjCSTMdhx+sv1JrW78zDV3QaFEAvLP27dTMTI2mTmvdw/pUnhNtguUIE0EWkuQbDppGHbQZ2I6Zontsh1vAwMXpPDleO4qPeQHBw4pW4yW9/WjTtYetQFvDNkKrwn4IIlUxSOfi6jdXhAvf10grPXEdRSxBvSgsAoPBp9DaCh3UtgYCMchE3OnVnueJtHp6+gjibKXbik6trVXnn99dfko48+lKnoZb/85S8rowlaN4XPGVz0zJvnjh09Kp0LOmXdunV+J2ATVOHZWegq9qHT3gf9xRl00GwPs9B5L0GHvho6whlY0dhOgk0kKm8UcLt4FnKjcd8NEdR6yZmBUXRVN0ZM9I6cunIbhoYMUzGaQgGpHgow+c0/bsdw/Ur7+jXLcIS0E2tcaIiPYPp8DsIKoxo1qmLGJh/uBPwCXLZ784+y9dw26e2Hid/42TJ3+gqZBe3UhElAjDyG24aUzSufZL2qGPep0rJCUPVdL3wSSZ2B5UpSD1Ju2dz/wP1y89ZNeemll+TVV19FneFum0ceka6FXUqY+TBnepHg2nYAm+Cbt2yRd999VxYtWhT7GC4gYBBD55VrFdkEyzMK6Vnsp6OPwO11qH+0kaOwBD0P3UYH1BAU0FYujzBgs+PnH/fdtV14PbxvPM8Kijeq4Ob/hqcUhoKyh8GuV/Qwzq5sOFrOnwmG44/TmtPoNfdgxOycpwWV171w+rtqoaaMoygN9Wm11IXp7qPQEy1C78or4rk+vQQBPoV47sScROVubd4re/Z+LKfe+VwufXZCJhwekJuHe+XcsdNyHrV/5SqM6OFS89atW+oUCU+WWLrVHinqLqS9IFDDJepajVMxElWgMLgMz9w6CtOn6i+CMTSG4f5q08dk6eK2TTtcuIzDZVVbt34uhw4exPZYPzrGCdDMT1J3tVbU0FZR/DyI+L379irhXrVylXJlqi5fNo+iweEZ65ezpvNXK7L1EJxCfggBRV1Px8g5D0sbWH6qDpz768dOoS1gtEVVKgO0w+isT1CgaXKKhx1/W4uv9GR4WG4N7f8bwYyAZym8jQgrL6hRY9TFyoP5zQqwIfGUSgEYbhCMf32wQDqNUZH7qit6dA95DWuTyehBOc0hDNclFNLTqChaINGrJ9epl2Fq+PkhPQVeuxLWgk9C6QthH98MG93jp2XowEkZf2pImi4MyvYt2+Stt96S995/T7Zs2Sw7d+6Uw4cOo9c+K9eu9aLXRkuBcNEIQGku+WceuybSs0fGmTKyDBkUXmwaJFDhXiQDRsezjJ4sT43PUqq0BWaKG2eNOEuXS7N613j8YFqDWSzDao+1s3OBzJ0zV7Zt2yaffPKJnIaVCU/c8DA6p8Hk3bi2cXIca9R9+/eBr9elDfGrcD0khdriC3l29XpFGcFsxtKGl+fROSR5B1Rqi64dM63ZENqFUDyeRxz9s9GOBTpBuYDvHUi35wA6ayyHuO8+E7BUNmbka3SmeGPLs0SlR9weSUBd51HDO0R0BTNbh/WKDz4zXIg8Rqe5CuOGP+4WefEtka89qRm84yDWogD8X5+1WzTDmOZW5K2tgMVfNypsMiqQPSlH5BXdsPVdjxEa2mJaLPVD13ur77rchADeorkTLhC9jEXt6TOn4Gv7lJyA1ur48eNyEr9nz51RIyr94S7Amqp74UJZuLBburq6pHP+fJkFlyXc7HdHhIzRLAwbudsiskj9MhY8M5jxE4ljHlWDLkU5OhZXGHPSDV5VWbrGGDcIPnEmsnPnDnkdh9DfRMc3hOnPs08/o/wzrVixAqNrC868/kEZ+u/YvkN56//Hf/xHWYuLl+002eUZ+7aPIGj0f34eM6teLGGeelDvBnB3gFPb5iZ2+lj63KrIf76pp8KPob65BMLOkZzA0mf7Iegr0I5oJw7HHfLl+yncw9CDuDzKy5KX1byVqEtC1uRZhLgxAXWtUfU4kRc2m96RFssP9as/Us0ojOmDYHHq2wRKaHnEnvMERtjt0PC++D7s6cHwmVMqMgdT4fuXAzNGVxoe0TCC0+BujKCssLlIxwptxhSMVkQTW6bI8ITJSgj5dED5sWjxItXQrl+/rrYRLly4IKdOnZRzmFNdvHRRLl+5jFH2iGzbul16r19TVjpdC7pgRteDP1wG/P+3d2VPXhxHugGDQBgQ9yEYBhjASEiDECCHdyMWW/aG7bAf9eL17r/lfdSj92m1GyhCxg4rQloJiVOAQAKGMcMtbma4ZAmz35fVVZ11dP+6Z5oZHd0Rv5nu6qqsrK8q68jKyoY3hKXL4akBXwKfNdP42g2FRerbIcC3LHG7mJk88lE7zNBl7qOfjlbw5uY/OqKqS5KdgRHzWWgBX9iyBVvUC7K1/f3ZftgH74fZ4Wc40vTLX/0ye+2117JVz6/KNqwfgO3wQTGWOHDwgJyFfXaNWc96mCGPhZg9zcTU9hbawW4YvuyEHmLRPAgZprBGjWCWEZQ5dtA8Zsz7+bh/gnScfS3DqEtz1CNDUDxC8Nlh/+vOadJ5UxcyaZiZ6m/9b21BlaYWVJxMtbzRpGiQdThlb/oIPeJFTHtoiUSFAAXypXWoCHBGU0Kj7kHPinXtWqxXqC0epYIB8sf47HXno6JYGboBcKTgz3wo6Yl84YymcuzhyTYN8P+O6e5DCOS9sfuybr0Lo+GbN2/BE8JVGXHHoMkYHR3NDh8+lB3CVI+0+tatzfrW9MloS8XJYji5ZqPl1E/yVAVPY+axiQeFWU34JA8bV2dYAXq4iSZRVX06Mu6GGbhIijK0wViXrl+/XrBcgwPnBw4czD7Bl9BpzXQB2t7BwUEzO0Hndvv2LWiLP862vrg1WwJjijmBS1auT8fQBngIA0dls12bTWc9c2ZQMDyyz2U8Tn25RcsYbH40juGeO5VJ/H0A1rk3j631bDaWREtR5R61gLQULoF9KlpvzJKpFH7ju60tqI3JJwoe0mAlsXeERh9rHwMyPXgOoMKWYITkudTiBI0Zcddi1M2WpDYVQuoVz8CSSiOur2bjt3AhtFq4OK2hxpIj7h2MsNeheLqKqTKPf3E9e+EizqKe/AzKlHPo2edCS70gWw5tBwWXe4s8MjZvHlxno9eZjTnbDLjk1PuzhqMawFSwXusVsijZjamVPIoEeuzYODuxLltocsiOirMLeofo61udvfPOn9ChHQJ2d8QX00svbc3+CqdqZ8+ezU6ePJn195uZia69e1ACDUObSweRr0BIaVgWe+0wmN1CB01/ADQ7XsC9eHVRYGml9qPVJvD8BYysmJWtQbtaiLjyJcoe0Mev45AIm0kKqC2oUY8sZQh7D/OcfJUoELddqDjCgJa9gN7P+keiIoBTGf4iqBjACaWXtYklf6MEJj7/hq8i7kGUGkyOFPxxykvhpXb4NuZlVzBNPncOe7PDw1CWnIVm+Qy0nF+K680Vy1dky5Yvl/3FlStxj0ZMweW6jB3CLGhGZoH29KgVNsPMwRgyr/CNhDTHzB9WTII6mD368iFO09yWDowKJZbHdkC0QKIAUliJ19t73hahvApFE0/ePOYcFNeJEyfklM7ziGfNEFkh16AAugAN7te4/9Ha3MgFjLq6khtT2EtYEnEWtgizKE6X1StXenbsG7EU4gf56GP9AvQY/VgeLYAQp8pvE7aJmWOmxZvagupKSXR4VTQU58YDcZWCNMk2TQWnY9rSl2/N6EiuslAlRp2Q6BsUM2khLShq9yKs5ahyEhyyQbIxrly1Un7bt2+XLQiOGpcuXcZoMZSdOnM6Gzp9Bmu1/TJVnoNjYc9jnbYGiqmBDRuydevWyxp3xaoV2GqYr6bkLI8BsglmCTZdUH3MTJI6mF3E8PTXd9/Nhs8OZ7//j3+XMhETx3vewdG5NzsqOlDbs2cP9lKPZD+DddLx48fhleMzEWAaUHDtL2tiFF0UgpBl6hpWo7O2DcvxpdoZNf5LEWctdBO0XBOpJZGgnT2DVr2xHxrkj6H/QEdACycR1JKrbcxKsplQcO3tGVcYAhcKqQBm+cBN3vgYpqMXgJi4HFyoEBhYgz1T/LjmtGtSxrBkSMXPNqfk/4v5IhHHm+JLiBfRQ75sSbz/jJRfHBW53cC9Ra7VXoXw0isCP3/46qvbobRaD2GdDWOMW/hm6SdouH/J3nv/vez4p8ez8+fPY9viPvwU/UM0pPzRAVpdzCLeFF989zQwo4HDA/B8Elta+/btyx5jeUClEteboTac2zjUmPdjdKUgcyo8irOLN27ekJnHpk2bsJ2zVOqTe6cfw0CFy5/NfeaLl65eXHsqSsw158BKbM0sNIqmMsywosruQ3t8EOtUOhrgnvtzVo9FcuPAzOvUE7x57SyqpIkH1B5R01PfnIGidRRTFnmlRAC3Ek0F8ZmaXq5JaVHin5QxgpQUIotaDpiHW5gg5y0MdoxYvnphqbac2G1wjcaf9dzHtRqnuwMDG2WK+ABKqrsYXTkF5L7iNbjqvHnrZvbhhx+KdQ8bbR8se1ZRk4z1LV2jzOf6FhuHs2FYwP1Inlj5AabLnCpySk5lVrTmVUOiLWNcVuKe90zjwGwuFoVbsd78GtPYD/7vAxktDx46JNsxFLz+/n4pO4WWv0X4chw7rOVQPHBbhsok/qicu03bz/y6CwGlxRnrfRmE0Gh4k9xLCnbkcqn2ZgL8NCwiafLHfpofSfCucWDmtTESC9mMeArynOBjbUH1zBdCJhUTfoHUk70NSkwwqdENr5LoJlrZfK2ipUZAS5+OK34RspI/m4hl0Sm0VCLxx4bK6+vHsIiCv5jRu6OyFfQFPolIrSiVK9we+uKLa3B7cjWfwZmpIAV/DulgxOaobX8/xMmVeegMFi1eBO3pYhnNOXrRIijEKsljMhBM1sCMZWNnsnPXTvkGK7djrsBdy1GcqqHxAzsZCizXqZz6svzkbRMEeSkEmAYR1Kg/fPgAndZFWTpQSz4KQaXzAJ6W4uEMXlXtrFwWwsLlheI/XnqaJgFF/BC7PIX5F5K1Lysw89K3+FBbUJMFqBDYkEcTtUECR2AcacoADpnCc13qdePpLGj7SmMJ/ri3uOXxFhlt+YkJOsO+euWKGGBcu349u41tIe7lUqBlmwfDC/ctOZpOxyg1E7RmonHPxYhLIeWalz/6NeL20FwI9yzsWdjtqLgH6lGCHphxpOQHkQc2DmTzoSQj70NDQ9kZKNTOnB7KLl2+LDMCCiU1vvzxnmXn1+UGB7dlhzAKU7i518rpMdecNFih9dE8rjnDy/LUg/UwGaNzK+cJ1r6UUT1TY9w0uVRoKkzl1gOzkK+JPNcW1CRPbmRjgXgpZANNjXkTTaCRxoJRBoqfs4mFMLkJ0rioOlzzloerLHUv7VPzaRTjVhpuL608eGODJOLIRA8K3MJZt65fNMpfwYUivw5+795YNoYN4vuYMj+A6R0FmiPQA77D+nAUJw8o3MPDZ8XIgDQ4dea0s79/HYwxVotgzMXuP6fJ9sftFAq8TJlzvjwhrokZp5CcBRzYfwBknsipmB07dkinc/r0abFW4uh67NgxWXdzKrxixUqj/UZHQgUbtefHjh0VAaegUtv/DDT8XD9S4x+Mp4KZXHXama4AWKJx/UvrNjn4IWW0EeJ6yTPJMzP/TOzxYuaRauWhtqAmKzmadFmeIJAoo2sDitVCVAkFLxvLj12Yavl7phZzQz8HUsjomtK04nvHW8Cgz1ucjtmEJmS22i1fEgcPItgpwVDlpfBwCsgft4OeYFshYInkSFHypfBex6bz8DDcl8I4/hOcUKGrz69wvISmjusgsBTcNTCFXIVpKK2Bli3BuVtMmakQ4tBCmFweNTFjMbiPeh0j/x//64/SYfzud/8GD/uvo8OBX2NYfP3iFz/HOvx6dhZbV59Dw/sZNLxHj36S/Q3bWXfv3JWOgmdct20bzB6J02ZzYgp9F8oP1qQl9sZMErpLtTMFHEdp9HfoGbGswrp2lqUt6XyEn0470zy2c19fUAXDYFxxsqGFxDCWbnAaKCUWbDykn/+XWAzghVZvb00AX+rGlkfTvHnsNOfNllPzY+89ZU7IW86yK3uLmDHfOdiTpQAuhWH8IBr8b377GzF5pCb58mX4f4IAcSvkvXffy+49vCdbQdwmWo3Ry2piac/MdaPYL/PUiwW1AjPG4ajMKTc/LPXmm2/iyNseKGkey5fMuTadznUsvru64LnnoEB6UTTbnMZfwRSZW1Y03eR6ezVMMjdi+syLIx4vmgPSHkGuFjCjIQ2N9XlKi1Zr9tisrcPJame2SG38ry+oBkMvTzNyxIJgQuLwkGHbSKwgRgLJISkMRFBqtHYNLq9sk1cPHnSNKeZCvoSkl0EeOQ+TXEqyCpNNBDOuP2fxBwGj0oqjME+z0JCAJ4DGxkZhsndbbJepuLoNl5+cUlOAqcDiFJtpaP7Yh/UthZejLkdjaq1ppTVDDDJCrk15Od3eja/AcQpLL4Xv7P0T3LR+IcL6MqbCnG4/i8P73LohPW7DcB3NI3E0fODUn65HacpJuKy9PKtBX2HuTTAjKX5miB5BaMHEs88828yrUTtD/FSVat6KdpiK6RVpwg+NBDXMzTAdwmqrOQ4P09d7DkAA2fqUq2M+Qc1Vx6jHodCoSchEiyOXhZdxwEZHBQ+Fhz9enJ5+hdMKj/AtWGunTMOMWzdvitKKJpBjGN1u4Pk61paHDx8WGivwbZCVMLTlf9rj0qSSVlUcde3xNdKnIK6EkNPDA0/S/BkfSObvMhRJv/r1r7PBlwchnIvdVhKnur7fpKIuecfZOHzOGT/NEC4x80tcZdikwqlE4veLcMoxWwjLtoVY/xo78ARhLygWtriWfBrF+14xe+Xd+/0EBDUumMmuLDx/i9dsZC5WZXQCwAhNroBgvYwkAxtV/lfy1YSfkrgN6LspG/kqIcdga79M4VgADa29OJo9gJHsLRjIX7uGLSEIFreJRkZGxI75xvVr0N4OQREFRRd+MlLDHJLTZo6Kz2E6y86AWy7PYNFHje7PX38dhxnGsrfe+p/sHewL38P5Mh7K37lrF0bq1TLNjS+/aXNflKMf7X0fwdileVqlAAAMTUlEQVTejnxxujykBmY8VUWzROweZet3aSfupVTxooV2VkW+hXcTENRYg2twrGpKWP1BSj28q6JLAXX8ImW5MMX0y/gqsmYM5qTS9uTLoG9o+y2onDfVQYG+n6qsNhOYlUV14T5mVtv8Q2ic1+IAwZNXjf0yPwzFafHFCzh0gHXuyPmR7NzIOdnfpZJlGabEHGkpmLTb7e83Nr2cdnPKTOdlNzE6//dbb2XHoc29xuk29kv5AaqBgQEZWVOXxWzh/GkcUOUw+B0ofxbQoCGBu8OpBmYUerp2xiw/W4bt7HnWSCLFiBdmMTNtwTJSXpdl7b9nRuOK0FhQdTFCTM2z1dIGzVAeGSNSSSnG0023yMekdyJlFx05BZu6N19MkMdyfDFsvLwVfBnKIKp4K8Os4HNyMWOHRFtd/niQgPa3FMxHaN1UAHFP9G/n4E4F2y6nYMP80Uf7ZK3L7ZZXtr+S7di+Q9LQ8GL37n+RtfH+/Qdx8uhx9r8YYUnnjTfeEOFOSZ4pN74jBEMHWiRBmS02v2txntRe48GM016ewrlwKcv61xuLN57Giq+qdmZrxcQRcWzUzuLc2ghpIKiWcZWtK68uuF33BeLi5CJcF+rtFy08qsqU9qhwGq3pB7x59WAfqvlibr6Y+ttC0uA8uiaF1Wz5zsnDyp4EzLzWAEYbYkZ0OK2l4HJ9yqkvj+/RZSjP5nK/lCMwTSL/8J9/EOsjjpjU6HJUHcM0+M6dWzhf/GW2cWSznOulIqkKs/kQok2w8aYLlpMj5qPVPDBuOuLmmN2GwJ+EkJ4+h48z/8RYOxmzRFNXRf1W16XEdhr9oB0Ld/mVbGeK7xZvGwhqzHBqmsKaqRqX4rVfKLgaiVx48qwFl5yALzO+YGi+iniRlEUw+iX0+TJ5h0mQwtZ5XDC/IDapLosiV8VdTDqBmUcL7xtixlrjaMpjfMbiaAjr2BGxV6atLI0YjC3vDNmOoaKJwnsD69IxjLZcw1q7X1oe8aRRL8x+AI8cG5+fJkfR6JD96HCW/fML00TJ5A1iNTCjIwF6pRzB2pRfWhhcB60vtmeKOi0wS/ElbdbDzACarpc8tCAeNozWnxsIqs47ZF9z3IP7Hq/DEsaUTUhdMkX1mDUF6ddNq3mpSlOPIx+zalELUKjKPAQsKJ/mrYwMtcWnPj+VfQS/vZ/DpQpHSSqgeKpnFbS89Ce1CPa8VDLNg0tAOWOLQwP0hEETwsvQ3EyHV7Fdr+3CNs3L4kaU2zNl+RUsw8UOpr6bMEOmOeF+nHZZgH1Pnic1DgPqY8bjbPS1Rcdor2wxHi7pkTB1pfjSYWX3KVq6LNXvJ/a2pChposW2Ixu9GjdDuZXkQWAURwVE7/L0qQ3TNGvOsD0azxVtUwEM4JUrA6K8ESBhqeocL18mvzxj8y/KN6edng7gZcG3EChL3xAzkqK10969e8UOl/uo63F+dgPOnHIrhmaJFFDaGNNdKKe7NH4gPtQk34AXRxo1cNuHx/tWQfkUulsRZgO+LLo0HaRXBvps5of43j0MTTA0TAM4n8rv4dJJu7tKyjz6cFp2DKPxhSvGxei2jbmrFpmK8MpzS6bH67LwIueCBcQ1CtFg3tiARoJsz6BGgqrbbtWIkGziAE2K5gqkYsltIGKuYgME5DFGxfJWxZdBg5kxfV59li/DghHQqACKN6/BKT568GXyLv8bZSkMasyCGC1hJs6yjxyR76LybO1v8QmLzZs3iZsZ/9KYky/6uZoj69g+HJCPr/qYLYZA7oBwsUhvf5Bl7x/Blg3csmyGAIvrTyiPaRwRYsSBgza9n56HjyR4IKQF0k/g2YEH0E3cOpgRZ3IftKke9dm7ncWITCSkkaCajEwjZ+lc0VDQItSWOYTVCoZ979PhWy+FE0ZfDW7kJKZd8BaozXPe/Lrw07sn3qgKshx6vHmdRJFXb77I4TcPM05xh2DovwYa2p/u/ql4sJgOywNXt7qSVUtrGzNaEP3Tizjyhu3fP+/Psr374Egdnhy2bcLoiqnwClgZzVC7PRRSGjacuDAte/t94y/px5jybkWf4QYUV5cF9pPSzhRObd02ElRTZ3kV4cEfYf1adPXrgpVwyG2ajkSXxEX8QjbsdCOnrjJxvAV8SU52zl5kqTpRxZdjC2EJOiFfhrZkkHczEUN5g0+X1cvZZOl3VvpJIqfp8I25Coq6PzGhCcyAy2MYxt7HqZ1++DJeAV9PM6x50CRjRh7pSXALRlFuqZwaybKz0OD+5WMILlhfBaGlnySeXeaxNbqZ5f7rp3APugFpfgwhp5cQ2HoEcLSMGajLkJJoH56rH8VGG7eNHHC7DFV7LGOiRhSVtEbsGlE8q/6AMZuc/3kVTTouQZ2sbCqJm0wQBCbj+HnXiKIS1IjdIwpfP4Yd7gmcdKHVEbdkrDkiM7LJ+Z/XZGFG6yJ+BYH2upfhAeI6/n8JyyWcw5cqlh+YodtQno7Zhj1TupKlX6RiOyZn2vvXAxDGrRGlqp2lcm0jbHyC2kbOTQFM5hmgWmi7krHrBtapq1bYDxhqnG/dAlUxC8z+gR8VJN7JoIa0G/NeIwG9VNIx2V1ohPl9IX4IjH59pZohpHMwH1wOn0hLMC2mEVRVR9KwOFNSM1U8TlxQU4CnwhwX6Zfp0BLWEfmJm26WxJmKYBYCV0/Fa6qwqbAOs6IWU/ikwr6jmEXeZGq3bwoLI9tuzM5JdFiKGFuxukzb5hovuPJGbzLhg/nZYK4T7H1IMAonb3UuFc3cJtLpILnnH6MFlSxQEJYlkVICJVxjJolUmH3W/zvMJhmzov6eSjtL1XGPsGYjKltZJFGmULH45SG2xSbS9eCt92vNTy3egkglaXpnXCOGpV2SRxysQnjLq8PMA/rbg1mN9tEwSrMRNdlwEqMhW5hubGE6+04zmwrDex0c3Wu6YR5CO+StRuchmQTMuEc/3D6FrMuz5acWX3kCS4hpwnRhJgk2GVQR7I/meZY2jf7vZx0wEvLlMuwwS2HZVliDEZUV4ddSHGLYahpeXpgySnGKpnqkMspl4XGOOqQsVRweh0wdZtVletpvy5HwDzikZmzfP8yajahPu+4mQL/U4q+EZmpgYNSy8BIyjYPL6JeFN87gO5AgrMsybJqGl0NTRqk8xWS/aSCocWFk/zCcJtrpprzzr5hCGCN8jlM4sriJs4hDCor5O/7TRMIsx/UcKLYcbzH/Ju+QT8XbVGEWsiR8TCJmLreJYzauKgwS6SYSQ1NeX23knaLRQFATyQXTUPtaAB0WMFS+mvcqlk4g90qbmmdvqOMlbuIqjUMKkvm7/J8Jz6nhIco67AZshJwvw12BiePLQJLgTRdgfJgxa8OG4jZifJyYKegKktWYFaVP8KUBqoMZ4se11w5mhspkYaZRae++saCyGYRNQUynwguoR8CbIdjFNO9VLP0+N/fxaNhs3HaFn2/IFzOqxVsvvgwhw7cyQ3K8ab4cSwVvbWHG/BphRha+DZi5FuFj5oLzm1p1qevK4SWBBbmqdtYmZmEBJvBcX5nEAkSSZ9qBDtYOqkuSeOxOJE6vtPq97zg75jvEMEk7EZgIElISXvIyDP4mYaZx6DAzaIT1FbaVyXiuL6i1uPFEg001TlVa6sSLRJAQrFTxphKlwmLWeoW0QyXM5fuAWdvIfbcxC1sInxtPfQsiBCu8EoIZRnnqz23zkConClESXF28VKK2+a3mYHxvU3xXUQrj52UMg6tIuHepRN8GzGoVrnakRoLKqVBxESw+B0DKI/8YhUkIs1leRqESP7pcFkH8UH/PHD3eQr5AyPFlcgljWK6j8ii+PA7LeAsK8e3ALFWXGjNT8hRmBZq24Kbu7dP3DrOoEbcT0GzqK3VghSY/BF08josjL7l9ICWvhn3SySwdbwFfPWiVMR2xksw0zZeXpePLhAaPZdlXhn/TMXNV12FWWY9NXjYT1CaUu7gdAh0CrSHQaOrbWq4doQ6BDoFGCHSC2giuLnKHwNQg0Anq1ODe5doh0AiBTlAbwdVF7hCYGgQ6QZ0a3LtcOwQaIdAJaiO4usgdAlODQCeoU4N7l2uHQCMEOkFtBFcXuUNgahDoBHVqcO9y7RBohEAnqI3g6iJ3CEwNAp2gTg3uXa4dAo0Q6AS1EVxd5A6BqUGgE9Spwb3LtUOgEQKdoDaCq4vcITA1CHSCOjW4d7l2CDRC4P8BTNnQaugM6RcAAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbnQaNM51Nns"
      },
      "source": [
        "\n",
        "#### Question: What should be the shape of W?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb9TAUhmbu-s"
      },
      "source": [
        "Before deciding on the shape of W, let's take a look at the data matrix, and the output matrix.\n",
        "\n",
        "\n",
        "> **Data Matrix**\n",
        "\n",
        "We have a total of $m$ training examples. Each of which contains of $d=2$ features.\n",
        "\n",
        "So, we have a data matrix $X$, that has the shape: **m x d**\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Output matrix**\n",
        "\n",
        "We know that we're getting 3 values as outputs: probabilities of the data point belonging to class A, B or C respectively.\n",
        "\n",
        "Let $n=3$.\n",
        "\n",
        "Therefore, we will get 3 output values for all $m$ examples.\n",
        "\n",
        "Hence, shape of output matrix is: **m x n**\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Weight matrix**\n",
        "\n",
        "Hence, in order to get the output matrix, as a dot product of data and weight matrices, the shape of weight matrix has to be: **d x n**\n",
        "\n",
        "$w^1 = \\begin{bmatrix}\n",
        "w_{11} & w_{12} & w_{13}\\\\\n",
        "w_{21} & w_{22} & w_{23}\n",
        "\\end{bmatrix}_{dxn}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzKsuxLA1Seq"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1xop0_R5WK6_IBR_k_SDc_4jZV22fQaQf' width=\"800\"></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie0tE0Szgcya"
      },
      "source": [
        "\n",
        "#### Question: How many bias parameters $b$ will this system (of three LR/neurons) have?\n",
        "\n",
        "- $n=3$ LRUs --> 1 bias term per LRU, thus $n$ bias terms.\n",
        "- The bias term has to be added to the dot product of $X.W$.\n",
        "- Therefore, the shape of bias matrix should also be: **m x n**\n",
        "\n",
        "However, the value of bias will not change for different data points.\n",
        "- The same bias values can be broadcasted for m examples.\n",
        "- Therefore, the shape of bias matrix becomes: **1 x n**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT4XZ7EQn47p"
      },
      "source": [
        "#### Vectorized Implementation\n",
        "![picture](https://drive.google.com/uc?export=view&id=1nQjWBk9q5_2YUMJtM8o8-ZLMOFFrmi_-)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzr-cOeK87l3"
      },
      "source": [
        "  \n",
        "<hr style=\"border:1px solid gray\"> </hr>\n",
        "  \n",
        "### Lets train this Softmax Classifier using Gradient Descent\n",
        "\n",
        "Let's implement the process of training this model using the process we followed earlier\n",
        "\n",
        "\n",
        "1. Initialise parameters: `W` and `b` matrices\n",
        "2. Calculate the output using the hypothesis.\n",
        "3. Calculate the error `J`\n",
        "4. Repeat until `J` converges\n",
        " - update $w_i = w_i - lr* \\frac{\\partial J}{\\partial w_i}$\n",
        " - calculate the output using hypothesis and updated params\n",
        " - calculate the error `J`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7IK21isrEV3"
      },
      "source": [
        "**Step 1**\n",
        "\n",
        "#### Let's initialise the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtRydh-2rEV4"
      },
      "outputs": [],
      "source": [
        "# initialize parameters randomly\n",
        "W = 0.01 * np.random.randn(d,n)\n",
        "b = np.zeros((1,n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqzgYKbxrEV4"
      },
      "source": [
        "**Step 2**\n",
        "\n",
        "#### Forward propagation\n",
        "\n",
        "#### Now, lets calculate the output using the hypothesis\n",
        "\n",
        "We will break this is into three parts\n",
        "1. calculate `z` using $XW$\n",
        "2. Apply Softmax on `z`\n",
        " - Raising z to exponential $e^z$ => `exp_z`\n",
        " - Normalising"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWtfhc4frEV4"
      },
      "source": [
        "Let's compute the z scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y_56F_wrEV5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c91172ae-7748-4eaf-a845-1ce95a3072aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 3)\n"
          ]
        }
      ],
      "source": [
        "# compute scores for a linear classifier\n",
        "z = np.dot(X, W) + b\n",
        "print(z.shape) # should be mXn = #examples X #output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASOSk_DtrEV5"
      },
      "source": [
        "Lets raise these scores to base $e$, and normalise them to get probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWwFRrtcrEV5"
      },
      "outputs": [],
      "source": [
        "exp_z = np.exp(z)\n",
        "probs = exp_z / np.sum(exp_z, axis=1, keepdims=True) # explain why axis=1 we have to do all columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuBTW3UjrEV6"
      },
      "source": [
        "Lets represent this process using a simple computation graph using the matrices.\n",
        "\n",
        "Here `s` represents softmax funtion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P0xW330YPDZ"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1rABeqxpwN2VtSF-J4GNo1MSA9UVPXS4g' width=600></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g1jhVFr8n_C"
      },
      "source": [
        "---\n",
        "\n",
        "## Categorical Cross entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3NG_UDv8tes"
      },
      "source": [
        "#### Question: What loss did we use for logistic regression ?\n",
        "\n",
        "Ans: Log loss\n",
        "\n",
        "$Log-loss$ = $y_i.log(ŷ_i) + (1 - y_i).log(1 - ŷ_i)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwJIFERa98x6"
      },
      "source": [
        "#### Question: Can we use this log-loss for multi-class setting ?\n",
        "\n",
        "**No**. Logloss only deals with binary setting as $ŷ$ over there represent prob. of datapoint belonging to class 1.\n",
        "\n",
        "We need a loss that can cater to the multi class setting. Let's define a loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na5fHu9M-N7s"
      },
      "source": [
        "Suppose we have a point $x_i$\n",
        "- it can belong to one of the k classes i.e. total number of classes are $k$\n",
        "- Let $P_{ij}$ be the probab. of it belonging to $j^{th}$ class where $∀ j: 1 → k$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRjgkSZaEJvW"
      },
      "source": [
        "#### How will $y_i$ be represented?\n",
        "\n",
        "$y_i$'s will be **one hot encoded.**\n",
        "\n",
        "For example: If $y_i$ = 2 i.e. it belong to class 2.\n",
        "- then only $y_{i2}$ = 1\n",
        "- rest will be 0\n",
        "\n",
        "i.e. [0, 1, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuFNwF8KEKWK"
      },
      "source": [
        "**Cross Entropy ($CE_i$)** for $i^{th}$ datapoint will be:\n",
        "\n",
        ">$CE_i$ = $-∑^k_{j =1}y_{ij}log(P_{ij})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIi_RfodFBxN"
      },
      "source": [
        "#### What happens to Cross entropy when k =2? (number of classes = 2)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMcNJ6wYFeyp"
      },
      "source": [
        "When k =2 i.e. class 1 and class 2, CE becomes\n",
        "\n",
        "$CE_i$ = $- [y_{i1}log(P_{i1}) + y_{i2}log(P_{i2})]$\n",
        "\n",
        "where\n",
        "- $y_{i1}, y_{i2}$ are OHE target label\n",
        "- $P_{i1}, P_{i2}$ are the probab. of datapoint belonging to class 1 and class 2 resp.\n",
        "\n",
        "Suppose the datapoint belongs to class 1.\n",
        "- In that case,\n",
        "    - $y_{i1}$ = 1\n",
        "    - $y_{i2}$ = 0\n",
        "\n",
        "Hence, we can write\n",
        "- $P_{i2}$ as ($1 - P_{i1}$)\n",
        "- $y_{i2}$ as ($1 - y_{i1}$)\n",
        "\n",
        "So, we are basically extending log loss to multiclass setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRzQSheJ5TV"
      },
      "source": [
        "Do NOTE that,\n",
        "- Logloss is also known as **Binary Cross entropy**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eehIr3WKqy1"
      },
      "source": [
        "To summarize,\n",
        "- In binary setting\n",
        "    - we use Sigmoid for probability\n",
        "    - and Binary CE/logloss as loss function\n",
        "\n",
        "- In multiclass setting,\n",
        "    - we use softmax to compute class probabilities\n",
        "    - and Cross entropy as loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOEAUZUmNzgE"
      },
      "source": [
        "**Step 3:**\n",
        "#### Calculate the loss.\n",
        "- Let's calculate **cross-entropy error**\n",
        " - It simplifies to taking log of predicted probability for the actual class\n",
        " - Why? Because the other terms will become $y_i*log(\\hat{y}_i)$ terms will become zero on multiplying $y_i=0$\n",
        " - Lets calculate cross-entropy error for each sample, and call it `error`\n",
        " - The full loss is then the average of these log probabilities to compute the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[range(m), y]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9GIcFA8314k",
        "outputId": "afe08646-7ba1-4972-e28f-14c7d4f64a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[range(0, 300),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -np.log(\n",
        "# probs[range(m), y]\n",
        "    # )\n",
        "exp_z = np.exp(z)\n",
        "probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "probs[range(m), y]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAcDQww72uA8",
        "outputId": "dc53f272-3db9-4c25-a627-e1b0fdc727d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.33333333, 0.33333385, 0.3333282 , 0.33332967, 0.33333399,\n",
              "       0.33333004, 0.33331736, 0.33332072, 0.33332576, 0.33331307,\n",
              "       0.33331791, 0.33331568, 0.33331181, 0.33331876, 0.33330128,\n",
              "       0.33328589, 0.3332835 , 0.33327067, 0.33327823, 0.33325459,\n",
              "       0.33324686, 0.33326745, 0.33324615, 0.33324692, 0.33322789,\n",
              "       0.33323375, 0.33322445, 0.33320969, 0.33319012, 0.33321439,\n",
              "       0.33318643, 0.33317534, 0.33315598, 0.33314969, 0.33315048,\n",
              "       0.33314171, 0.33313509, 0.33312669, 0.33312193, 0.33311618,\n",
              "       0.33311535, 0.33311591, 0.33310833, 0.33309371, 0.33309127,\n",
              "       0.33313577, 0.33310123, 0.33307923, 0.33307459, 0.33307883,\n",
              "       0.33305435, 0.33308783, 0.33307942, 0.33304443, 0.33308279,\n",
              "       0.3331029 , 0.33308512, 0.33311225, 0.33312455, 0.33314497,\n",
              "       0.33313373, 0.33329439, 0.33320567, 0.33314358, 0.33316875,\n",
              "       0.33314303, 0.33321668, 0.33323278, 0.33315841, 0.33314651,\n",
              "       0.33322197, 0.33329916, 0.33318067, 0.33314063, 0.33324985,\n",
              "       0.33322844, 0.33328949, 0.33329636, 0.33336218, 0.33333612,\n",
              "       0.33336145, 0.33325798, 0.33341025, 0.33355353, 0.33357898,\n",
              "       0.33331328, 0.33356634, 0.33348901, 0.33352946, 0.33354007,\n",
              "       0.33361176, 0.33358785, 0.33369696, 0.3336751 , 0.33364609,\n",
              "       0.33364244, 0.33366049, 0.33363014, 0.3338193 , 0.33361394,\n",
              "       0.33333333, 0.33331801, 0.33330234, 0.33328997, 0.33327502,\n",
              "       0.3332567 , 0.3332405 , 0.33322413, 0.33321255, 0.33319298,\n",
              "       0.33317985, 0.33317102, 0.33314941, 0.33313193, 0.33312077,\n",
              "       0.33309996, 0.33309879, 0.33307224, 0.33306509, 0.3330417 ,\n",
              "       0.33304431, 0.33306089, 0.33309534, 0.33315894, 0.33299495,\n",
              "       0.33295343, 0.33301138, 0.33309872, 0.33291454, 0.33297393,\n",
              "       0.33299903, 0.33300355, 0.3330025 , 0.33312621, 0.33302066,\n",
              "       0.3329684 , 0.33309722, 0.33315821, 0.33309047, 0.33313136,\n",
              "       0.33299189, 0.33307731, 0.3332827 , 0.33311904, 0.33328216,\n",
              "       0.33305216, 0.33320056, 0.3335222 , 0.33344674, 0.33339775,\n",
              "       0.33346036, 0.33346701, 0.33337371, 0.3334497 , 0.33362062,\n",
              "       0.33354081, 0.3338412 , 0.33368836, 0.33396918, 0.33374717,\n",
              "       0.33372151, 0.33387649, 0.33384768, 0.33390359, 0.3340628 ,\n",
              "       0.33393271, 0.33423169, 0.33406888, 0.33387242, 0.33436111,\n",
              "       0.3339712 , 0.33404653, 0.33441805, 0.33423008, 0.33426555,\n",
              "       0.33435034, 0.33430586, 0.3344965 , 0.33444464, 0.33437549,\n",
              "       0.33451921, 0.33450616, 0.33460916, 0.33462934, 0.33464339,\n",
              "       0.33464622, 0.33467332, 0.33458058, 0.33470527, 0.33471391,\n",
              "       0.33469687, 0.33472396, 0.33477036, 0.33476449, 0.3347976 ,\n",
              "       0.3348129 , 0.33475894, 0.33469813, 0.33463407, 0.33434854,\n",
              "       0.33333333, 0.33332549, 0.33331665, 0.33330603, 0.33331254,\n",
              "       0.33331199, 0.33329869, 0.3332989 , 0.33329821, 0.33330769,\n",
              "       0.33330019, 0.33328394, 0.33327476, 0.33326334, 0.33330817,\n",
              "       0.33328965, 0.333316  , 0.33327405, 0.33328043, 0.33330596,\n",
              "       0.33331729, 0.33324232, 0.33328711, 0.33333195, 0.33330704,\n",
              "       0.33338302, 0.33339148, 0.33344542, 0.33333031, 0.33331814,\n",
              "       0.33347372, 0.33336318, 0.33351758, 0.33355743, 0.33362499,\n",
              "       0.33361162, 0.33338196, 0.33367473, 0.33350153, 0.33367158,\n",
              "       0.33363768, 0.3335649 , 0.33369388, 0.33373793, 0.33368793,\n",
              "       0.33370125, 0.33377053, 0.33378433, 0.33380166, 0.33379528,\n",
              "       0.33386345, 0.33387235, 0.33388782, 0.33389657, 0.33388478,\n",
              "       0.33391748, 0.33392116, 0.33391741, 0.33395285, 0.33393247,\n",
              "       0.33397421, 0.33396147, 0.33399378, 0.33397032, 0.33397323,\n",
              "       0.33402471, 0.33403664, 0.33403256, 0.33401435, 0.33403269,\n",
              "       0.33407892, 0.33392538, 0.33407373, 0.33386999, 0.33406306,\n",
              "       0.3341035 , 0.33391223, 0.33391992, 0.33375718, 0.33401661,\n",
              "       0.33389843, 0.33385623, 0.33365851, 0.33398489, 0.33401291,\n",
              "       0.33363421, 0.33370003, 0.33376696, 0.33363231, 0.33368445,\n",
              "       0.33374428, 0.33412134, 0.33389762, 0.33368322, 0.33304429,\n",
              "       0.33379299, 0.33365155, 0.3334647 , 0.33338936, 0.3332516 ])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi11JrGQ3Syv",
        "outputId": "8bda11d8-1c40-4f0d-b750-10bf5b6070de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.33333333, 0.33333333, 0.33333333],\n",
              "       [0.33333385, 0.33333802, 0.33332812],\n",
              "       [0.3333282 , 0.33335732, 0.33331448],\n",
              "       [0.33332967, 0.33336051, 0.33330983],\n",
              "       [0.33333399, 0.33335584, 0.33331016],\n",
              "       [0.33333004, 0.33337189, 0.33329807],\n",
              "       [0.33331736, 0.33340642, 0.33327622],\n",
              "       [0.33332072, 0.33340594, 0.33327333],\n",
              "       [0.33332576, 0.33340059, 0.33327365],\n",
              "       [0.33331307, 0.33343541, 0.33325152],\n",
              "       [0.33331791, 0.33343126, 0.33325083],\n",
              "       [0.33331568, 0.33344262, 0.33324171],\n",
              "       [0.33331181, 0.33345759, 0.3332306 ],\n",
              "       [0.33331876, 0.333448  , 0.33323325],\n",
              "       [0.33330128, 0.33349322, 0.3332055 ],\n",
              "       [0.33328589, 0.33353039, 0.33318371],\n",
              "       [0.3332835 , 0.33354212, 0.33317439],\n",
              "       [0.33327067, 0.33357207, 0.33315726],\n",
              "       [0.33327823, 0.33356643, 0.33315534],\n",
              "       [0.33325459, 0.33361329, 0.33313212],\n",
              "       [0.33324686, 0.33363274, 0.3331204 ],\n",
              "       [0.33326745, 0.33360821, 0.33312433],\n",
              "       [0.33324615, 0.33365177, 0.33310208],\n",
              "       [0.33324692, 0.33365896, 0.33309413],\n",
              "       [0.33322789, 0.33369467, 0.33307745],\n",
              "       [0.33323375, 0.33369594, 0.33307031],\n",
              "       [0.33322445, 0.33371796, 0.33305759],\n",
              "       [0.33320969, 0.33374542, 0.33304489],\n",
              "       [0.33319012, 0.33377039, 0.33303948],\n",
              "       [0.33321439, 0.33375889, 0.33302673],\n",
              "       [0.33318643, 0.33379914, 0.33301443],\n",
              "       [0.33317534, 0.33381716, 0.3330075 ],\n",
              "       [0.33315598, 0.33381447, 0.33302955],\n",
              "       [0.33314969, 0.33382388, 0.33302643],\n",
              "       [0.33315048, 0.33386009, 0.33298944],\n",
              "       [0.33314171, 0.33386914, 0.33298915],\n",
              "       [0.33313509, 0.33380742, 0.33305749],\n",
              "       [0.33312669, 0.33386022, 0.33301309],\n",
              "       [0.33312193, 0.33389915, 0.33297891],\n",
              "       [0.33311618, 0.33391227, 0.33297154],\n",
              "       [0.33311535, 0.33394829, 0.33293636],\n",
              "       [0.33311591, 0.33381547, 0.33306862],\n",
              "       [0.33310833, 0.33384094, 0.33305072],\n",
              "       [0.33309371, 0.3339319 , 0.33297439],\n",
              "       [0.33309127, 0.33391057, 0.33299816],\n",
              "       [0.33313577, 0.33367399, 0.33319024],\n",
              "       [0.33310123, 0.3338119 , 0.33308688],\n",
              "       [0.33307923, 0.33391625, 0.33300452],\n",
              "       [0.33307459, 0.33392355, 0.33300185],\n",
              "       [0.33307883, 0.33387983, 0.33304134],\n",
              "       [0.33305435, 0.3340688 , 0.33287685],\n",
              "       [0.33308783, 0.33380928, 0.33310289],\n",
              "       [0.33307942, 0.33383466, 0.33308592],\n",
              "       [0.33304443, 0.3340071 , 0.33294847],\n",
              "       [0.33308279, 0.33379689, 0.33312032],\n",
              "       [0.3331029 , 0.33370692, 0.33319018],\n",
              "       [0.33308512, 0.33376662, 0.33314826],\n",
              "       [0.33311225, 0.33365472, 0.33323303],\n",
              "       [0.33312455, 0.33360287, 0.33327258],\n",
              "       [0.33314497, 0.33352564, 0.3333294 ],\n",
              "       [0.33313373, 0.33355588, 0.33331039],\n",
              "       [0.33329439, 0.33306649, 0.33363912],\n",
              "       [0.33320567, 0.33331325, 0.33348108],\n",
              "       [0.33314358, 0.33350109, 0.33335533],\n",
              "       [0.33316875, 0.33341341, 0.33341783],\n",
              "       [0.33314303, 0.33348856, 0.3333684 ],\n",
              "       [0.33321668, 0.33325473, 0.3335286 ],\n",
              "       [0.33323278, 0.33320152, 0.3335657 ],\n",
              "       [0.33315841, 0.33341861, 0.33342298],\n",
              "       [0.33314651, 0.33344935, 0.33340415],\n",
              "       [0.33322197, 0.33321385, 0.33356418],\n",
              "       [0.33329916, 0.33299273, 0.33370812],\n",
              "       [0.33318067, 0.33332334, 0.333496  ],\n",
              "       [0.33314063, 0.33344039, 0.33341899],\n",
              "       [0.33324985, 0.33310935, 0.3336408 ],\n",
              "       [0.33322844, 0.33316376, 0.33360781],\n",
              "       [0.33328949, 0.33298819, 0.33372232],\n",
              "       [0.33329636, 0.33296367, 0.33373997],\n",
              "       [0.33336218, 0.33278715, 0.33385067],\n",
              "       [0.33333612, 0.33284727, 0.33381661],\n",
              "       [0.33336145, 0.33277681, 0.33386174],\n",
              "       [0.33325798, 0.33304354, 0.33369848],\n",
              "       [0.33341025, 0.33264492, 0.33394483],\n",
              "       [0.33355353, 0.33232289, 0.33412358],\n",
              "       [0.33357898, 0.33226628, 0.33415475],\n",
              "       [0.33331328, 0.33287022, 0.3338165 ],\n",
              "       [0.33356634, 0.33227672, 0.33415694],\n",
              "       [0.33348901, 0.33243224, 0.33407875],\n",
              "       [0.33352946, 0.33233844, 0.33413211],\n",
              "       [0.33354007, 0.33230957, 0.33415036],\n",
              "       [0.33361176, 0.33216136, 0.33422689],\n",
              "       [0.33358785, 0.33219963, 0.33421252],\n",
              "       [0.33369696, 0.33200243, 0.33430062],\n",
              "       [0.3336751 , 0.33202806, 0.33429685],\n",
              "       [0.33364609, 0.33206948, 0.33428443],\n",
              "       [0.33364244, 0.33206845, 0.33428911],\n",
              "       [0.33366049, 0.332029  , 0.33431051],\n",
              "       [0.33363014, 0.3320762 , 0.33429366],\n",
              "       [0.3338193 , 0.33180965, 0.33437106],\n",
              "       [0.33361394, 0.33209265, 0.33429341],\n",
              "       [0.33333333, 0.33333333, 0.33333333],\n",
              "       [0.33333797, 0.33331801, 0.33334402],\n",
              "       [0.33334406, 0.33330234, 0.3333536 ],\n",
              "       [0.33334518, 0.33328997, 0.33336485],\n",
              "       [0.33334947, 0.33327502, 0.33337552],\n",
              "       [0.33335656, 0.3332567 , 0.33338674],\n",
              "       [0.33336224, 0.3332405 , 0.33339726],\n",
              "       [0.33336894, 0.33322413, 0.33340692],\n",
              "       [0.33336876, 0.33321255, 0.33341868],\n",
              "       [0.33338028, 0.33319298, 0.33342675],\n",
              "       [0.33338001, 0.33317985, 0.33344014],\n",
              "       [0.33339459, 0.33317102, 0.3334344 ],\n",
              "       [0.33339884, 0.33314941, 0.33345175],\n",
              "       [0.33340308, 0.33313193, 0.33346499],\n",
              "       [0.33341039, 0.33312077, 0.33346884],\n",
              "       [0.33341273, 0.33309996, 0.33348731],\n",
              "       [0.33342252, 0.33309879, 0.33347869],\n",
              "       [0.33342589, 0.33307224, 0.33350187],\n",
              "       [0.3334333 , 0.33306509, 0.3335016 ],\n",
              "       [0.33343684, 0.3330417 , 0.33352146],\n",
              "       [0.33344493, 0.33304431, 0.33351076],\n",
              "       [0.33344841, 0.33306089, 0.3334907 ],\n",
              "       [0.33344613, 0.33309534, 0.33345853],\n",
              "       [0.33343437, 0.33315894, 0.33340668],\n",
              "       [0.33346713, 0.33299495, 0.33353793],\n",
              "       [0.3334708 , 0.33295343, 0.33357577],\n",
              "       [0.33347372, 0.33301138, 0.3335149 ],\n",
              "       [0.33345926, 0.33309872, 0.33344202],\n",
              "       [0.3334886 , 0.33291454, 0.33359685],\n",
              "       [0.33348996, 0.33297393, 0.33353611],\n",
              "       [0.33348901, 0.33299903, 0.33351195],\n",
              "       [0.33349116, 0.33300355, 0.33350529],\n",
              "       [0.33349431, 0.3330025 , 0.33350319],\n",
              "       [0.33346707, 0.33312621, 0.33340672],\n",
              "       [0.33349591, 0.33302066, 0.33348343],\n",
              "       [0.33351003, 0.3329684 , 0.33352157],\n",
              "       [0.33348194, 0.33309722, 0.33342084],\n",
              "       [0.33346745, 0.33315821, 0.33337434],\n",
              "       [0.3334885 , 0.33309047, 0.33342103],\n",
              "       [0.33347963, 0.33313136, 0.33338902],\n",
              "       [0.33351843, 0.33299189, 0.33348968],\n",
              "       [0.33349914, 0.33307731, 0.33342355],\n",
              "       [0.33344075, 0.3332827 , 0.33327656],\n",
              "       [0.33349227, 0.33311904, 0.3333887 ],\n",
              "       [0.33344526, 0.33328216, 0.33327258],\n",
              "       [0.33351536, 0.33305216, 0.33343247],\n",
              "       [0.33347505, 0.33320056, 0.33332439],\n",
              "       [0.33336941, 0.3335222 , 0.33310839],\n",
              "       [0.33339871, 0.33344674, 0.33315455],\n",
              "       [0.33341786, 0.33339775, 0.33318439],\n",
              "       [0.3333983 , 0.33346036, 0.33314134],\n",
              "       [0.33339815, 0.33346701, 0.33313484],\n",
              "       [0.33343252, 0.33337371, 0.33319377],\n",
              "       [0.33340862, 0.3334497 , 0.33314168],\n",
              "       [0.33334852, 0.33362062, 0.33303086],\n",
              "       [0.3333805 , 0.33354081, 0.3330787 ],\n",
              "       [0.33326447, 0.3338412 , 0.33289433],\n",
              "       [0.3333294 , 0.33368836, 0.33298223],\n",
              "       [0.33321293, 0.33396918, 0.33281789],\n",
              "       [0.33331095, 0.33374717, 0.33294188],\n",
              "       [0.33332352, 0.33372151, 0.33295496],\n",
              "       [0.33326261, 0.33387649, 0.3328609 ],\n",
              "       [0.33327735, 0.33384768, 0.33287497],\n",
              "       [0.33325631, 0.33390359, 0.3328401 ],\n",
              "       [0.3331873 , 0.3340628 , 0.3327499 ],\n",
              "       [0.33324912, 0.33393271, 0.33281817],\n",
              "       [0.33310684, 0.33423169, 0.33266147],\n",
              "       [0.33319373, 0.33406888, 0.33273739],\n",
              "       [0.3332823 , 0.33387242, 0.33284528],\n",
              "       [0.33303895, 0.33436111, 0.33259993],\n",
              "       [0.3332459 , 0.3339712 , 0.33278289],\n",
              "       [0.33321564, 0.33404653, 0.33273783],\n",
              "       [0.33301634, 0.33441805, 0.33256561],\n",
              "       [0.3331349 , 0.33423008, 0.33263503],\n",
              "       [0.33312027, 0.33426555, 0.33261417],\n",
              "       [0.33307813, 0.33435034, 0.33257153],\n",
              "       [0.33310659, 0.33430586, 0.33258755],\n",
              "       [0.33299168, 0.3344965 , 0.33251183],\n",
              "       [0.3330356 , 0.33444464, 0.33251976],\n",
              "       [0.3330808 , 0.33437549, 0.3325437 ],\n",
              "       [0.33299636, 0.33451921, 0.33248443],\n",
              "       [0.33301184, 0.33450616, 0.332482  ],\n",
              "       [0.33292784, 0.33460916, 0.332463  ],\n",
              "       [0.3329003 , 0.33462934, 0.33247036],\n",
              "       [0.33291105, 0.33464339, 0.33244555],\n",
              "       [0.33292729, 0.33464622, 0.33242649],\n",
              "       [0.33287756, 0.33467332, 0.33244912],\n",
              "       [0.33299624, 0.33458058, 0.33242318],\n",
              "       [0.33286822, 0.33470527, 0.33242651],\n",
              "       [0.33285469, 0.33471391, 0.3324314 ],\n",
              "       [0.33283565, 0.33469687, 0.33246748],\n",
              "       [0.33283329, 0.33472396, 0.33244275],\n",
              "       [0.33286235, 0.33477036, 0.33236729],\n",
              "       [0.33289536, 0.33476449, 0.33234016],\n",
              "       [0.33283427, 0.3347976 , 0.33236813],\n",
              "       [0.33282853, 0.3348129 , 0.33235858],\n",
              "       [0.33279811, 0.33475894, 0.33244295],\n",
              "       [0.33279155, 0.33469813, 0.33251032],\n",
              "       [0.33279213, 0.33463407, 0.33257381],\n",
              "       [0.33283673, 0.33434854, 0.33281473],\n",
              "       [0.33333333, 0.33333333, 0.33333333],\n",
              "       [0.33332781, 0.3333467 , 0.33332549],\n",
              "       [0.3333222 , 0.33336116, 0.33331665],\n",
              "       [0.33331661, 0.33337736, 0.33330603],\n",
              "       [0.33331332, 0.33337413, 0.33331254],\n",
              "       [0.33330964, 0.33337836, 0.33331199],\n",
              "       [0.33330244, 0.33339887, 0.33329869],\n",
              "       [0.33329885, 0.33340225, 0.3332989 ],\n",
              "       [0.33329513, 0.33340666, 0.33329821],\n",
              "       [0.33329484, 0.33339747, 0.33330769],\n",
              "       [0.33328898, 0.33341083, 0.33330019],\n",
              "       [0.33328048, 0.33343558, 0.33328394],\n",
              "       [0.33327434, 0.3334509 , 0.33327476],\n",
              "       [0.33326766, 0.333469  , 0.33326334],\n",
              "       [0.33327883, 0.333413  , 0.33330817],\n",
              "       [0.33326886, 0.3334415 , 0.33328965],\n",
              "       [0.3332755 , 0.3334085 , 0.333316  ],\n",
              "       [0.33325696, 0.33346899, 0.33327405],\n",
              "       [0.33325579, 0.33346378, 0.33328043],\n",
              "       [0.33326197, 0.33343207, 0.33330596],\n",
              "       [0.33326332, 0.3334194 , 0.33331729],\n",
              "       [0.3332334 , 0.33352428, 0.33324232],\n",
              "       [0.33324518, 0.33346771, 0.33328711],\n",
              "       [0.33325996, 0.3334081 , 0.33333195],\n",
              "       [0.33324647, 0.33344649, 0.33330704],\n",
              "       [0.33327699, 0.33333999, 0.33338302],\n",
              "       [0.33327791, 0.33333061, 0.33339148],\n",
              "       [0.33330284, 0.33325174, 0.33344542],\n",
              "       [0.33324343, 0.33342626, 0.33333031],\n",
              "       [0.33323514, 0.33344672, 0.33331814],\n",
              "       [0.3333084 , 0.33321788, 0.33347372],\n",
              "       [0.33324849, 0.33338833, 0.33336318],\n",
              "       [0.3333273 , 0.33315512, 0.33351758],\n",
              "       [0.33334912, 0.33309346, 0.33355743],\n",
              "       [0.33339464, 0.33298036, 0.33362499],\n",
              "       [0.33337905, 0.33300933, 0.33361162],\n",
              "       [0.33324126, 0.33337678, 0.33338196],\n",
              "       [0.33342112, 0.33290415, 0.33367473],\n",
              "       [0.33329626, 0.3332022 , 0.33350153],\n",
              "       [0.33340672, 0.3329217 , 0.33367158],\n",
              "       [0.33337537, 0.33298695, 0.33363768],\n",
              "       [0.33332282, 0.33311228, 0.3335649 ],\n",
              "       [0.33340925, 0.33289687, 0.33369388],\n",
              "       [0.33344284, 0.33281924, 0.33373793],\n",
              "       [0.3333944 , 0.33291767, 0.33368793],\n",
              "       [0.33339987, 0.33289888, 0.33370125],\n",
              "       [0.33345481, 0.33277466, 0.33377053],\n",
              "       [0.33346182, 0.33275386, 0.33378433],\n",
              "       [0.33347277, 0.33272556, 0.33380166],\n",
              "       [0.33345896, 0.33274576, 0.33379528],\n",
              "       [0.33354178, 0.33259477, 0.33386345],\n",
              "       [0.33354149, 0.33258616, 0.33387235],\n",
              "       [0.33358144, 0.33253073, 0.33388782],\n",
              "       [0.33355831, 0.33254512, 0.33389657],\n",
              "       [0.33352013, 0.3325951 , 0.33388478],\n",
              "       [0.33356564, 0.33251688, 0.33391748],\n",
              "       [0.33355301, 0.33252583, 0.33392116],\n",
              "       [0.3335333 , 0.3325493 , 0.33391741],\n",
              "       [0.33360208, 0.33244507, 0.33395285],\n",
              "       [0.33364869, 0.33241884, 0.33393247],\n",
              "       [0.33360605, 0.33241974, 0.33397421],\n",
              "       [0.33365453, 0.332384  , 0.33396147],\n",
              "       [0.3336317 , 0.33237452, 0.33399378],\n",
              "       [0.3336713 , 0.33235838, 0.33397032],\n",
              "       [0.33367952, 0.33234725, 0.33397323],\n",
              "       [0.33361125, 0.33236404, 0.33402471],\n",
              "       [0.33362082, 0.33234254, 0.33403664],\n",
              "       [0.3335884 , 0.33237904, 0.33403256],\n",
              "       [0.33370071, 0.33228494, 0.33401435],\n",
              "       [0.33370266, 0.33226465, 0.33403269],\n",
              "       [0.33367035, 0.33225073, 0.33407892],\n",
              "       [0.33372816, 0.33234646, 0.33392538],\n",
              "       [0.33371296, 0.3322133 , 0.33407373],\n",
              "       [0.33373159, 0.33239842, 0.33386999],\n",
              "       [0.33373675, 0.33220019, 0.33406306],\n",
              "       [0.33372941, 0.33216709, 0.3341035 ],\n",
              "       [0.33375072, 0.33233705, 0.33391223],\n",
              "       [0.33375622, 0.33232386, 0.33391992],\n",
              "       [0.33372811, 0.33251471, 0.33375718],\n",
              "       [0.33377356, 0.33220983, 0.33401661],\n",
              "       [0.3337661 , 0.33233547, 0.33389843],\n",
              "       [0.33376232, 0.33238144, 0.33385623],\n",
              "       [0.33371419, 0.3326273 , 0.33365851],\n",
              "       [0.33379139, 0.33222371, 0.33398489],\n",
              "       [0.33379885, 0.33218825, 0.33401291],\n",
              "       [0.33371659, 0.33264919, 0.33363421],\n",
              "       [0.3337406 , 0.33255936, 0.33370003],\n",
              "       [0.33376332, 0.33246973, 0.33376696],\n",
              "       [0.33372601, 0.33264167, 0.33363231],\n",
              "       [0.33374612, 0.33256942, 0.33368445],\n",
              "       [0.33376756, 0.33248816, 0.33374428],\n",
              "       [0.33384033, 0.33203833, 0.33412134],\n",
              "       [0.33381423, 0.33228815, 0.33389762],\n",
              "       [0.33375931, 0.33255747, 0.33368322],\n",
              "       [0.33349373, 0.33346198, 0.33304429],\n",
              "       [0.33379902, 0.33240799, 0.33379299],\n",
              "       [0.33375907, 0.33258938, 0.33365155],\n",
              "       [0.33369444, 0.33284086, 0.3334647 ],\n",
              "       [0.33366723, 0.33294341, 0.33338936],\n",
              "       [0.33361058, 0.33313782, 0.3332516 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg9jClP9rEV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72638ace-a459-448a-b75c-81d260f1a1e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0980003261250324"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def loss(y, probs):\n",
        "    m = y.shape[0]    #dz = probs\n",
        "    error = -np.log(probs[range(m), y])\n",
        "    return np.sum(error)/m\n",
        "loss(y, probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFtvIbLn-pEt"
      },
      "source": [
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=10RpkDV24RUonN49GjuZSW3wWsLkf0wRd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AYDLffdTUr5"
      },
      "source": [
        "## Backward propogation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEOnsPn-bxOK"
      },
      "source": [
        "We had the following NN for our multiclass classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tEB6rzyZTqG"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1LDBuwe2xUQ_CF9ymiHvxzb2ojuYq16oz' width=\"800\"></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOgfYrQsZURy"
      },
      "source": [
        "Let's draw computational graph for this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umfBGT3TTLtr"
      },
      "source": [
        "\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1B9cHNnwhrLp2tJlle8TmCktltlbtey3Z' width=600></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiJYH46xP75-"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#### Question: How did we decrease the loss in Logistic Regression?\n",
        "\n",
        "- We calculate partial derivative of J wrt each W and b.\n",
        "- And then we iteratively updated the params values of $W$ and $b$\n",
        "- What does the partial derivative explain inituitively? How does J change with a small change in any param?\n",
        "\n",
        "Problem: **We cannot directly calculate dW and db here**\n",
        "\n",
        "- But what we can see is the error J is directly dependent on `p`, so we can calculate $\\frac{\\partial J}{\\partial p_i}$ directly\n",
        "- Further, the `p` in-turn  depend on `z`, and `z` is of course dependent on `W` (and `b`).\n",
        "\n",
        "#### Lets break this down using chain rule  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci-NT0-ueRw7"
      },
      "source": [
        "\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1b_cQdk8t8YR2GBs78pDFYXZM-dRYwZPJ' width=700></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBAj5kyu7ht0"
      },
      "source": [
        "\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial w} =  \\frac{\\partial J}{\\partial z} \\frac{\\partial z}{\\partial w}$$\n",
        "\n",
        "We can see from the picture that $\\frac{\\partial J}{\\partial z}$ can be calculated using\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial w} =  \\frac{\\partial J}{\\partial p} \\frac{\\partial p}{\\partial z} \\frac{\\partial z}{\\partial w}$$\n",
        "\n",
        "- Since, J (cost function) is the final function we want to optimise, we would come across a lot of intermediate calculations like $\\frac{\\partial J}{\\partial <var>}$.\n",
        "- In Python, we will represent gradients of J, $\\frac{\\partial J}{\\partial w}$ as `dw`.\n",
        "- And the equation $\\frac{\\partial J}{\\partial w} =  \\frac{\\partial J}{\\partial z} \\frac{\\partial z}{\\partial w}$ can be represented as\n",
        "$$dw =  dz  \\frac{\\partial z}{\\partial w}$$\n",
        "\n",
        "Since, we are moving from right to left across the computational graph, we call this process backpropagation.\n",
        "\n",
        "<hr style=\"border:1px solid gray\"> </hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz0K71j27ht0"
      },
      "source": [
        "### Let's calculate gradients using backpropagation\n",
        "\n",
        "- We previously calculated $\\frac{\\partial z}{\\partial w}$ in LR - since $z=w^Tx$, $\\frac{\\partial z}{\\partial w}=x$\n",
        "- Now, we need to calculate `dz`, $dz=\\frac{\\partial J}{\\partial p} \\frac{\\partial p}{\\partial z}$\n",
        "\n",
        "Turns out that the result for `dz` has a very neat and intuitive solution (won't get into derivation)\n",
        "\n",
        "$$dz = (p_k - I (i=k))$$\n",
        "\n",
        "- First, the second term is called an **Indicator Function**,\n",
        "    - where p_k  is the prob. of class K\n",
        "    - it is 1 when `i==k`, else 0.\n",
        "- Suppose `probs = [0.2, 0.3, 0.5]`, and that the correct class was the middle one  (0.3).\n",
        "- According to the formula, gradients would be `dz = [0.2, -0.7, 0.5]`\n",
        "- If we were saving ground truth, y,  as one-hot encoded vectors, we could have done this by just doing $p -y$\n",
        "\n",
        "We won't go into its derivation, but lets intuitively understand this\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Why are we subtracting 1 only from the probability of true class?\n",
        "- The use of subtracting 1 from the true class is to make it derivative negative.\n",
        "- For the true class (middle), increasing its probability values will decrease the error (0.3 --> -0.7) - aligns with goal.\n",
        "- For other values, first and last, increase (or keeping it same) their probability values will increase the error - aligns with goal."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY7ocwMl4lgE",
        "outputId": "a24b8b51-8fc2-435c-cf08-3f1a8c7d53d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.33333333, 0.33333333, 0.33333333],\n",
              "       [0.33333385, 0.33333802, 0.33332812],\n",
              "       [0.3333282 , 0.33335732, 0.33331448],\n",
              "       [0.33332967, 0.33336051, 0.33330983],\n",
              "       [0.33333399, 0.33335584, 0.33331016],\n",
              "       [0.33333004, 0.33337189, 0.33329807],\n",
              "       [0.33331736, 0.33340642, 0.33327622],\n",
              "       [0.33332072, 0.33340594, 0.33327333],\n",
              "       [0.33332576, 0.33340059, 0.33327365],\n",
              "       [0.33331307, 0.33343541, 0.33325152],\n",
              "       [0.33331791, 0.33343126, 0.33325083],\n",
              "       [0.33331568, 0.33344262, 0.33324171],\n",
              "       [0.33331181, 0.33345759, 0.3332306 ],\n",
              "       [0.33331876, 0.333448  , 0.33323325],\n",
              "       [0.33330128, 0.33349322, 0.3332055 ],\n",
              "       [0.33328589, 0.33353039, 0.33318371],\n",
              "       [0.3332835 , 0.33354212, 0.33317439],\n",
              "       [0.33327067, 0.33357207, 0.33315726],\n",
              "       [0.33327823, 0.33356643, 0.33315534],\n",
              "       [0.33325459, 0.33361329, 0.33313212],\n",
              "       [0.33324686, 0.33363274, 0.3331204 ],\n",
              "       [0.33326745, 0.33360821, 0.33312433],\n",
              "       [0.33324615, 0.33365177, 0.33310208],\n",
              "       [0.33324692, 0.33365896, 0.33309413],\n",
              "       [0.33322789, 0.33369467, 0.33307745],\n",
              "       [0.33323375, 0.33369594, 0.33307031],\n",
              "       [0.33322445, 0.33371796, 0.33305759],\n",
              "       [0.33320969, 0.33374542, 0.33304489],\n",
              "       [0.33319012, 0.33377039, 0.33303948],\n",
              "       [0.33321439, 0.33375889, 0.33302673],\n",
              "       [0.33318643, 0.33379914, 0.33301443],\n",
              "       [0.33317534, 0.33381716, 0.3330075 ],\n",
              "       [0.33315598, 0.33381447, 0.33302955],\n",
              "       [0.33314969, 0.33382388, 0.33302643],\n",
              "       [0.33315048, 0.33386009, 0.33298944],\n",
              "       [0.33314171, 0.33386914, 0.33298915],\n",
              "       [0.33313509, 0.33380742, 0.33305749],\n",
              "       [0.33312669, 0.33386022, 0.33301309],\n",
              "       [0.33312193, 0.33389915, 0.33297891],\n",
              "       [0.33311618, 0.33391227, 0.33297154],\n",
              "       [0.33311535, 0.33394829, 0.33293636],\n",
              "       [0.33311591, 0.33381547, 0.33306862],\n",
              "       [0.33310833, 0.33384094, 0.33305072],\n",
              "       [0.33309371, 0.3339319 , 0.33297439],\n",
              "       [0.33309127, 0.33391057, 0.33299816],\n",
              "       [0.33313577, 0.33367399, 0.33319024],\n",
              "       [0.33310123, 0.3338119 , 0.33308688],\n",
              "       [0.33307923, 0.33391625, 0.33300452],\n",
              "       [0.33307459, 0.33392355, 0.33300185],\n",
              "       [0.33307883, 0.33387983, 0.33304134],\n",
              "       [0.33305435, 0.3340688 , 0.33287685],\n",
              "       [0.33308783, 0.33380928, 0.33310289],\n",
              "       [0.33307942, 0.33383466, 0.33308592],\n",
              "       [0.33304443, 0.3340071 , 0.33294847],\n",
              "       [0.33308279, 0.33379689, 0.33312032],\n",
              "       [0.3331029 , 0.33370692, 0.33319018],\n",
              "       [0.33308512, 0.33376662, 0.33314826],\n",
              "       [0.33311225, 0.33365472, 0.33323303],\n",
              "       [0.33312455, 0.33360287, 0.33327258],\n",
              "       [0.33314497, 0.33352564, 0.3333294 ],\n",
              "       [0.33313373, 0.33355588, 0.33331039],\n",
              "       [0.33329439, 0.33306649, 0.33363912],\n",
              "       [0.33320567, 0.33331325, 0.33348108],\n",
              "       [0.33314358, 0.33350109, 0.33335533],\n",
              "       [0.33316875, 0.33341341, 0.33341783],\n",
              "       [0.33314303, 0.33348856, 0.3333684 ],\n",
              "       [0.33321668, 0.33325473, 0.3335286 ],\n",
              "       [0.33323278, 0.33320152, 0.3335657 ],\n",
              "       [0.33315841, 0.33341861, 0.33342298],\n",
              "       [0.33314651, 0.33344935, 0.33340415],\n",
              "       [0.33322197, 0.33321385, 0.33356418],\n",
              "       [0.33329916, 0.33299273, 0.33370812],\n",
              "       [0.33318067, 0.33332334, 0.333496  ],\n",
              "       [0.33314063, 0.33344039, 0.33341899],\n",
              "       [0.33324985, 0.33310935, 0.3336408 ],\n",
              "       [0.33322844, 0.33316376, 0.33360781],\n",
              "       [0.33328949, 0.33298819, 0.33372232],\n",
              "       [0.33329636, 0.33296367, 0.33373997],\n",
              "       [0.33336218, 0.33278715, 0.33385067],\n",
              "       [0.33333612, 0.33284727, 0.33381661],\n",
              "       [0.33336145, 0.33277681, 0.33386174],\n",
              "       [0.33325798, 0.33304354, 0.33369848],\n",
              "       [0.33341025, 0.33264492, 0.33394483],\n",
              "       [0.33355353, 0.33232289, 0.33412358],\n",
              "       [0.33357898, 0.33226628, 0.33415475],\n",
              "       [0.33331328, 0.33287022, 0.3338165 ],\n",
              "       [0.33356634, 0.33227672, 0.33415694],\n",
              "       [0.33348901, 0.33243224, 0.33407875],\n",
              "       [0.33352946, 0.33233844, 0.33413211],\n",
              "       [0.33354007, 0.33230957, 0.33415036],\n",
              "       [0.33361176, 0.33216136, 0.33422689],\n",
              "       [0.33358785, 0.33219963, 0.33421252],\n",
              "       [0.33369696, 0.33200243, 0.33430062],\n",
              "       [0.3336751 , 0.33202806, 0.33429685],\n",
              "       [0.33364609, 0.33206948, 0.33428443],\n",
              "       [0.33364244, 0.33206845, 0.33428911],\n",
              "       [0.33366049, 0.332029  , 0.33431051],\n",
              "       [0.33363014, 0.3320762 , 0.33429366],\n",
              "       [0.3338193 , 0.33180965, 0.33437106],\n",
              "       [0.33361394, 0.33209265, 0.33429341],\n",
              "       [0.33333333, 0.33333333, 0.33333333],\n",
              "       [0.33333797, 0.33331801, 0.33334402],\n",
              "       [0.33334406, 0.33330234, 0.3333536 ],\n",
              "       [0.33334518, 0.33328997, 0.33336485],\n",
              "       [0.33334947, 0.33327502, 0.33337552],\n",
              "       [0.33335656, 0.3332567 , 0.33338674],\n",
              "       [0.33336224, 0.3332405 , 0.33339726],\n",
              "       [0.33336894, 0.33322413, 0.33340692],\n",
              "       [0.33336876, 0.33321255, 0.33341868],\n",
              "       [0.33338028, 0.33319298, 0.33342675],\n",
              "       [0.33338001, 0.33317985, 0.33344014],\n",
              "       [0.33339459, 0.33317102, 0.3334344 ],\n",
              "       [0.33339884, 0.33314941, 0.33345175],\n",
              "       [0.33340308, 0.33313193, 0.33346499],\n",
              "       [0.33341039, 0.33312077, 0.33346884],\n",
              "       [0.33341273, 0.33309996, 0.33348731],\n",
              "       [0.33342252, 0.33309879, 0.33347869],\n",
              "       [0.33342589, 0.33307224, 0.33350187],\n",
              "       [0.3334333 , 0.33306509, 0.3335016 ],\n",
              "       [0.33343684, 0.3330417 , 0.33352146],\n",
              "       [0.33344493, 0.33304431, 0.33351076],\n",
              "       [0.33344841, 0.33306089, 0.3334907 ],\n",
              "       [0.33344613, 0.33309534, 0.33345853],\n",
              "       [0.33343437, 0.33315894, 0.33340668],\n",
              "       [0.33346713, 0.33299495, 0.33353793],\n",
              "       [0.3334708 , 0.33295343, 0.33357577],\n",
              "       [0.33347372, 0.33301138, 0.3335149 ],\n",
              "       [0.33345926, 0.33309872, 0.33344202],\n",
              "       [0.3334886 , 0.33291454, 0.33359685],\n",
              "       [0.33348996, 0.33297393, 0.33353611],\n",
              "       [0.33348901, 0.33299903, 0.33351195],\n",
              "       [0.33349116, 0.33300355, 0.33350529],\n",
              "       [0.33349431, 0.3330025 , 0.33350319],\n",
              "       [0.33346707, 0.33312621, 0.33340672],\n",
              "       [0.33349591, 0.33302066, 0.33348343],\n",
              "       [0.33351003, 0.3329684 , 0.33352157],\n",
              "       [0.33348194, 0.33309722, 0.33342084],\n",
              "       [0.33346745, 0.33315821, 0.33337434],\n",
              "       [0.3334885 , 0.33309047, 0.33342103],\n",
              "       [0.33347963, 0.33313136, 0.33338902],\n",
              "       [0.33351843, 0.33299189, 0.33348968],\n",
              "       [0.33349914, 0.33307731, 0.33342355],\n",
              "       [0.33344075, 0.3332827 , 0.33327656],\n",
              "       [0.33349227, 0.33311904, 0.3333887 ],\n",
              "       [0.33344526, 0.33328216, 0.33327258],\n",
              "       [0.33351536, 0.33305216, 0.33343247],\n",
              "       [0.33347505, 0.33320056, 0.33332439],\n",
              "       [0.33336941, 0.3335222 , 0.33310839],\n",
              "       [0.33339871, 0.33344674, 0.33315455],\n",
              "       [0.33341786, 0.33339775, 0.33318439],\n",
              "       [0.3333983 , 0.33346036, 0.33314134],\n",
              "       [0.33339815, 0.33346701, 0.33313484],\n",
              "       [0.33343252, 0.33337371, 0.33319377],\n",
              "       [0.33340862, 0.3334497 , 0.33314168],\n",
              "       [0.33334852, 0.33362062, 0.33303086],\n",
              "       [0.3333805 , 0.33354081, 0.3330787 ],\n",
              "       [0.33326447, 0.3338412 , 0.33289433],\n",
              "       [0.3333294 , 0.33368836, 0.33298223],\n",
              "       [0.33321293, 0.33396918, 0.33281789],\n",
              "       [0.33331095, 0.33374717, 0.33294188],\n",
              "       [0.33332352, 0.33372151, 0.33295496],\n",
              "       [0.33326261, 0.33387649, 0.3328609 ],\n",
              "       [0.33327735, 0.33384768, 0.33287497],\n",
              "       [0.33325631, 0.33390359, 0.3328401 ],\n",
              "       [0.3331873 , 0.3340628 , 0.3327499 ],\n",
              "       [0.33324912, 0.33393271, 0.33281817],\n",
              "       [0.33310684, 0.33423169, 0.33266147],\n",
              "       [0.33319373, 0.33406888, 0.33273739],\n",
              "       [0.3332823 , 0.33387242, 0.33284528],\n",
              "       [0.33303895, 0.33436111, 0.33259993],\n",
              "       [0.3332459 , 0.3339712 , 0.33278289],\n",
              "       [0.33321564, 0.33404653, 0.33273783],\n",
              "       [0.33301634, 0.33441805, 0.33256561],\n",
              "       [0.3331349 , 0.33423008, 0.33263503],\n",
              "       [0.33312027, 0.33426555, 0.33261417],\n",
              "       [0.33307813, 0.33435034, 0.33257153],\n",
              "       [0.33310659, 0.33430586, 0.33258755],\n",
              "       [0.33299168, 0.3344965 , 0.33251183],\n",
              "       [0.3330356 , 0.33444464, 0.33251976],\n",
              "       [0.3330808 , 0.33437549, 0.3325437 ],\n",
              "       [0.33299636, 0.33451921, 0.33248443],\n",
              "       [0.33301184, 0.33450616, 0.332482  ],\n",
              "       [0.33292784, 0.33460916, 0.332463  ],\n",
              "       [0.3329003 , 0.33462934, 0.33247036],\n",
              "       [0.33291105, 0.33464339, 0.33244555],\n",
              "       [0.33292729, 0.33464622, 0.33242649],\n",
              "       [0.33287756, 0.33467332, 0.33244912],\n",
              "       [0.33299624, 0.33458058, 0.33242318],\n",
              "       [0.33286822, 0.33470527, 0.33242651],\n",
              "       [0.33285469, 0.33471391, 0.3324314 ],\n",
              "       [0.33283565, 0.33469687, 0.33246748],\n",
              "       [0.33283329, 0.33472396, 0.33244275],\n",
              "       [0.33286235, 0.33477036, 0.33236729],\n",
              "       [0.33289536, 0.33476449, 0.33234016],\n",
              "       [0.33283427, 0.3347976 , 0.33236813],\n",
              "       [0.33282853, 0.3348129 , 0.33235858],\n",
              "       [0.33279811, 0.33475894, 0.33244295],\n",
              "       [0.33279155, 0.33469813, 0.33251032],\n",
              "       [0.33279213, 0.33463407, 0.33257381],\n",
              "       [0.33283673, 0.33434854, 0.33281473],\n",
              "       [0.33333333, 0.33333333, 0.33333333],\n",
              "       [0.33332781, 0.3333467 , 0.33332549],\n",
              "       [0.3333222 , 0.33336116, 0.33331665],\n",
              "       [0.33331661, 0.33337736, 0.33330603],\n",
              "       [0.33331332, 0.33337413, 0.33331254],\n",
              "       [0.33330964, 0.33337836, 0.33331199],\n",
              "       [0.33330244, 0.33339887, 0.33329869],\n",
              "       [0.33329885, 0.33340225, 0.3332989 ],\n",
              "       [0.33329513, 0.33340666, 0.33329821],\n",
              "       [0.33329484, 0.33339747, 0.33330769],\n",
              "       [0.33328898, 0.33341083, 0.33330019],\n",
              "       [0.33328048, 0.33343558, 0.33328394],\n",
              "       [0.33327434, 0.3334509 , 0.33327476],\n",
              "       [0.33326766, 0.333469  , 0.33326334],\n",
              "       [0.33327883, 0.333413  , 0.33330817],\n",
              "       [0.33326886, 0.3334415 , 0.33328965],\n",
              "       [0.3332755 , 0.3334085 , 0.333316  ],\n",
              "       [0.33325696, 0.33346899, 0.33327405],\n",
              "       [0.33325579, 0.33346378, 0.33328043],\n",
              "       [0.33326197, 0.33343207, 0.33330596],\n",
              "       [0.33326332, 0.3334194 , 0.33331729],\n",
              "       [0.3332334 , 0.33352428, 0.33324232],\n",
              "       [0.33324518, 0.33346771, 0.33328711],\n",
              "       [0.33325996, 0.3334081 , 0.33333195],\n",
              "       [0.33324647, 0.33344649, 0.33330704],\n",
              "       [0.33327699, 0.33333999, 0.33338302],\n",
              "       [0.33327791, 0.33333061, 0.33339148],\n",
              "       [0.33330284, 0.33325174, 0.33344542],\n",
              "       [0.33324343, 0.33342626, 0.33333031],\n",
              "       [0.33323514, 0.33344672, 0.33331814],\n",
              "       [0.3333084 , 0.33321788, 0.33347372],\n",
              "       [0.33324849, 0.33338833, 0.33336318],\n",
              "       [0.3333273 , 0.33315512, 0.33351758],\n",
              "       [0.33334912, 0.33309346, 0.33355743],\n",
              "       [0.33339464, 0.33298036, 0.33362499],\n",
              "       [0.33337905, 0.33300933, 0.33361162],\n",
              "       [0.33324126, 0.33337678, 0.33338196],\n",
              "       [0.33342112, 0.33290415, 0.33367473],\n",
              "       [0.33329626, 0.3332022 , 0.33350153],\n",
              "       [0.33340672, 0.3329217 , 0.33367158],\n",
              "       [0.33337537, 0.33298695, 0.33363768],\n",
              "       [0.33332282, 0.33311228, 0.3335649 ],\n",
              "       [0.33340925, 0.33289687, 0.33369388],\n",
              "       [0.33344284, 0.33281924, 0.33373793],\n",
              "       [0.3333944 , 0.33291767, 0.33368793],\n",
              "       [0.33339987, 0.33289888, 0.33370125],\n",
              "       [0.33345481, 0.33277466, 0.33377053],\n",
              "       [0.33346182, 0.33275386, 0.33378433],\n",
              "       [0.33347277, 0.33272556, 0.33380166],\n",
              "       [0.33345896, 0.33274576, 0.33379528],\n",
              "       [0.33354178, 0.33259477, 0.33386345],\n",
              "       [0.33354149, 0.33258616, 0.33387235],\n",
              "       [0.33358144, 0.33253073, 0.33388782],\n",
              "       [0.33355831, 0.33254512, 0.33389657],\n",
              "       [0.33352013, 0.3325951 , 0.33388478],\n",
              "       [0.33356564, 0.33251688, 0.33391748],\n",
              "       [0.33355301, 0.33252583, 0.33392116],\n",
              "       [0.3335333 , 0.3325493 , 0.33391741],\n",
              "       [0.33360208, 0.33244507, 0.33395285],\n",
              "       [0.33364869, 0.33241884, 0.33393247],\n",
              "       [0.33360605, 0.33241974, 0.33397421],\n",
              "       [0.33365453, 0.332384  , 0.33396147],\n",
              "       [0.3336317 , 0.33237452, 0.33399378],\n",
              "       [0.3336713 , 0.33235838, 0.33397032],\n",
              "       [0.33367952, 0.33234725, 0.33397323],\n",
              "       [0.33361125, 0.33236404, 0.33402471],\n",
              "       [0.33362082, 0.33234254, 0.33403664],\n",
              "       [0.3335884 , 0.33237904, 0.33403256],\n",
              "       [0.33370071, 0.33228494, 0.33401435],\n",
              "       [0.33370266, 0.33226465, 0.33403269],\n",
              "       [0.33367035, 0.33225073, 0.33407892],\n",
              "       [0.33372816, 0.33234646, 0.33392538],\n",
              "       [0.33371296, 0.3322133 , 0.33407373],\n",
              "       [0.33373159, 0.33239842, 0.33386999],\n",
              "       [0.33373675, 0.33220019, 0.33406306],\n",
              "       [0.33372941, 0.33216709, 0.3341035 ],\n",
              "       [0.33375072, 0.33233705, 0.33391223],\n",
              "       [0.33375622, 0.33232386, 0.33391992],\n",
              "       [0.33372811, 0.33251471, 0.33375718],\n",
              "       [0.33377356, 0.33220983, 0.33401661],\n",
              "       [0.3337661 , 0.33233547, 0.33389843],\n",
              "       [0.33376232, 0.33238144, 0.33385623],\n",
              "       [0.33371419, 0.3326273 , 0.33365851],\n",
              "       [0.33379139, 0.33222371, 0.33398489],\n",
              "       [0.33379885, 0.33218825, 0.33401291],\n",
              "       [0.33371659, 0.33264919, 0.33363421],\n",
              "       [0.3337406 , 0.33255936, 0.33370003],\n",
              "       [0.33376332, 0.33246973, 0.33376696],\n",
              "       [0.33372601, 0.33264167, 0.33363231],\n",
              "       [0.33374612, 0.33256942, 0.33368445],\n",
              "       [0.33376756, 0.33248816, 0.33374428],\n",
              "       [0.33384033, 0.33203833, 0.33412134],\n",
              "       [0.33381423, 0.33228815, 0.33389762],\n",
              "       [0.33375931, 0.33255747, 0.33368322],\n",
              "       [0.33349373, 0.33346198, 0.33304429],\n",
              "       [0.33379902, 0.33240799, 0.33379299],\n",
              "       [0.33375907, 0.33258938, 0.33365155],\n",
              "       [0.33369444, 0.33284086, 0.3334647 ],\n",
              "       [0.33366723, 0.33294341, 0.33338936],\n",
              "       [0.33361058, 0.33313782, 0.3332516 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dz = probs"
      ],
      "metadata": {
        "id": "za78c8M_4tKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "0.33333333 - 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS28xwwX5aEd",
        "outputId": "da9eed58-46a2-40c8-9f5e-8ca4efe5bf36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.66666667"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs[range(m),y]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2wxqiiH451H",
        "outputId": "83b647ab-b9f3-49ee-c54a-94389b44dd50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.33333333, 0.33333385, 0.3333282 , 0.33332967, 0.33333399,\n",
              "       0.33333004, 0.33331736, 0.33332072, 0.33332576, 0.33331307,\n",
              "       0.33331791, 0.33331568, 0.33331181, 0.33331876, 0.33330128,\n",
              "       0.33328589, 0.3332835 , 0.33327067, 0.33327823, 0.33325459,\n",
              "       0.33324686, 0.33326745, 0.33324615, 0.33324692, 0.33322789,\n",
              "       0.33323375, 0.33322445, 0.33320969, 0.33319012, 0.33321439,\n",
              "       0.33318643, 0.33317534, 0.33315598, 0.33314969, 0.33315048,\n",
              "       0.33314171, 0.33313509, 0.33312669, 0.33312193, 0.33311618,\n",
              "       0.33311535, 0.33311591, 0.33310833, 0.33309371, 0.33309127,\n",
              "       0.33313577, 0.33310123, 0.33307923, 0.33307459, 0.33307883,\n",
              "       0.33305435, 0.33308783, 0.33307942, 0.33304443, 0.33308279,\n",
              "       0.3331029 , 0.33308512, 0.33311225, 0.33312455, 0.33314497,\n",
              "       0.33313373, 0.33329439, 0.33320567, 0.33314358, 0.33316875,\n",
              "       0.33314303, 0.33321668, 0.33323278, 0.33315841, 0.33314651,\n",
              "       0.33322197, 0.33329916, 0.33318067, 0.33314063, 0.33324985,\n",
              "       0.33322844, 0.33328949, 0.33329636, 0.33336218, 0.33333612,\n",
              "       0.33336145, 0.33325798, 0.33341025, 0.33355353, 0.33357898,\n",
              "       0.33331328, 0.33356634, 0.33348901, 0.33352946, 0.33354007,\n",
              "       0.33361176, 0.33358785, 0.33369696, 0.3336751 , 0.33364609,\n",
              "       0.33364244, 0.33366049, 0.33363014, 0.3338193 , 0.33361394,\n",
              "       0.33333333, 0.33331801, 0.33330234, 0.33328997, 0.33327502,\n",
              "       0.3332567 , 0.3332405 , 0.33322413, 0.33321255, 0.33319298,\n",
              "       0.33317985, 0.33317102, 0.33314941, 0.33313193, 0.33312077,\n",
              "       0.33309996, 0.33309879, 0.33307224, 0.33306509, 0.3330417 ,\n",
              "       0.33304431, 0.33306089, 0.33309534, 0.33315894, 0.33299495,\n",
              "       0.33295343, 0.33301138, 0.33309872, 0.33291454, 0.33297393,\n",
              "       0.33299903, 0.33300355, 0.3330025 , 0.33312621, 0.33302066,\n",
              "       0.3329684 , 0.33309722, 0.33315821, 0.33309047, 0.33313136,\n",
              "       0.33299189, 0.33307731, 0.3332827 , 0.33311904, 0.33328216,\n",
              "       0.33305216, 0.33320056, 0.3335222 , 0.33344674, 0.33339775,\n",
              "       0.33346036, 0.33346701, 0.33337371, 0.3334497 , 0.33362062,\n",
              "       0.33354081, 0.3338412 , 0.33368836, 0.33396918, 0.33374717,\n",
              "       0.33372151, 0.33387649, 0.33384768, 0.33390359, 0.3340628 ,\n",
              "       0.33393271, 0.33423169, 0.33406888, 0.33387242, 0.33436111,\n",
              "       0.3339712 , 0.33404653, 0.33441805, 0.33423008, 0.33426555,\n",
              "       0.33435034, 0.33430586, 0.3344965 , 0.33444464, 0.33437549,\n",
              "       0.33451921, 0.33450616, 0.33460916, 0.33462934, 0.33464339,\n",
              "       0.33464622, 0.33467332, 0.33458058, 0.33470527, 0.33471391,\n",
              "       0.33469687, 0.33472396, 0.33477036, 0.33476449, 0.3347976 ,\n",
              "       0.3348129 , 0.33475894, 0.33469813, 0.33463407, 0.33434854,\n",
              "       0.33333333, 0.33332549, 0.33331665, 0.33330603, 0.33331254,\n",
              "       0.33331199, 0.33329869, 0.3332989 , 0.33329821, 0.33330769,\n",
              "       0.33330019, 0.33328394, 0.33327476, 0.33326334, 0.33330817,\n",
              "       0.33328965, 0.333316  , 0.33327405, 0.33328043, 0.33330596,\n",
              "       0.33331729, 0.33324232, 0.33328711, 0.33333195, 0.33330704,\n",
              "       0.33338302, 0.33339148, 0.33344542, 0.33333031, 0.33331814,\n",
              "       0.33347372, 0.33336318, 0.33351758, 0.33355743, 0.33362499,\n",
              "       0.33361162, 0.33338196, 0.33367473, 0.33350153, 0.33367158,\n",
              "       0.33363768, 0.3335649 , 0.33369388, 0.33373793, 0.33368793,\n",
              "       0.33370125, 0.33377053, 0.33378433, 0.33380166, 0.33379528,\n",
              "       0.33386345, 0.33387235, 0.33388782, 0.33389657, 0.33388478,\n",
              "       0.33391748, 0.33392116, 0.33391741, 0.33395285, 0.33393247,\n",
              "       0.33397421, 0.33396147, 0.33399378, 0.33397032, 0.33397323,\n",
              "       0.33402471, 0.33403664, 0.33403256, 0.33401435, 0.33403269,\n",
              "       0.33407892, 0.33392538, 0.33407373, 0.33386999, 0.33406306,\n",
              "       0.3341035 , 0.33391223, 0.33391992, 0.33375718, 0.33401661,\n",
              "       0.33389843, 0.33385623, 0.33365851, 0.33398489, 0.33401291,\n",
              "       0.33363421, 0.33370003, 0.33376696, 0.33363231, 0.33368445,\n",
              "       0.33374428, 0.33412134, 0.33389762, 0.33368322, 0.33304429,\n",
              "       0.33379299, 0.33365155, 0.3334647 , 0.33338936, 0.3332516 ])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs[range(m),y] - 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjrySlpY4x7J",
        "outputId": "a6849cdc-3b86-46db-a7f7-bb221cd1bc4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.66666667, -0.66666615, -0.6666718 , -0.66667033, -0.66666601,\n",
              "       -0.66666996, -0.66668264, -0.66667928, -0.66667424, -0.66668693,\n",
              "       -0.66668209, -0.66668432, -0.66668819, -0.66668124, -0.66669872,\n",
              "       -0.66671411, -0.6667165 , -0.66672933, -0.66672177, -0.66674541,\n",
              "       -0.66675314, -0.66673255, -0.66675385, -0.66675308, -0.66677211,\n",
              "       -0.66676625, -0.66677555, -0.66679031, -0.66680988, -0.66678561,\n",
              "       -0.66681357, -0.66682466, -0.66684402, -0.66685031, -0.66684952,\n",
              "       -0.66685829, -0.66686491, -0.66687331, -0.66687807, -0.66688382,\n",
              "       -0.66688465, -0.66688409, -0.66689167, -0.66690629, -0.66690873,\n",
              "       -0.66686423, -0.66689877, -0.66692077, -0.66692541, -0.66692117,\n",
              "       -0.66694565, -0.66691217, -0.66692058, -0.66695557, -0.66691721,\n",
              "       -0.6668971 , -0.66691488, -0.66688775, -0.66687545, -0.66685503,\n",
              "       -0.66686627, -0.66670561, -0.66679433, -0.66685642, -0.66683125,\n",
              "       -0.66685697, -0.66678332, -0.66676722, -0.66684159, -0.66685349,\n",
              "       -0.66677803, -0.66670084, -0.66681933, -0.66685937, -0.66675015,\n",
              "       -0.66677156, -0.66671051, -0.66670364, -0.66663782, -0.66666388,\n",
              "       -0.66663855, -0.66674202, -0.66658975, -0.66644647, -0.66642102,\n",
              "       -0.66668672, -0.66643366, -0.66651099, -0.66647054, -0.66645993,\n",
              "       -0.66638824, -0.66641215, -0.66630304, -0.6663249 , -0.66635391,\n",
              "       -0.66635756, -0.66633951, -0.66636986, -0.6661807 , -0.66638606,\n",
              "       -0.66666667, -0.66668199, -0.66669766, -0.66671003, -0.66672498,\n",
              "       -0.6667433 , -0.6667595 , -0.66677587, -0.66678745, -0.66680702,\n",
              "       -0.66682015, -0.66682898, -0.66685059, -0.66686807, -0.66687923,\n",
              "       -0.66690004, -0.66690121, -0.66692776, -0.66693491, -0.6669583 ,\n",
              "       -0.66695569, -0.66693911, -0.66690466, -0.66684106, -0.66700505,\n",
              "       -0.66704657, -0.66698862, -0.66690128, -0.66708546, -0.66702607,\n",
              "       -0.66700097, -0.66699645, -0.6669975 , -0.66687379, -0.66697934,\n",
              "       -0.6670316 , -0.66690278, -0.66684179, -0.66690953, -0.66686864,\n",
              "       -0.66700811, -0.66692269, -0.6667173 , -0.66688096, -0.66671784,\n",
              "       -0.66694784, -0.66679944, -0.6664778 , -0.66655326, -0.66660225,\n",
              "       -0.66653964, -0.66653299, -0.66662629, -0.6665503 , -0.66637938,\n",
              "       -0.66645919, -0.6661588 , -0.66631164, -0.66603082, -0.66625283,\n",
              "       -0.66627849, -0.66612351, -0.66615232, -0.66609641, -0.6659372 ,\n",
              "       -0.66606729, -0.66576831, -0.66593112, -0.66612758, -0.66563889,\n",
              "       -0.6660288 , -0.66595347, -0.66558195, -0.66576992, -0.66573445,\n",
              "       -0.66564966, -0.66569414, -0.6655035 , -0.66555536, -0.66562451,\n",
              "       -0.66548079, -0.66549384, -0.66539084, -0.66537066, -0.66535661,\n",
              "       -0.66535378, -0.66532668, -0.66541942, -0.66529473, -0.66528609,\n",
              "       -0.66530313, -0.66527604, -0.66522964, -0.66523551, -0.6652024 ,\n",
              "       -0.6651871 , -0.66524106, -0.66530187, -0.66536593, -0.66565146,\n",
              "       -0.66666667, -0.66667451, -0.66668335, -0.66669397, -0.66668746,\n",
              "       -0.66668801, -0.66670131, -0.6667011 , -0.66670179, -0.66669231,\n",
              "       -0.66669981, -0.66671606, -0.66672524, -0.66673666, -0.66669183,\n",
              "       -0.66671035, -0.666684  , -0.66672595, -0.66671957, -0.66669404,\n",
              "       -0.66668271, -0.66675768, -0.66671289, -0.66666805, -0.66669296,\n",
              "       -0.66661698, -0.66660852, -0.66655458, -0.66666969, -0.66668186,\n",
              "       -0.66652628, -0.66663682, -0.66648242, -0.66644257, -0.66637501,\n",
              "       -0.66638838, -0.66661804, -0.66632527, -0.66649847, -0.66632842,\n",
              "       -0.66636232, -0.6664351 , -0.66630612, -0.66626207, -0.66631207,\n",
              "       -0.66629875, -0.66622947, -0.66621567, -0.66619834, -0.66620472,\n",
              "       -0.66613655, -0.66612765, -0.66611218, -0.66610343, -0.66611522,\n",
              "       -0.66608252, -0.66607884, -0.66608259, -0.66604715, -0.66606753,\n",
              "       -0.66602579, -0.66603853, -0.66600622, -0.66602968, -0.66602677,\n",
              "       -0.66597529, -0.66596336, -0.66596744, -0.66598565, -0.66596731,\n",
              "       -0.66592108, -0.66607462, -0.66592627, -0.66613001, -0.66593694,\n",
              "       -0.6658965 , -0.66608777, -0.66608008, -0.66624282, -0.66598339,\n",
              "       -0.66610157, -0.66614377, -0.66634149, -0.66601511, -0.66598709,\n",
              "       -0.66636579, -0.66629997, -0.66623304, -0.66636769, -0.66631555,\n",
              "       -0.66625572, -0.66587866, -0.66610238, -0.66631678, -0.66695571,\n",
              "       -0.66620701, -0.66634845, -0.6665353 , -0.66661064, -0.6667484 ])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpvBysnG7ht0"
      },
      "outputs": [],
      "source": [
        "dz = probs # dz = probabilities of class\n",
        "dz[range(m),y] -= 1  # subtracting 1 from class where i == j as dz = pi - I\n",
        "#dz = dz/m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE3TFTzT7ht0"
      },
      "source": [
        "$\\frac{\\partial z_i}{\\partial w_i}$ is something we calculated earlier as well in LR, it will be equal to X.\n",
        "\n",
        "Now, to calculate dW, we just need to multiply X with dz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ijpwZ3yTYC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad39e9e5-5586-40c6-9173-30db1d2219e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "X.shape # shape (m, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIlwEiGWTbJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd30b832-0fbe-46eb-d677-5c4b3ec9c2d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# shape (m, n ) as we have m samples and each sample has 3 class probab.\n",
        "dz.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk_WYdWL7ht0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d23737-2b5d-4de3-aab7-477479ee2fd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# dW = dZ . X\n",
        "\n",
        "dW = np.dot(X.T, dz) # check dimensions (2, 300) x (300 , 3) => (2, 3)\n",
        "dW.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwyolIARTu6p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e67b69-b2e2-40f8-8011-8d250446349c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "b.shape # shape (1, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8QfBTDiIlFr"
      },
      "source": [
        "We know that,\n",
        "\n",
        "$z = wx + b$\n",
        "\n",
        "So,\n",
        "\n",
        "- $\\frac{∂z}{∂b}$ = $\\frac{∂(wx + b)}{∂b}$ = 1\n",
        "\n",
        "- as wx will be treated as constant when calculating partial derivative w.r.t b\n",
        "\n",
        "\n",
        "Now,\n",
        "\n",
        "For calculating db,\n",
        "\n",
        "db = dz.$\\frac{∂Z}{∂b}$ = dz\n",
        "\n",
        "#### But why sum?\n",
        "\n",
        "Since we are performing GD and not SGD,\n",
        "- we'll take sum of average across all points\n",
        "- we already took the average of dz (dz = dz/m)\n",
        "\n",
        "Recall update equation of bias\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIPYvH5XMq7u"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1IeM-uQ6cYp-167twAm8CuEK5avjn-j68' width=600></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvRQyLRo5-yV",
        "outputId": "207a3dd7-4be7-4fc6-f6a4-2e40480bd3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.66666667,  0.33333333,  0.33333333],\n",
              "       [-0.66666615,  0.33333802,  0.33332812],\n",
              "       [-0.6666718 ,  0.33335732,  0.33331448],\n",
              "       [-0.66667033,  0.33336051,  0.33330983],\n",
              "       [-0.66666601,  0.33335584,  0.33331016],\n",
              "       [-0.66666996,  0.33337189,  0.33329807],\n",
              "       [-0.66668264,  0.33340642,  0.33327622],\n",
              "       [-0.66667928,  0.33340594,  0.33327333],\n",
              "       [-0.66667424,  0.33340059,  0.33327365],\n",
              "       [-0.66668693,  0.33343541,  0.33325152],\n",
              "       [-0.66668209,  0.33343126,  0.33325083],\n",
              "       [-0.66668432,  0.33344262,  0.33324171],\n",
              "       [-0.66668819,  0.33345759,  0.3332306 ],\n",
              "       [-0.66668124,  0.333448  ,  0.33323325],\n",
              "       [-0.66669872,  0.33349322,  0.3332055 ],\n",
              "       [-0.66671411,  0.33353039,  0.33318371],\n",
              "       [-0.6667165 ,  0.33354212,  0.33317439],\n",
              "       [-0.66672933,  0.33357207,  0.33315726],\n",
              "       [-0.66672177,  0.33356643,  0.33315534],\n",
              "       [-0.66674541,  0.33361329,  0.33313212],\n",
              "       [-0.66675314,  0.33363274,  0.3331204 ],\n",
              "       [-0.66673255,  0.33360821,  0.33312433],\n",
              "       [-0.66675385,  0.33365177,  0.33310208],\n",
              "       [-0.66675308,  0.33365896,  0.33309413],\n",
              "       [-0.66677211,  0.33369467,  0.33307745],\n",
              "       [-0.66676625,  0.33369594,  0.33307031],\n",
              "       [-0.66677555,  0.33371796,  0.33305759],\n",
              "       [-0.66679031,  0.33374542,  0.33304489],\n",
              "       [-0.66680988,  0.33377039,  0.33303948],\n",
              "       [-0.66678561,  0.33375889,  0.33302673],\n",
              "       [-0.66681357,  0.33379914,  0.33301443],\n",
              "       [-0.66682466,  0.33381716,  0.3330075 ],\n",
              "       [-0.66684402,  0.33381447,  0.33302955],\n",
              "       [-0.66685031,  0.33382388,  0.33302643],\n",
              "       [-0.66684952,  0.33386009,  0.33298944],\n",
              "       [-0.66685829,  0.33386914,  0.33298915],\n",
              "       [-0.66686491,  0.33380742,  0.33305749],\n",
              "       [-0.66687331,  0.33386022,  0.33301309],\n",
              "       [-0.66687807,  0.33389915,  0.33297891],\n",
              "       [-0.66688382,  0.33391227,  0.33297154],\n",
              "       [-0.66688465,  0.33394829,  0.33293636],\n",
              "       [-0.66688409,  0.33381547,  0.33306862],\n",
              "       [-0.66689167,  0.33384094,  0.33305072],\n",
              "       [-0.66690629,  0.3339319 ,  0.33297439],\n",
              "       [-0.66690873,  0.33391057,  0.33299816],\n",
              "       [-0.66686423,  0.33367399,  0.33319024],\n",
              "       [-0.66689877,  0.3338119 ,  0.33308688],\n",
              "       [-0.66692077,  0.33391625,  0.33300452],\n",
              "       [-0.66692541,  0.33392355,  0.33300185],\n",
              "       [-0.66692117,  0.33387983,  0.33304134],\n",
              "       [-0.66694565,  0.3340688 ,  0.33287685],\n",
              "       [-0.66691217,  0.33380928,  0.33310289],\n",
              "       [-0.66692058,  0.33383466,  0.33308592],\n",
              "       [-0.66695557,  0.3340071 ,  0.33294847],\n",
              "       [-0.66691721,  0.33379689,  0.33312032],\n",
              "       [-0.6668971 ,  0.33370692,  0.33319018],\n",
              "       [-0.66691488,  0.33376662,  0.33314826],\n",
              "       [-0.66688775,  0.33365472,  0.33323303],\n",
              "       [-0.66687545,  0.33360287,  0.33327258],\n",
              "       [-0.66685503,  0.33352564,  0.3333294 ],\n",
              "       [-0.66686627,  0.33355588,  0.33331039],\n",
              "       [-0.66670561,  0.33306649,  0.33363912],\n",
              "       [-0.66679433,  0.33331325,  0.33348108],\n",
              "       [-0.66685642,  0.33350109,  0.33335533],\n",
              "       [-0.66683125,  0.33341341,  0.33341783],\n",
              "       [-0.66685697,  0.33348856,  0.3333684 ],\n",
              "       [-0.66678332,  0.33325473,  0.3335286 ],\n",
              "       [-0.66676722,  0.33320152,  0.3335657 ],\n",
              "       [-0.66684159,  0.33341861,  0.33342298],\n",
              "       [-0.66685349,  0.33344935,  0.33340415],\n",
              "       [-0.66677803,  0.33321385,  0.33356418],\n",
              "       [-0.66670084,  0.33299273,  0.33370812],\n",
              "       [-0.66681933,  0.33332334,  0.333496  ],\n",
              "       [-0.66685937,  0.33344039,  0.33341899],\n",
              "       [-0.66675015,  0.33310935,  0.3336408 ],\n",
              "       [-0.66677156,  0.33316376,  0.33360781],\n",
              "       [-0.66671051,  0.33298819,  0.33372232],\n",
              "       [-0.66670364,  0.33296367,  0.33373997],\n",
              "       [-0.66663782,  0.33278715,  0.33385067],\n",
              "       [-0.66666388,  0.33284727,  0.33381661],\n",
              "       [-0.66663855,  0.33277681,  0.33386174],\n",
              "       [-0.66674202,  0.33304354,  0.33369848],\n",
              "       [-0.66658975,  0.33264492,  0.33394483],\n",
              "       [-0.66644647,  0.33232289,  0.33412358],\n",
              "       [-0.66642102,  0.33226628,  0.33415475],\n",
              "       [-0.66668672,  0.33287022,  0.3338165 ],\n",
              "       [-0.66643366,  0.33227672,  0.33415694],\n",
              "       [-0.66651099,  0.33243224,  0.33407875],\n",
              "       [-0.66647054,  0.33233844,  0.33413211],\n",
              "       [-0.66645993,  0.33230957,  0.33415036],\n",
              "       [-0.66638824,  0.33216136,  0.33422689],\n",
              "       [-0.66641215,  0.33219963,  0.33421252],\n",
              "       [-0.66630304,  0.33200243,  0.33430062],\n",
              "       [-0.6663249 ,  0.33202806,  0.33429685],\n",
              "       [-0.66635391,  0.33206948,  0.33428443],\n",
              "       [-0.66635756,  0.33206845,  0.33428911],\n",
              "       [-0.66633951,  0.332029  ,  0.33431051],\n",
              "       [-0.66636986,  0.3320762 ,  0.33429366],\n",
              "       [-0.6661807 ,  0.33180965,  0.33437106],\n",
              "       [-0.66638606,  0.33209265,  0.33429341],\n",
              "       [ 0.33333333, -0.66666667,  0.33333333],\n",
              "       [ 0.33333797, -0.66668199,  0.33334402],\n",
              "       [ 0.33334406, -0.66669766,  0.3333536 ],\n",
              "       [ 0.33334518, -0.66671003,  0.33336485],\n",
              "       [ 0.33334947, -0.66672498,  0.33337552],\n",
              "       [ 0.33335656, -0.6667433 ,  0.33338674],\n",
              "       [ 0.33336224, -0.6667595 ,  0.33339726],\n",
              "       [ 0.33336894, -0.66677587,  0.33340692],\n",
              "       [ 0.33336876, -0.66678745,  0.33341868],\n",
              "       [ 0.33338028, -0.66680702,  0.33342675],\n",
              "       [ 0.33338001, -0.66682015,  0.33344014],\n",
              "       [ 0.33339459, -0.66682898,  0.3334344 ],\n",
              "       [ 0.33339884, -0.66685059,  0.33345175],\n",
              "       [ 0.33340308, -0.66686807,  0.33346499],\n",
              "       [ 0.33341039, -0.66687923,  0.33346884],\n",
              "       [ 0.33341273, -0.66690004,  0.33348731],\n",
              "       [ 0.33342252, -0.66690121,  0.33347869],\n",
              "       [ 0.33342589, -0.66692776,  0.33350187],\n",
              "       [ 0.3334333 , -0.66693491,  0.3335016 ],\n",
              "       [ 0.33343684, -0.6669583 ,  0.33352146],\n",
              "       [ 0.33344493, -0.66695569,  0.33351076],\n",
              "       [ 0.33344841, -0.66693911,  0.3334907 ],\n",
              "       [ 0.33344613, -0.66690466,  0.33345853],\n",
              "       [ 0.33343437, -0.66684106,  0.33340668],\n",
              "       [ 0.33346713, -0.66700505,  0.33353793],\n",
              "       [ 0.3334708 , -0.66704657,  0.33357577],\n",
              "       [ 0.33347372, -0.66698862,  0.3335149 ],\n",
              "       [ 0.33345926, -0.66690128,  0.33344202],\n",
              "       [ 0.3334886 , -0.66708546,  0.33359685],\n",
              "       [ 0.33348996, -0.66702607,  0.33353611],\n",
              "       [ 0.33348901, -0.66700097,  0.33351195],\n",
              "       [ 0.33349116, -0.66699645,  0.33350529],\n",
              "       [ 0.33349431, -0.6669975 ,  0.33350319],\n",
              "       [ 0.33346707, -0.66687379,  0.33340672],\n",
              "       [ 0.33349591, -0.66697934,  0.33348343],\n",
              "       [ 0.33351003, -0.6670316 ,  0.33352157],\n",
              "       [ 0.33348194, -0.66690278,  0.33342084],\n",
              "       [ 0.33346745, -0.66684179,  0.33337434],\n",
              "       [ 0.3334885 , -0.66690953,  0.33342103],\n",
              "       [ 0.33347963, -0.66686864,  0.33338902],\n",
              "       [ 0.33351843, -0.66700811,  0.33348968],\n",
              "       [ 0.33349914, -0.66692269,  0.33342355],\n",
              "       [ 0.33344075, -0.6667173 ,  0.33327656],\n",
              "       [ 0.33349227, -0.66688096,  0.3333887 ],\n",
              "       [ 0.33344526, -0.66671784,  0.33327258],\n",
              "       [ 0.33351536, -0.66694784,  0.33343247],\n",
              "       [ 0.33347505, -0.66679944,  0.33332439],\n",
              "       [ 0.33336941, -0.6664778 ,  0.33310839],\n",
              "       [ 0.33339871, -0.66655326,  0.33315455],\n",
              "       [ 0.33341786, -0.66660225,  0.33318439],\n",
              "       [ 0.3333983 , -0.66653964,  0.33314134],\n",
              "       [ 0.33339815, -0.66653299,  0.33313484],\n",
              "       [ 0.33343252, -0.66662629,  0.33319377],\n",
              "       [ 0.33340862, -0.6665503 ,  0.33314168],\n",
              "       [ 0.33334852, -0.66637938,  0.33303086],\n",
              "       [ 0.3333805 , -0.66645919,  0.3330787 ],\n",
              "       [ 0.33326447, -0.6661588 ,  0.33289433],\n",
              "       [ 0.3333294 , -0.66631164,  0.33298223],\n",
              "       [ 0.33321293, -0.66603082,  0.33281789],\n",
              "       [ 0.33331095, -0.66625283,  0.33294188],\n",
              "       [ 0.33332352, -0.66627849,  0.33295496],\n",
              "       [ 0.33326261, -0.66612351,  0.3328609 ],\n",
              "       [ 0.33327735, -0.66615232,  0.33287497],\n",
              "       [ 0.33325631, -0.66609641,  0.3328401 ],\n",
              "       [ 0.3331873 , -0.6659372 ,  0.3327499 ],\n",
              "       [ 0.33324912, -0.66606729,  0.33281817],\n",
              "       [ 0.33310684, -0.66576831,  0.33266147],\n",
              "       [ 0.33319373, -0.66593112,  0.33273739],\n",
              "       [ 0.3332823 , -0.66612758,  0.33284528],\n",
              "       [ 0.33303895, -0.66563889,  0.33259993],\n",
              "       [ 0.3332459 , -0.6660288 ,  0.33278289],\n",
              "       [ 0.33321564, -0.66595347,  0.33273783],\n",
              "       [ 0.33301634, -0.66558195,  0.33256561],\n",
              "       [ 0.3331349 , -0.66576992,  0.33263503],\n",
              "       [ 0.33312027, -0.66573445,  0.33261417],\n",
              "       [ 0.33307813, -0.66564966,  0.33257153],\n",
              "       [ 0.33310659, -0.66569414,  0.33258755],\n",
              "       [ 0.33299168, -0.6655035 ,  0.33251183],\n",
              "       [ 0.3330356 , -0.66555536,  0.33251976],\n",
              "       [ 0.3330808 , -0.66562451,  0.3325437 ],\n",
              "       [ 0.33299636, -0.66548079,  0.33248443],\n",
              "       [ 0.33301184, -0.66549384,  0.332482  ],\n",
              "       [ 0.33292784, -0.66539084,  0.332463  ],\n",
              "       [ 0.3329003 , -0.66537066,  0.33247036],\n",
              "       [ 0.33291105, -0.66535661,  0.33244555],\n",
              "       [ 0.33292729, -0.66535378,  0.33242649],\n",
              "       [ 0.33287756, -0.66532668,  0.33244912],\n",
              "       [ 0.33299624, -0.66541942,  0.33242318],\n",
              "       [ 0.33286822, -0.66529473,  0.33242651],\n",
              "       [ 0.33285469, -0.66528609,  0.3324314 ],\n",
              "       [ 0.33283565, -0.66530313,  0.33246748],\n",
              "       [ 0.33283329, -0.66527604,  0.33244275],\n",
              "       [ 0.33286235, -0.66522964,  0.33236729],\n",
              "       [ 0.33289536, -0.66523551,  0.33234016],\n",
              "       [ 0.33283427, -0.6652024 ,  0.33236813],\n",
              "       [ 0.33282853, -0.6651871 ,  0.33235858],\n",
              "       [ 0.33279811, -0.66524106,  0.33244295],\n",
              "       [ 0.33279155, -0.66530187,  0.33251032],\n",
              "       [ 0.33279213, -0.66536593,  0.33257381],\n",
              "       [ 0.33283673, -0.66565146,  0.33281473],\n",
              "       [ 0.33333333,  0.33333333, -0.66666667],\n",
              "       [ 0.33332781,  0.3333467 , -0.66667451],\n",
              "       [ 0.3333222 ,  0.33336116, -0.66668335],\n",
              "       [ 0.33331661,  0.33337736, -0.66669397],\n",
              "       [ 0.33331332,  0.33337413, -0.66668746],\n",
              "       [ 0.33330964,  0.33337836, -0.66668801],\n",
              "       [ 0.33330244,  0.33339887, -0.66670131],\n",
              "       [ 0.33329885,  0.33340225, -0.6667011 ],\n",
              "       [ 0.33329513,  0.33340666, -0.66670179],\n",
              "       [ 0.33329484,  0.33339747, -0.66669231],\n",
              "       [ 0.33328898,  0.33341083, -0.66669981],\n",
              "       [ 0.33328048,  0.33343558, -0.66671606],\n",
              "       [ 0.33327434,  0.3334509 , -0.66672524],\n",
              "       [ 0.33326766,  0.333469  , -0.66673666],\n",
              "       [ 0.33327883,  0.333413  , -0.66669183],\n",
              "       [ 0.33326886,  0.3334415 , -0.66671035],\n",
              "       [ 0.3332755 ,  0.3334085 , -0.666684  ],\n",
              "       [ 0.33325696,  0.33346899, -0.66672595],\n",
              "       [ 0.33325579,  0.33346378, -0.66671957],\n",
              "       [ 0.33326197,  0.33343207, -0.66669404],\n",
              "       [ 0.33326332,  0.3334194 , -0.66668271],\n",
              "       [ 0.3332334 ,  0.33352428, -0.66675768],\n",
              "       [ 0.33324518,  0.33346771, -0.66671289],\n",
              "       [ 0.33325996,  0.3334081 , -0.66666805],\n",
              "       [ 0.33324647,  0.33344649, -0.66669296],\n",
              "       [ 0.33327699,  0.33333999, -0.66661698],\n",
              "       [ 0.33327791,  0.33333061, -0.66660852],\n",
              "       [ 0.33330284,  0.33325174, -0.66655458],\n",
              "       [ 0.33324343,  0.33342626, -0.66666969],\n",
              "       [ 0.33323514,  0.33344672, -0.66668186],\n",
              "       [ 0.3333084 ,  0.33321788, -0.66652628],\n",
              "       [ 0.33324849,  0.33338833, -0.66663682],\n",
              "       [ 0.3333273 ,  0.33315512, -0.66648242],\n",
              "       [ 0.33334912,  0.33309346, -0.66644257],\n",
              "       [ 0.33339464,  0.33298036, -0.66637501],\n",
              "       [ 0.33337905,  0.33300933, -0.66638838],\n",
              "       [ 0.33324126,  0.33337678, -0.66661804],\n",
              "       [ 0.33342112,  0.33290415, -0.66632527],\n",
              "       [ 0.33329626,  0.3332022 , -0.66649847],\n",
              "       [ 0.33340672,  0.3329217 , -0.66632842],\n",
              "       [ 0.33337537,  0.33298695, -0.66636232],\n",
              "       [ 0.33332282,  0.33311228, -0.6664351 ],\n",
              "       [ 0.33340925,  0.33289687, -0.66630612],\n",
              "       [ 0.33344284,  0.33281924, -0.66626207],\n",
              "       [ 0.3333944 ,  0.33291767, -0.66631207],\n",
              "       [ 0.33339987,  0.33289888, -0.66629875],\n",
              "       [ 0.33345481,  0.33277466, -0.66622947],\n",
              "       [ 0.33346182,  0.33275386, -0.66621567],\n",
              "       [ 0.33347277,  0.33272556, -0.66619834],\n",
              "       [ 0.33345896,  0.33274576, -0.66620472],\n",
              "       [ 0.33354178,  0.33259477, -0.66613655],\n",
              "       [ 0.33354149,  0.33258616, -0.66612765],\n",
              "       [ 0.33358144,  0.33253073, -0.66611218],\n",
              "       [ 0.33355831,  0.33254512, -0.66610343],\n",
              "       [ 0.33352013,  0.3325951 , -0.66611522],\n",
              "       [ 0.33356564,  0.33251688, -0.66608252],\n",
              "       [ 0.33355301,  0.33252583, -0.66607884],\n",
              "       [ 0.3335333 ,  0.3325493 , -0.66608259],\n",
              "       [ 0.33360208,  0.33244507, -0.66604715],\n",
              "       [ 0.33364869,  0.33241884, -0.66606753],\n",
              "       [ 0.33360605,  0.33241974, -0.66602579],\n",
              "       [ 0.33365453,  0.332384  , -0.66603853],\n",
              "       [ 0.3336317 ,  0.33237452, -0.66600622],\n",
              "       [ 0.3336713 ,  0.33235838, -0.66602968],\n",
              "       [ 0.33367952,  0.33234725, -0.66602677],\n",
              "       [ 0.33361125,  0.33236404, -0.66597529],\n",
              "       [ 0.33362082,  0.33234254, -0.66596336],\n",
              "       [ 0.3335884 ,  0.33237904, -0.66596744],\n",
              "       [ 0.33370071,  0.33228494, -0.66598565],\n",
              "       [ 0.33370266,  0.33226465, -0.66596731],\n",
              "       [ 0.33367035,  0.33225073, -0.66592108],\n",
              "       [ 0.33372816,  0.33234646, -0.66607462],\n",
              "       [ 0.33371296,  0.3322133 , -0.66592627],\n",
              "       [ 0.33373159,  0.33239842, -0.66613001],\n",
              "       [ 0.33373675,  0.33220019, -0.66593694],\n",
              "       [ 0.33372941,  0.33216709, -0.6658965 ],\n",
              "       [ 0.33375072,  0.33233705, -0.66608777],\n",
              "       [ 0.33375622,  0.33232386, -0.66608008],\n",
              "       [ 0.33372811,  0.33251471, -0.66624282],\n",
              "       [ 0.33377356,  0.33220983, -0.66598339],\n",
              "       [ 0.3337661 ,  0.33233547, -0.66610157],\n",
              "       [ 0.33376232,  0.33238144, -0.66614377],\n",
              "       [ 0.33371419,  0.3326273 , -0.66634149],\n",
              "       [ 0.33379139,  0.33222371, -0.66601511],\n",
              "       [ 0.33379885,  0.33218825, -0.66598709],\n",
              "       [ 0.33371659,  0.33264919, -0.66636579],\n",
              "       [ 0.3337406 ,  0.33255936, -0.66629997],\n",
              "       [ 0.33376332,  0.33246973, -0.66623304],\n",
              "       [ 0.33372601,  0.33264167, -0.66636769],\n",
              "       [ 0.33374612,  0.33256942, -0.66631555],\n",
              "       [ 0.33376756,  0.33248816, -0.66625572],\n",
              "       [ 0.33384033,  0.33203833, -0.66587866],\n",
              "       [ 0.33381423,  0.33228815, -0.66610238],\n",
              "       [ 0.33375931,  0.33255747, -0.66631678],\n",
              "       [ 0.33349373,  0.33346198, -0.66695571],\n",
              "       [ 0.33379902,  0.33240799, -0.66620701],\n",
              "       [ 0.33375907,  0.33258938, -0.66634845],\n",
              "       [ 0.33369444,  0.33284086, -0.6665353 ],\n",
              "       [ 0.33366723,  0.33294341, -0.66661064],\n",
              "       [ 0.33361058,  0.33313782, -0.6667484 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(dz, axis=0, keepdims=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFmoc08j53XT",
        "outputId": "5dca3524-1b0d-40a9-b51f-dbcad33eadb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00364537, -0.01447561,  0.01083024]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBNyMDF4Tqdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfbfc30b-aa01-46db-e4ec-6c8c2c7d45cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "db = np.sum(dz, axis=0, keepdims=True)\n",
        "db.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dz/m"
      ],
      "metadata": {
        "id": "6dP1fwT561C1",
        "outputId": "3100c47c-0690-455c-b3ac-08a54eba6b52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.00222222,  0.00111111,  0.00111111],\n",
              "       [-0.00222222,  0.00111113,  0.00111109],\n",
              "       [-0.00222224,  0.00111119,  0.00111105],\n",
              "       [-0.00222223,  0.0011112 ,  0.00111103],\n",
              "       [-0.00222222,  0.00111119,  0.00111103],\n",
              "       [-0.00222223,  0.00111124,  0.00111099],\n",
              "       [-0.00222228,  0.00111135,  0.00111092],\n",
              "       [-0.00222226,  0.00111135,  0.00111091],\n",
              "       [-0.00222225,  0.00111134,  0.00111091],\n",
              "       [-0.00222229,  0.00111145,  0.00111084],\n",
              "       [-0.00222227,  0.00111144,  0.00111084],\n",
              "       [-0.00222228,  0.00111148,  0.00111081],\n",
              "       [-0.00222229,  0.00111153,  0.00111077],\n",
              "       [-0.00222227,  0.00111149,  0.00111078],\n",
              "       [-0.00222233,  0.00111164,  0.00111069],\n",
              "       [-0.00222238,  0.00111177,  0.00111061],\n",
              "       [-0.00222239,  0.00111181,  0.00111058],\n",
              "       [-0.00222243,  0.00111191,  0.00111052],\n",
              "       [-0.00222241,  0.00111189,  0.00111052],\n",
              "       [-0.00222248,  0.00111204,  0.00111044],\n",
              "       [-0.00222251,  0.00111211,  0.0011104 ],\n",
              "       [-0.00222244,  0.00111203,  0.00111041],\n",
              "       [-0.00222251,  0.00111217,  0.00111034],\n",
              "       [-0.00222251,  0.0011122 ,  0.00111031],\n",
              "       [-0.00222257,  0.00111232,  0.00111026],\n",
              "       [-0.00222255,  0.00111232,  0.00111023],\n",
              "       [-0.00222259,  0.00111239,  0.00111019],\n",
              "       [-0.00222263,  0.00111248,  0.00111015],\n",
              "       [-0.0022227 ,  0.00111257,  0.00111013],\n",
              "       [-0.00222262,  0.00111253,  0.00111009],\n",
              "       [-0.00222271,  0.00111266,  0.00111005],\n",
              "       [-0.00222275,  0.00111272,  0.00111002],\n",
              "       [-0.00222281,  0.00111271,  0.0011101 ],\n",
              "       [-0.00222283,  0.00111275,  0.00111009],\n",
              "       [-0.00222283,  0.00111287,  0.00110996],\n",
              "       [-0.00222286,  0.0011129 ,  0.00110996],\n",
              "       [-0.00222288,  0.00111269,  0.00111019],\n",
              "       [-0.00222291,  0.00111287,  0.00111004],\n",
              "       [-0.00222293,  0.001113  ,  0.00110993],\n",
              "       [-0.00222295,  0.00111304,  0.00110991],\n",
              "       [-0.00222295,  0.00111316,  0.00110979],\n",
              "       [-0.00222295,  0.00111272,  0.00111023],\n",
              "       [-0.00222297,  0.0011128 ,  0.00111017],\n",
              "       [-0.00222302,  0.00111311,  0.00110991],\n",
              "       [-0.00222303,  0.00111304,  0.00110999],\n",
              "       [-0.00222288,  0.00111225,  0.00111063],\n",
              "       [-0.002223  ,  0.00111271,  0.00111029],\n",
              "       [-0.00222307,  0.00111305,  0.00111002],\n",
              "       [-0.00222308,  0.00111308,  0.00111001],\n",
              "       [-0.00222307,  0.00111293,  0.00111014],\n",
              "       [-0.00222315,  0.00111356,  0.00110959],\n",
              "       [-0.00222304,  0.0011127 ,  0.00111034],\n",
              "       [-0.00222307,  0.00111278,  0.00111029],\n",
              "       [-0.00222319,  0.00111336,  0.00110983],\n",
              "       [-0.00222306,  0.00111266,  0.0011104 ],\n",
              "       [-0.00222299,  0.00111236,  0.00111063],\n",
              "       [-0.00222305,  0.00111256,  0.00111049],\n",
              "       [-0.00222296,  0.00111218,  0.00111078],\n",
              "       [-0.00222292,  0.00111201,  0.00111091],\n",
              "       [-0.00222285,  0.00111175,  0.0011111 ],\n",
              "       [-0.00222289,  0.00111185,  0.00111103],\n",
              "       [-0.00222235,  0.00111022,  0.00111213],\n",
              "       [-0.00222265,  0.00111104,  0.0011116 ],\n",
              "       [-0.00222285,  0.00111167,  0.00111118],\n",
              "       [-0.00222277,  0.00111138,  0.00111139],\n",
              "       [-0.00222286,  0.00111163,  0.00111123],\n",
              "       [-0.00222261,  0.00111085,  0.00111176],\n",
              "       [-0.00222256,  0.00111067,  0.00111189],\n",
              "       [-0.00222281,  0.0011114 ,  0.00111141],\n",
              "       [-0.00222284,  0.0011115 ,  0.00111135],\n",
              "       [-0.00222259,  0.00111071,  0.00111188],\n",
              "       [-0.00222234,  0.00110998,  0.00111236],\n",
              "       [-0.00222273,  0.00111108,  0.00111165],\n",
              "       [-0.00222286,  0.00111147,  0.0011114 ],\n",
              "       [-0.0022225 ,  0.00111036,  0.00111214],\n",
              "       [-0.00222257,  0.00111055,  0.00111203],\n",
              "       [-0.00222237,  0.00110996,  0.00111241],\n",
              "       [-0.00222235,  0.00110988,  0.00111247],\n",
              "       [-0.00222213,  0.00110929,  0.00111284],\n",
              "       [-0.00222221,  0.00110949,  0.00111272],\n",
              "       [-0.00222213,  0.00110926,  0.00111287],\n",
              "       [-0.00222247,  0.00111015,  0.00111233],\n",
              "       [-0.00222197,  0.00110882,  0.00111315],\n",
              "       [-0.00222149,  0.00110774,  0.00111375],\n",
              "       [-0.0022214 ,  0.00110755,  0.00111385],\n",
              "       [-0.00222229,  0.00110957,  0.00111272],\n",
              "       [-0.00222145,  0.00110759,  0.00111386],\n",
              "       [-0.0022217 ,  0.00110811,  0.0011136 ],\n",
              "       [-0.00222157,  0.00110779,  0.00111377],\n",
              "       [-0.00222153,  0.0011077 ,  0.00111383],\n",
              "       [-0.00222129,  0.0011072 ,  0.00111409],\n",
              "       [-0.00222137,  0.00110733,  0.00111404],\n",
              "       [-0.00222101,  0.00110667,  0.00111434],\n",
              "       [-0.00222108,  0.00110676,  0.00111432],\n",
              "       [-0.00222118,  0.0011069 ,  0.00111428],\n",
              "       [-0.00222119,  0.00110689,  0.0011143 ],\n",
              "       [-0.00222113,  0.00110676,  0.00111437],\n",
              "       [-0.00222123,  0.00110692,  0.00111431],\n",
              "       [-0.0022206 ,  0.00110603,  0.00111457],\n",
              "       [-0.00222129,  0.00110698,  0.00111431],\n",
              "       [ 0.00111111, -0.00222222,  0.00111111],\n",
              "       [ 0.00111113, -0.00222227,  0.00111115],\n",
              "       [ 0.00111115, -0.00222233,  0.00111118],\n",
              "       [ 0.00111115, -0.00222237,  0.00111122],\n",
              "       [ 0.00111116, -0.00222242,  0.00111125],\n",
              "       [ 0.00111119, -0.00222248,  0.00111129],\n",
              "       [ 0.00111121, -0.00222253,  0.00111132],\n",
              "       [ 0.00111123, -0.00222259,  0.00111136],\n",
              "       [ 0.00111123, -0.00222262,  0.0011114 ],\n",
              "       [ 0.00111127, -0.00222269,  0.00111142],\n",
              "       [ 0.00111127, -0.00222273,  0.00111147],\n",
              "       [ 0.00111132, -0.00222276,  0.00111145],\n",
              "       [ 0.00111133, -0.00222284,  0.00111151],\n",
              "       [ 0.00111134, -0.00222289,  0.00111155],\n",
              "       [ 0.00111137, -0.00222293,  0.00111156],\n",
              "       [ 0.00111138, -0.002223  ,  0.00111162],\n",
              "       [ 0.00111141, -0.002223  ,  0.0011116 ],\n",
              "       [ 0.00111142, -0.00222309,  0.00111167],\n",
              "       [ 0.00111144, -0.00222312,  0.00111167],\n",
              "       [ 0.00111146, -0.00222319,  0.00111174],\n",
              "       [ 0.00111148, -0.00222319,  0.0011117 ],\n",
              "       [ 0.00111149, -0.00222313,  0.00111164],\n",
              "       [ 0.00111149, -0.00222302,  0.00111153],\n",
              "       [ 0.00111145, -0.0022228 ,  0.00111136],\n",
              "       [ 0.00111156, -0.00222335,  0.00111179],\n",
              "       [ 0.00111157, -0.00222349,  0.00111192],\n",
              "       [ 0.00111158, -0.0022233 ,  0.00111172],\n",
              "       [ 0.00111153, -0.002223  ,  0.00111147],\n",
              "       [ 0.00111163, -0.00222362,  0.00111199],\n",
              "       [ 0.00111163, -0.00222342,  0.00111179],\n",
              "       [ 0.00111163, -0.00222334,  0.00111171],\n",
              "       [ 0.00111164, -0.00222332,  0.00111168],\n",
              "       [ 0.00111165, -0.00222332,  0.00111168],\n",
              "       [ 0.00111156, -0.00222291,  0.00111136],\n",
              "       [ 0.00111165, -0.00222326,  0.00111161],\n",
              "       [ 0.0011117 , -0.00222344,  0.00111174],\n",
              "       [ 0.00111161, -0.00222301,  0.0011114 ],\n",
              "       [ 0.00111156, -0.00222281,  0.00111125],\n",
              "       [ 0.00111163, -0.00222303,  0.0011114 ],\n",
              "       [ 0.0011116 , -0.0022229 ,  0.0011113 ],\n",
              "       [ 0.00111173, -0.00222336,  0.00111163],\n",
              "       [ 0.00111166, -0.00222308,  0.00111141],\n",
              "       [ 0.00111147, -0.00222239,  0.00111092],\n",
              "       [ 0.00111164, -0.00222294,  0.0011113 ],\n",
              "       [ 0.00111148, -0.00222239,  0.00111091],\n",
              "       [ 0.00111172, -0.00222316,  0.00111144],\n",
              "       [ 0.00111158, -0.00222266,  0.00111108],\n",
              "       [ 0.00111123, -0.00222159,  0.00111036],\n",
              "       [ 0.00111133, -0.00222184,  0.00111052],\n",
              "       [ 0.00111139, -0.00222201,  0.00111061],\n",
              "       [ 0.00111133, -0.0022218 ,  0.00111047],\n",
              "       [ 0.00111133, -0.00222178,  0.00111045],\n",
              "       [ 0.00111144, -0.00222209,  0.00111065],\n",
              "       [ 0.00111136, -0.00222183,  0.00111047],\n",
              "       [ 0.00111116, -0.00222126,  0.0011101 ],\n",
              "       [ 0.00111127, -0.00222153,  0.00111026],\n",
              "       [ 0.00111088, -0.00222053,  0.00110965],\n",
              "       [ 0.0011111 , -0.00222104,  0.00110994],\n",
              "       [ 0.00111071, -0.0022201 ,  0.00110939],\n",
              "       [ 0.00111104, -0.00222084,  0.00110981],\n",
              "       [ 0.00111108, -0.00222093,  0.00110985],\n",
              "       [ 0.00111088, -0.00222041,  0.00110954],\n",
              "       [ 0.00111092, -0.00222051,  0.00110958],\n",
              "       [ 0.00111085, -0.00222032,  0.00110947],\n",
              "       [ 0.00111062, -0.00221979,  0.00110917],\n",
              "       [ 0.00111083, -0.00222022,  0.00110939],\n",
              "       [ 0.00111036, -0.00221923,  0.00110887],\n",
              "       [ 0.00111065, -0.00221977,  0.00110912],\n",
              "       [ 0.00111094, -0.00222043,  0.00110948],\n",
              "       [ 0.00111013, -0.0022188 ,  0.00110867],\n",
              "       [ 0.00111082, -0.0022201 ,  0.00110928],\n",
              "       [ 0.00111072, -0.00221984,  0.00110913],\n",
              "       [ 0.00111005, -0.00221861,  0.00110855],\n",
              "       [ 0.00111045, -0.00221923,  0.00110878],\n",
              "       [ 0.0011104 , -0.00221911,  0.00110871],\n",
              "       [ 0.00111026, -0.00221883,  0.00110857],\n",
              "       [ 0.00111036, -0.00221898,  0.00110863],\n",
              "       [ 0.00110997, -0.00221835,  0.00110837],\n",
              "       [ 0.00111012, -0.00221852,  0.0011084 ],\n",
              "       [ 0.00111027, -0.00221875,  0.00110848],\n",
              "       [ 0.00110999, -0.00221827,  0.00110828],\n",
              "       [ 0.00111004, -0.00221831,  0.00110827],\n",
              "       [ 0.00110976, -0.00221797,  0.00110821],\n",
              "       [ 0.00110967, -0.0022179 ,  0.00110823],\n",
              "       [ 0.0011097 , -0.00221786,  0.00110815],\n",
              "       [ 0.00110976, -0.00221785,  0.00110809],\n",
              "       [ 0.00110959, -0.00221776,  0.00110816],\n",
              "       [ 0.00110999, -0.00221806,  0.00110808],\n",
              "       [ 0.00110956, -0.00221765,  0.00110809],\n",
              "       [ 0.00110952, -0.00221762,  0.0011081 ],\n",
              "       [ 0.00110945, -0.00221768,  0.00110822],\n",
              "       [ 0.00110944, -0.00221759,  0.00110814],\n",
              "       [ 0.00110954, -0.00221743,  0.00110789],\n",
              "       [ 0.00110965, -0.00221745,  0.0011078 ],\n",
              "       [ 0.00110945, -0.00221734,  0.00110789],\n",
              "       [ 0.00110943, -0.00221729,  0.00110786],\n",
              "       [ 0.00110933, -0.00221747,  0.00110814],\n",
              "       [ 0.00110931, -0.00221767,  0.00110837],\n",
              "       [ 0.00110931, -0.00221789,  0.00110858],\n",
              "       [ 0.00110946, -0.00221884,  0.00110938],\n",
              "       [ 0.00111111,  0.00111111, -0.00222222],\n",
              "       [ 0.00111109,  0.00111116, -0.00222225],\n",
              "       [ 0.00111107,  0.0011112 , -0.00222228],\n",
              "       [ 0.00111106,  0.00111126, -0.00222231],\n",
              "       [ 0.00111104,  0.00111125, -0.00222229],\n",
              "       [ 0.00111103,  0.00111126, -0.00222229],\n",
              "       [ 0.00111101,  0.00111133, -0.00222234],\n",
              "       [ 0.001111  ,  0.00111134, -0.00222234],\n",
              "       [ 0.00111098,  0.00111136, -0.00222234],\n",
              "       [ 0.00111098,  0.00111132, -0.00222231],\n",
              "       [ 0.00111096,  0.00111137, -0.00222233],\n",
              "       [ 0.00111093,  0.00111145, -0.00222239],\n",
              "       [ 0.00111091,  0.0011115 , -0.00222242],\n",
              "       [ 0.00111089,  0.00111156, -0.00222246],\n",
              "       [ 0.00111093,  0.00111138, -0.00222231],\n",
              "       [ 0.0011109 ,  0.00111147, -0.00222237],\n",
              "       [ 0.00111092,  0.00111136, -0.00222228],\n",
              "       [ 0.00111086,  0.00111156, -0.00222242],\n",
              "       [ 0.00111085,  0.00111155, -0.0022224 ],\n",
              "       [ 0.00111087,  0.00111144, -0.00222231],\n",
              "       [ 0.00111088,  0.0011114 , -0.00222228],\n",
              "       [ 0.00111078,  0.00111175, -0.00222253],\n",
              "       [ 0.00111082,  0.00111156, -0.00222238],\n",
              "       [ 0.00111087,  0.00111136, -0.00222223],\n",
              "       [ 0.00111082,  0.00111149, -0.00222231],\n",
              "       [ 0.00111092,  0.00111113, -0.00222206],\n",
              "       [ 0.00111093,  0.0011111 , -0.00222203],\n",
              "       [ 0.00111101,  0.00111084, -0.00222185],\n",
              "       [ 0.00111081,  0.00111142, -0.00222223],\n",
              "       [ 0.00111078,  0.00111149, -0.00222227],\n",
              "       [ 0.00111103,  0.00111073, -0.00222175],\n",
              "       [ 0.00111083,  0.00111129, -0.00222212],\n",
              "       [ 0.00111109,  0.00111052, -0.00222161],\n",
              "       [ 0.00111116,  0.00111031, -0.00222148],\n",
              "       [ 0.00111132,  0.00110993, -0.00222125],\n",
              "       [ 0.00111126,  0.00111003, -0.00222129],\n",
              "       [ 0.0011108 ,  0.00111126, -0.00222206],\n",
              "       [ 0.0011114 ,  0.00110968, -0.00222108],\n",
              "       [ 0.00111099,  0.00111067, -0.00222166],\n",
              "       [ 0.00111136,  0.00110974, -0.00222109],\n",
              "       [ 0.00111125,  0.00110996, -0.00222121],\n",
              "       [ 0.00111108,  0.00111037, -0.00222145],\n",
              "       [ 0.00111136,  0.00110966, -0.00222102],\n",
              "       [ 0.00111148,  0.0011094 , -0.00222087],\n",
              "       [ 0.00111131,  0.00110973, -0.00222104],\n",
              "       [ 0.00111133,  0.00110966, -0.002221  ],\n",
              "       [ 0.00111152,  0.00110925, -0.00222076],\n",
              "       [ 0.00111154,  0.00110918, -0.00222072],\n",
              "       [ 0.00111158,  0.00110909, -0.00222066],\n",
              "       [ 0.00111153,  0.00110915, -0.00222068],\n",
              "       [ 0.00111181,  0.00110865, -0.00222046],\n",
              "       [ 0.0011118 ,  0.00110862, -0.00222043],\n",
              "       [ 0.00111194,  0.00110844, -0.00222037],\n",
              "       [ 0.00111186,  0.00110848, -0.00222034],\n",
              "       [ 0.00111173,  0.00110865, -0.00222038],\n",
              "       [ 0.00111189,  0.00110839, -0.00222028],\n",
              "       [ 0.00111184,  0.00110842, -0.00222026],\n",
              "       [ 0.00111178,  0.0011085 , -0.00222028],\n",
              "       [ 0.00111201,  0.00110815, -0.00222016],\n",
              "       [ 0.00111216,  0.00110806, -0.00222023],\n",
              "       [ 0.00111202,  0.00110807, -0.00222009],\n",
              "       [ 0.00111218,  0.00110795, -0.00222013],\n",
              "       [ 0.00111211,  0.00110792, -0.00222002],\n",
              "       [ 0.00111224,  0.00110786, -0.0022201 ],\n",
              "       [ 0.00111227,  0.00110782, -0.00222009],\n",
              "       [ 0.00111204,  0.00110788, -0.00221992],\n",
              "       [ 0.00111207,  0.00110781, -0.00221988],\n",
              "       [ 0.00111196,  0.00110793, -0.00221989],\n",
              "       [ 0.00111234,  0.00110762, -0.00221995],\n",
              "       [ 0.00111234,  0.00110755, -0.00221989],\n",
              "       [ 0.00111223,  0.0011075 , -0.00221974],\n",
              "       [ 0.00111243,  0.00110782, -0.00222025],\n",
              "       [ 0.00111238,  0.00110738, -0.00221975],\n",
              "       [ 0.00111244,  0.00110799, -0.00222043],\n",
              "       [ 0.00111246,  0.00110733, -0.00221979],\n",
              "       [ 0.00111243,  0.00110722, -0.00221965],\n",
              "       [ 0.0011125 ,  0.00110779, -0.00222029],\n",
              "       [ 0.00111252,  0.00110775, -0.00222027],\n",
              "       [ 0.00111243,  0.00110838, -0.00222081],\n",
              "       [ 0.00111258,  0.00110737, -0.00221994],\n",
              "       [ 0.00111255,  0.00110778, -0.00222034],\n",
              "       [ 0.00111254,  0.00110794, -0.00222048],\n",
              "       [ 0.00111238,  0.00110876, -0.00222114],\n",
              "       [ 0.00111264,  0.00110741, -0.00222005],\n",
              "       [ 0.00111266,  0.00110729, -0.00221996],\n",
              "       [ 0.00111239,  0.00110883, -0.00222122],\n",
              "       [ 0.00111247,  0.00110853, -0.002221  ],\n",
              "       [ 0.00111254,  0.00110823, -0.00222078],\n",
              "       [ 0.00111242,  0.00110881, -0.00222123],\n",
              "       [ 0.00111249,  0.00110856, -0.00222105],\n",
              "       [ 0.00111256,  0.00110829, -0.00222085],\n",
              "       [ 0.0011128 ,  0.00110679, -0.0022196 ],\n",
              "       [ 0.00111271,  0.00110763, -0.00222034],\n",
              "       [ 0.00111253,  0.00110852, -0.00222106],\n",
              "       [ 0.00111165,  0.00111154, -0.00222319],\n",
              "       [ 0.00111266,  0.00110803, -0.00222069],\n",
              "       [ 0.00111253,  0.00110863, -0.00222116],\n",
              "       [ 0.00111231,  0.00110947, -0.00222178],\n",
              "       [ 0.00111222,  0.00110981, -0.00222204],\n",
              "       [ 0.00111204,  0.00111046, -0.00222249]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRAiyPl97ht1"
      },
      "outputs": [],
      "source": [
        "def backprop(probs, y):\n",
        "    # we know that dz = pi - I\n",
        "    dz = probs # dz = pi\n",
        "    dz[range(m),y] -= 1 # subtacting 1 where i ==j i.e. class label matches\n",
        "    dz = dz/m # taking average as we have m points\n",
        "    dW = np.dot(X.T, dz)\n",
        "    db = np.sum(dz, axis=0, keepdims=True)\n",
        "    return dW, db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u5MHDezrEWB"
      },
      "source": [
        "Since, we are moving from right to left, to calculate the gradients, lets call it backproprogation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb5OvKTo7ht1"
      },
      "source": [
        "#### Once, we have the gradients, we will update the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u_c0r5Z7ht1"
      },
      "outputs": [],
      "source": [
        "lr = 0.1\n",
        "W += -lr * dW\n",
        "b += -lr * db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPfHt-LT7ht1"
      },
      "source": [
        "Lets put all this together in a class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFUX0j2f7ht1"
      },
      "outputs": [],
      "source": [
        "max_iters = 500\n",
        "lr = 1\n",
        "\n",
        "d = X.shape[1]\n",
        "n = len(np.unique(y))\n",
        "m = X.shape[0]\n",
        "W = 0.01 * np.random.randn(d,n)\n",
        "b = np.zeros((1,n))\n",
        "loss_history = []\n",
        "\n",
        "for i in range(max_iters):\n",
        "    # evaluate the class probs\n",
        "    z = np.dot(X, W) + b\n",
        "    exp_z = np.exp(z)\n",
        "    probs = exp_z/np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    # compute the loss: average cross-entropy loss and regularization\n",
        "    error = -np.log(probs[range(m), y])\n",
        "    loss = np.sum(error)/m\n",
        "    loss_history.append(loss)\n",
        "    if i % 100 == 0:\n",
        "        print(f\"iteration: {i}, loss: {loss}\")\n",
        "\n",
        "\n",
        "    # compute the gradient on score\n",
        "    dZ = probs\n",
        "    dZ[range(m),y] -= 1\n",
        "    dZ = dZ/m\n",
        "    dW = np.dot(X.T, dZ)\n",
        "    db = np.sum(dZ, axis=0, keepdims=True)\n",
        "\n",
        "    # perform a parameter update using gradient descent\n",
        "    W += -lr * dW\n",
        "    b += -lr * db\n",
        "# history = pd.DataFrame({'step': list(range(max_iters)), 'loss': loss_history})\n",
        "# history.plot(x='step', y='loss',xlabel='step', ylabel='loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvL9xpSh7ht1"
      },
      "outputs": [],
      "source": [
        "def predict(X):\n",
        "    Z = np.dot(X, W) + b\n",
        "    Z_e = np.exp(Z)\n",
        "    probs = Z_e/np.sum(Z_e, axis=1, keepdims=True)\n",
        "    return np.argmax(probs, axis=1)\n",
        "\n",
        "print(f\"Training Accuracy {np.sum(predict(X) == y)/m}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFHx_NP87ht1"
      },
      "source": [
        "We have adapted the Logistic Regression model to work for mult-class setting.\n",
        "\n",
        "Let's plot the decision boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3kYQGsx7ht2"
      },
      "outputs": [],
      "source": [
        "# create a 2D grid\n",
        "step = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))\n",
        "\n",
        "# predict for all the points in the grid\n",
        "y_hat = predict(np.c_[xx.ravel(), yy.ravel()]) # concatenates along second axis\n",
        "y_hat = y_hat.reshape(xx.shape)\n",
        "\n",
        "# plot\n",
        "fig = plt.figure()\n",
        "plt.contourf(xx, yy, y_hat, cmap=plt.cm.Spectral, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc_TAEO_7ht2"
      },
      "source": [
        "- We can see that the model has learnt three decision boundaries\n",
        "- But since  haven't added any intermediate layer of neurons to create complex features, we don't expect the model to learn complex boundaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxgxh6xa7ht2"
      },
      "source": [
        "#### Softmax Classifier code wrapped in a Python class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KksJRrV7ht2"
      },
      "outputs": [],
      "source": [
        "class SoftmaxClassfier:\n",
        "    def __init__(self, n_features, n_outputs):\n",
        "        self.d = n_features\n",
        "        self.n = n_outputs\n",
        "        self.W = 0.01 * np.random.randn(d,n)\n",
        "        self.b = np.zeros((1,n))\n",
        "        self.loss = []\n",
        "\n",
        "    def fwdprop(self, X):\n",
        "        z = np.dot(X, self.W) + self.b\n",
        "        exp_z = np.exp(z)\n",
        "        probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "        return probs\n",
        "\n",
        "    def cce_loss(self, probs, y):\n",
        "        m = y.shape[0]\n",
        "        error = -np.log(probs[range(m), y])\n",
        "        return np.sum(error)/m\n",
        "\n",
        "    def backprop(self, probs, y):\n",
        "        m = y.shape[0]\n",
        "        dz = probs\n",
        "        dz[range(m),y] -= 1\n",
        "        dz = dz/m\n",
        "        dW = np.dot(X.T, dz)\n",
        "        db = np.sum(dz, axis=0, keepdims=True)\n",
        "        return dW, db\n",
        "\n",
        "    def fit(self, X, y, lr=0.1, max_iters=50):\n",
        "\n",
        "        for i in range(max_iters):\n",
        "            # evaluate the class probs\n",
        "            probs = self.fwdprop(X)\n",
        "\n",
        "            # compute the loss: average cross-entropy loss and regularization\n",
        "            loss = self.cce_loss(probs, y)\n",
        "\n",
        "            # compute the gradient on score\n",
        "            dW, db = self.backprop(probs, y)\n",
        "\n",
        "            # perform a parameter update using gradient descent\n",
        "            self.W += -lr * dW\n",
        "            self.b += -lr * db\n",
        "            self.loss.append(loss)\n",
        "\n",
        "        self.history = pd.DataFrame({\n",
        "        'step': list(range(max_iters)),\n",
        "        'loss': self.loss})\n",
        "\n",
        "    def plot_loss(self):\n",
        "        return self.history.plot(x='step', y='loss',xlabel='step', ylabel='loss')\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.fwdprop(X)\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "model = SoftmaxClassfier(n_features=2, n_outputs=3)\n",
        "model.fit(X, y, lr=1, max_iters=500)\n",
        "#model.plot_loss()\n",
        "#print('training accuracy:', np.sum(model.predict(X) == y)/X.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ARqCbbzN6IC"
      },
      "outputs": [],
      "source": [
        "X = [[100, 129, 157, 133], [168, 150, 30, 19], [4, 148, 106, 74], [123, 195, 60, 93], [169, 40, 188, 179], [40, 59, 29, 94], [165, 126, 16, 99], [167, 157, 65, 23], [128, 87, 37, 111], [191, 154, 89, 134], [101, 41, 145, 112], [43, 110, 197, 118], [147, 22, 109, 139], [11, 161, 135, 119], [26, 48, 199, 182], [96, 100, 82, 87], [149, 2, 8, 10], [5, 38, 166, 100], [193, 117, 59, 164], [133, 5, 38, 163], [88, 177, 84, 114], [9, 132, 177, 24], [94, 130, 83, 131], [77, 11, 141, 81], [154, 198, 175, 98], [21, 148, 170, 122], [185, 145, 101, 183], [100, 196, 111, 11], [97, 147, 112, 11], [25, 97, 95, 45], [6, 89, 88, 38], [51, 16, 151, 3], [90, 174, 122, 157], [2, 133, 121, 199], [15, 78, 163, 180], [103, 118, 7, 179], [102, 179, 157, 183], [113, 139, 195, 122], [55, 88, 68, 117], [115, 185, 93, 102], [139, 82, 3, 165], [135, 29, 78, 11], [11, 16, 60, 123], [103, 191, 187, 129], [146, 181, 28, 192], [85, 73, 136, 139], [117, 179, 81, 183], [15, 131, 106, 28], [58, 78, 111, 65], [76, 11, 25, 103], [11, 90, 162, 129], [144, 1, 16, 33], [33, 172, 40, 72], [106, 83, 160, 151], [68, 159, 150, 64], [31, 79, 83, 15], [51, 140, 173, 10], [105, 80, 70, 21], [195, 80, 64, 129], [50, 96, 107, 82], [185, 150, 15, 143], [28, 71, 27, 57], [58, 13, 146, 78], [20, 71, 183, 44], [91, 44, 15, 87], [77, 157, 95, 110], [132, 28, 193, 49], [177, 87, 57, 41], [194, 175, 17, 20], [166, 64, 134, 150], [79, 74, 162, 168], [166, 149, 34, 117], [160, 170, 127, 44], [99, 41, 103, 155], [48, 127, 138, 68], [17, 3, 101, 94], [29, 102, 123, 158], [194, 60, 135, 179], [73, 192, 145, 168], [21, 94, 154, 143], [17, 10, 145, 131], [73, 29, 195, 199], [132, 189, 90, 100], [134, 32, 81, 119], [118, 37, 119, 27], [51, 78, 187, 86], [95, 8, 56, 29], [156, 162, 186, 127], [126, 111, 144, 59], [7, 140, 32, 75], [40, 0, 109, 92], [165, 175, 61, 103], [178, 68, 185, 119], [132, 105, 36, 80], [165, 117, 35, 176], [128, 49, 185, 9], [50, 176, 12, 198], [124, 164, 99, 102], [36, 30, 114, 147], [166, 172, 35, 14]]\n",
        "y = [1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHxK-m-uN6mN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "X = np.array(X)\n",
        "#dependent variable\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIFJOKsLODTc"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gv1JnJoOD64"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFqZ6IStOFiB"
      },
      "outputs": [],
      "source": [
        "m = X.shape[0]  #no. of samples\n",
        "n = X.shape[1]  #no. of features\n",
        "c = 1  #no. of classes in the data and therefore no. of neurons in the layer\n",
        "# example of a function for calculating softmax for a list of numbers\n",
        "from numpy import exp\n",
        "\n",
        "# calculate the softmax of a vector\n",
        "def softmax(vector):\n",
        " e = exp(vector)\n",
        " return e / e.sum()\n",
        "#weight vector of dimension (number of features, number of neurons in the layer)\n",
        "for i in range(m):\n",
        "  w = np.random.randn(n,c)\n",
        "  b = np.zeros((1, c))\n",
        "  z = np.dot(X,w) + b\n",
        "  # a = np.exp(z)\n",
        "  result = softmax(z)\n",
        "  print(result)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sr6urOyOXMi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}